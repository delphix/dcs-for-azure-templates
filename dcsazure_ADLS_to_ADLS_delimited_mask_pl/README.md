# dcsazure_ADLS_to_ADLS_delimited_mask_pl
## Delphix Compliance Services (DCS) for Azure - ADLS to ADLS Delimited Masking Pipeline

This pipeline will perform masking of your delimited data from your Azure Data Lake (ADLS) from one location to another.

### Prerequisites

1. Configure the hosted metadata database and associated Azure SQL service (version `V2025.07.22.0`+).
1. Configure the DCS for Azure REST service.
1. Configure the Azure Data Lake Storage service associated with your ADLS source data.
1. Configure the Azure Data Lake Storage service associated with your ADLS sink data.


### Importing
There are several linked services that will need to be selected in order to perform the masking of your ADLS
instance.

These linked services types are needed for the following steps:

`Azure Data Lake Storage` (source) - Linked service associated with ADLS source data. This will be used for the
following steps:
* dcsazure_ADLS_to_ADLS_delimited_filter_test_utility_df/Source (dataFlow)
* dcsazure_adls_container_and_directory_mask_ds (DelimitedText dataset)
* dcsazure_ADLS_to_ADLS_delimited_unfiltered_mask_df/Source (dataFlow)
* dcsazure_ADLS_to_ADLS_delimited_filtered_mask_df/Source (dataFlow)
* dcsazure_ADLS_to_ADLS_delimited_copy_df/Source (dataFlow)

`Azure Data Lake Storage` (sink) - Linked service associated with ADLS sink data. This will be used for the
following steps:
* dcsazure_ADLS_to_ADLS_delimited_filter_test_utility_df/Sink (dataFlow)
* dcsazure_ADLS_to_ADLS_delimited_unfiltered_mask_df/Sink (dataFlow)
* dcsazure_ADLS_to_ADLS_delimited_filtered_mask_df/Sink (dataFlow)
* dcsazure_ADLS_to_ADLS_delimited_copy_df/Sink (dataFlow)

`Azure SQL` (metadata) - Linked service associated with your hosted metadata store. This will be used for the following
steps:
* Check For Conditional Masking (If Condition activity)
* If Copy Via Dataflow (If Condition activity)
* Check If We Should Reapply Mapping (If Condition activity)
* Configure Masked Status (Script activity)
* Lookup Masking Parameters (Lookup activity)
* dcsazure_ADLS_to_ADLS_delimited_metadata_mask_ds (Azure SQL Database dataset)

`REST` (DCS for Azure) - Linked service associated with calling DCS for Azure. This will be used for the following
steps:
* dcsazure_ADLS_to_ADLS_delimited_unfiltered_mask_df (dataFlow)
* dcsazure_ADLS_to_ADLS_delimited_filtered_mask_df (dataFlow)

### How It Works
* Check If We Should Reapply Mapping
  * If we should, Mark Table Mapping Incomplete. This is done by updating the metadata store to indicate that tables
    have not had their mapping applied
* Select Directories We Should Purge
  * Select sink directories with an incomplete mapping and based on the value of `P_TRUNCATE_SINK_BEFORE_WRITE`, create
    a list of directories that we should purge 
    * For Each Directory To Purge:
      * Check For The Directory
      * If the directory exists, delete everything in that directory
* Select Tables Without Required Masking. This is done by querying the metadata store.
  * Filter If Copy Unmask Enabled. This is done by applying a filter based on the value of `P_COPY_UNMASKED_TABLES`
    * For Each Table With No Masking. Provided we have any rows left after applying the filter
      * If Copy Via Dataflow - based on the value of `P_COPY_USE_DATAFLOW`
        * If the data flow is to be used for copy then call `dcsazure_ADLS_to_ADLS_delimited_copy_df`
          * Update the mapped status based on the success of this dataflow, and fail accordingly
        * If the data flow is not to be used for copy, then use a copy activity
          * Update the mapped status based on the success of this dataflow, and fail accordingly
* Select Tables That Require Masking. This is done by querying the metadata store. This will provide a list of tables
  that need masking, and if they need to be masked leveraging conditional algorithms, the set of required filters.
  * Configure Masked Status. Set the masked status based on the defined filters that need to be applied for the table to
    be marked as completely mapped.
  * For Each Table To Mask
    * Check if the table must be masked with a filter condition
      * If no filter needs to be applied:
        * Call the `dcsazure_ADLS_to_ADLS_delimited_unfiltered_mask_df` data flow, passing in parameters as generated by
          the `Lookup Masking Parameters` activity
        * Update the mapped status based on the success of this dataflow, and fail accordingly
      * If a filter needs to be applied:
        * Call the `dcsazure_ADLS_to_ADLS_delimited_filterd_mask_df` data flow, passing in parameters as generated by
          the `Lookup Masking Parameters` activity and the filter as determined by the output of For Each Table To Mask
        * Update the mapped status based on the success of this dataflow, and fail accordingly
* Note that there is a deactivated activity `Test Filter Conditions` that exists in order to support importing the
  filter test utility dataflow, this is making it easier to test writing filter conditions leveraging a dataflow debug
  session
* 
### Notes
As ADLS does not provide us the number of rows in the files, and as delimited files don't have a fixed width to columns,
we make a request run into request size limits or have a suboptimal number of batches.
* If you are running into request size limits, decrease the number of rows in a batch by modifying the `ROWS_PER_BATCH`
  variable.
* If you are not hitting request size limits, but are finding that your masking pipeline is taking a long time, you may
  wish to increase the number of rows per batch.

### Variables

If you have configured your database using the metadata store scripts, these variables will not need editing. If you
have customized your metadata store, then these variables may need editing.

* `METADATA_SCHEMA` - This is the schema to be used for in the self-hosted AzureSQL database for storing metadata
  (default `dbo`).
* `METADATA_RULESET_TABLE` - This is the table to be used for storing the discovered ruleset (default
  `discovered_ruleset`).
* `METADATA_SOURCE_TO_SINK_MAPPING_TABLE` - This is the table used for determining where data that is run through the
  pipeline starts and ends (default `adf_data_mapping`)
* `METADATA_ADF_TYPE_MAPPING_TABLE` - This is the table used for determining how Azure Data Factory should interpret
  data that flows through the pipeline (default `adf_type_mapping`)
* `TARGET_BATCH_SIZE` - This is the target number of rows per batch (default `50000`)
* `DATASET` - This is the way this data set is referred to in the metadata store (default `ADLS-DELIMITED`)
* `CONDITIONAL_MASKING_RESERVED_CHARACTER` - This is a string (preferably a character) reserved as for shorthand for
  when referring to the key column when defining filter conditions, in the pipeline this will be expanded out to use the
  ADF syntax for referencing the key column (default `%`)
* `METADATA_EVENT_PROCEDURE_NAME` - This is the name of the procedure used to capture pipeline information in the
  metadata data store and sets the masked and mapping states on the items processed during execution
  (default `insert_adf_masking_event`).
  parameters for both conditional and non-conditional masking scenarios (default `generate_masking_parameters`).
### Parameters

* `P_COPY_UNMASKED_TABLES` - Bool - This enables the pipeline to copy data from source to destination when a mapping
  exists, but no algorithms have been defined (default `false`)
* `P_COPY_USE_DATAFLOW` - Bool - This enables the pipeline to use a data flow to copy data from source to sink when
  there is no data to mask (this value does not matter if `P_COPY_UNMASKED_TABLES` is `false`)
* `P_FAIL_ON_NONCONFORMANT_DATA` - Bool - This enables the pipeline to handle non-conformant data errors without failing
  the pipeline - if set to `true`, unmasked data that did not conform to the format required to apply the specified
  algorithm will appear in the output data; if set to `false`, data that did not conform to the format required to apply
  the specified algorithm will cause the pipeline to fail (default `true`)
* `P_REAPPLY_MAPPING` - Bool - This controls whether we should reset the mapping between source and sink tables, this
  will mark all mappings as incomplete (default `true`)
* `P_TRUNCATE_SINK_BEFORE_WRITE` - Bool - This controls whether we should purge the directories in the sink locations
  that have not already been completed by this pipeline, note that the set of locations to purge does _not_ depend on
  the value of `P_COPY_UNMASKED_TABLES` (default `true`)
* `P_SOURCE_CONTAINER` - String - This is the source storage container in ADLS that contains the unmasked data
* `P_SINK_CONTAINER` - String - This is the sink storage container in ADLS that will serve as a destination for masked
  data
* `P_SOURCE_DIRECTORY` - String - This is the source schema in ADLS and was discovered during the profiling pipeline
  run - it will have a format that defines the prefix of a file name in ADLS (with its full location) and that contains
  the unmasked data
* `P_SINK_DIRECTORY` - String - This is the sink schema in ADLS that will have a structure similar to the source schema
  name, except that anything after the last `/` is ignored and that will serve as a destination for masked data (file
  names will be preserved)
