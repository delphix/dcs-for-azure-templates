# dcsazure_Databricks_to_Databricks_mask_pl
## Delphix Compliance Services (DCS) for Azure - Databricks to Databricks Masking Pipeline

This pipeline will perform masking of data from your Databricks Delta Lake from one Delta Lake to another.

### Prerequisites

1. Configure the hosted metadata database and associated Azure SQL service.
1. Configure the DCS for Azure REST service.
1. Configure the Azure Data Lake Storage service associated with your Databricks source data.
1. Configure the Azure Databricks Delta Lake service associated with your Databricks Deltalake source data.


### Importing
There are several linked services that will need to be selected in order to perform the masking of your Databricks
instance.

These linked services types are needed for the following steps:

`Azure Data Lake Storage` (source) - Linked service associated with Databricks source data. This will be used for the
following steps:
* Switch On Copy Methodology And Supported Write (Switch activity)
* dcsazure_Databricks_to_Databricks_mask_df/DatabricksSource (dataFlow)
* dcsazure_Databricks_to_Databricks_copy_df/DatabricksSource (dataFlow)

`Azure Data Lake Storage` (sink) - Linked service associated with Databricks sink data. This will be used for the
following steps:
* dcsazure_Databricks_to_Databricks_mask_df/DatabricksSink (dataFlow)
* dcsazure_Databricks_to_Databricks_copy_df/DatabricksSink (dataFlow)

`Azure Databricks Delta Lake` (source) - Linked service associated with Databricks Delta Lake. This will be used for the
following steps:
* dcsazure_Databricks_to_Databricks_for_mask_query_ds (Azure Databricks Delta Lake dataset)
* dcsazure_Databricks_to_Databricks_copy_ds (Azure Databricks Delta Lake dataset)

`Azure SQL` (metadata) - Linked service associated with your hosted metadata store. This will be used for the following
steps:
* dcsazure_Databricks_to_Databricks_metadata_mask_ds (Azure SQL Database dataset)
* dcsazure_Databricks_to_Databricks_mask_params_df/Ruleset (dataFlow)
* dcsazure_Databricks_to_Databricks_mask_params_df/TypeMapping (dataFlow)

`REST` (DCS for Azure) - Linked service associated with calling DCS for Azure. This will be used for the following
steps:
* dcsazure_Databricks_to_Databricks_mask_df (dataFlow)

### How It Works
* Determine Sink Schema Data Path
  * Parses the output of a describe call against the sink schema to determine if the schema has a managed storage path
* Determine Sink Catalog Data Path
  * Parses the output of a describe call against the sink catalog to determine if the catalog has a managed storage path
* Set Sink Delta Path
  * Combines results from `Determine Sink Schema Data Path` and `Determine Sink Catalog Data Path` pipeline groups to
    choose the appropriate sink delta path
* Select Tables That Require Masking
  * Choose, based on what is present in the metadata data store, which tables require masking
    * For each table that requires masking:
      * Get the sink table's details
        * If the table can be written to (i.e. the Data Factory backend supports this table writer version)
          * Get the delta path for the sink table
          * Call the `dcsazure_Databricks_to_Databricks_mask_params_df` data flow to generate masking parameters
          * Call the `dcsazure_Databricks_to_Databricks_mask_df` data flow, passing in parameters as generated by
            the generate masking parameters dataflow
        * If the table cannot be written to (i.e. the Data Factory backend doesn't support this table writer version)
          * Append the table name to the `TABLES_UNABLE_TO_MASK` variable
          * Provide a `400` status error code to indicate the failure of this table (with the appropriate message)
* Select Tables Without Required Masking
  * Choose, based on what is present in the metadata data store, which tables do not require masking

### Notes
The first four steps of the masking pipeline can be skipped if you know the value of variables: `CATALOG_STORAGE_LOCATION`, `CATALOG_CONTAINER_NAME`, and `SCHEMA_STORAGE_LOCATION` (if present).
If you know these values you can define them in the Variables section and deactivate the first four steps.

To obtain these values:
* In databricks, run `DESCRIBE CATALOG EXTENDED <sink_database>;`
  * The value of `Storage Location` will look something like this: `abfss://<container_name>@<storage_account>.dfs.core.windows.net/__unitystorage/catalogs/<catalog_id>`
  * `CATALOG_STORAGE_LOCATION` is the `__unitystorage/catalogs/<catalog_id>` part
  * `CATALOG_CONTAINER_NAME` is the `<container_name>` part
* In databricks, run `DESCRIBE SCHEMA EXTENDED <sink_database>.<sink_schema>;`
  * If defined, the value of `Location` will look something like this: `abfss://<container_name>@<storage_account>.dfs.core.windows.net/__unitystorage/schemas/<schema_id>`
  * `SCHEMA_STORAGE_LOCATION` is the `__unitystorage/schemas/<schema_id>` part

### Parameters

* `P_METADATA_SCHEMA` - String - This is the schema to be used for in the self-hosted AzureSQL database for storing metadata (default `dbo`)
* `P_METADATA_RULESET_TABLE` - String - This is the table to be used for storing the discovered ruleset (default `discovered_ruleset`)
* `P_METADATA_SOURCE_TO_SINK_MAPPING_TABLE` - String - This is the table used for determining where data that is run through the pipeline starts and ends (default `adf_data_mapping`)
* `P_METADATA_ADF_TYPE_MAPPING_TABLE` - String - This is the table used for determining how Azure Data Factory should interpret data that flows through the pipeline (default `adf_type_mapping`)
* `P_COPY_UNMASKED_TABLES` - Bool - This enables the pipeline to copy data from source to destination when a mapping exists, but no algorithms have been defined (default `false`)
* `P_SOURCE_DB` - String - This is the source database (catalog) in Databricks that contains the unmasked data
* `P_SINK_DB` - String - This is the sink database (catalog) in Databricks that will serve as a destination for masked data
* `P_SOURCE_SCHEMA` - String - This is the source schema in Databricks (that lives under the specified catalog) and that contains the unmasked data
* `P_SINK_SCHEMA` - String - This is the sink schema in Databricks (that lives under the specified catalog) and that will serve as a destination for masked data
* `P_BLOB_STORE_STAGING_STORAGE_PATH` - String - This is a storage container that can be used by Databricks as a staging area
* `P_COPY_USE_DATAFLOW` - Bool - This enables the pipeline to use a data flow to copy data from source to sink when there is no data to mask (this value does not matter if `P_COPY_UNMASKED_TABLES` is `false`)

