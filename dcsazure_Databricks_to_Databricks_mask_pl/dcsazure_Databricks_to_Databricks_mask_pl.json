{
  "$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "factoryName": {
      "type": "string",
      "metadata": "Data Factory name"
    },
    "Metadata Datastore": {
      "type": "string"
    },
    "AzureDataLakeStorage_Source": {
      "type": "string"
    },
    "AzureDataLakeStorage_Sink": {
      "type": "string"
    },
    "AzureDatabricksDeltaLake_Source": {
      "type": "string"
    },
    "DCSForAzureProd": {
      "type": "string"
    }
  },
  "variables": {
    "factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
  },
  "resources": [
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_Databricks_to_Databricks_mask_pl')]",
      "type": "Microsoft.DataFactory/factories/pipelines",
      "apiVersion": "2018-06-01",
      "properties": {
        "activities": [
          {
            "name": "Select Tables That Require Masking",
            "description": "Select tables with a data mapping and assigned algorithms, as well as filters that may be needed for conditional masking.",
            "type": "Lookup",
            "dependsOn": [],
            "policy": {
              "timeout": "0.12:00:00",
              "retry": 0,
              "retryIntervalInSeconds": 30,
              "secureOutput": false,
              "secureInput": false
            },
            "userProperties": [],
            "typeProperties": {
              "source": {
                "type": "AzureSqlSource",
                "sqlReaderQuery": {
                  "value": "WITH\ntables_requiring_filters AS\n(\n    SELECT DISTINCT\n        dataset, specified_database, specified_schema, identified_table, max(row_count) as row_count\n    FROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')}\n    WHERE ISJSON(assigned_algorithm) = 1\n    AND dataset = '@{variables('DATASET')}'\n    AND specified_database = '@{pipeline().parameters.P_SOURCE_CATALOG}'\n    AND specified_schema = '@{pipeline().parameters.P_SOURCE_SCHEMA}'\n    AND row_count > 0\n    GROUP BY dataset, specified_database, specified_schema, identified_table\n),\ntables_not_requiring_filters AS\n(\n    SELECT DISTINCT\n        dataset, specified_database, specified_schema, identified_table, max(row_count) as row_count\n    FROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')}\n    WHERE ISJSON(assigned_algorithm) = 0\n    AND dataset = '@{variables('DATASET')}'\n    AND specified_database = '@{pipeline().parameters.P_SOURCE_CATALOG}'\n    AND specified_schema = '@{pipeline().parameters.P_SOURCE_SCHEMA}'\n    AND row_count > 0\n    GROUP BY dataset, specified_database, specified_schema, identified_table\n    EXCEPT\n    SELECT dataset, specified_database, specified_schema, identified_table, row_count\n    FROM tables_requiring_filters\n    WHERE dataset = '@{variables('DATASET')}'\n    AND specified_database = '@{pipeline().parameters.P_SOURCE_CATALOG}'\n    AND specified_schema = '@{pipeline().parameters.P_SOURCE_SCHEMA}'\n),\nall_filters AS\n(\n    SELECT DISTINCT\n        JSON_VALUE(kc.value, '$.alias') as filter_alias,\n        replace(JSON_VALUE(kc.value, '$.condition'), '@{variables('CONDITIONAL_MASKING_RESERVED_CHARACTER')}', concat('byName(''', d.identified_column, ''') ')) as filter_value\n    FROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')} d \n        CROSS APPLY OPENJSON(d.assigned_algorithm) kc\n    WHERE\n        ISJSON(assigned_algorithm,ARRAY) = 1\n        AND\n        identified_table in (\n            SELECT identified_table FROM tables_requiring_filters\n        )\n        AND JSON_VALUE(kc.value,'$.alias') != 'default'\n    UNION\n    SELECT DISTINCT\n        'default' as filter_alias,\n        STRING_AGG(concat('not(', replace(JSON_VALUE(kc.value, '$.condition'), '@{variables('CONDITIONAL_MASKING_RESERVED_CHARACTER')}', concat('byName(''', d.identified_column, ''')')),')'), ' && ') as filter_value\n    FROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')} d \n        CROSS APPLY OPENJSON(d.assigned_algorithm) kc\n    WHERE\n        ISJSON(assigned_algorithm,ARRAY) = 1\n        AND\n        identified_table in (\n            SELECT identified_table FROM tables_requiring_filters\n        )\n        AND JSON_VALUE(kc.value,'$.alias') != 'default'\n), sources_and_filters AS (\n    SELECT\n        dataset,\n        specified_database,\n        specified_schema,\n        identified_table,\n        filter_alias,\n        filter_value\n    FROM tables_requiring_filters CROSS JOIN all_filters\n    UNION\n    SELECT\n        dataset,\n        specified_database,\n        specified_schema,\n        identified_table,\n        '',\n        ''\n    FROM tables_not_requiring_filters\n)\nSELECT DISTINCT\n    dm.source_dataset,\n    dm.source_database,\n    dm.source_schema,\n    dm.source_table,\n    dm.sink_database,\n    dm.sink_schema,\n    dm.sink_table,\n    rs.filter_alias,\n    rs.filter_value\nFROM\n    sources_and_filters rs\n    JOIN @{variables('METADATA_SCHEMA')}.@{variables('METADATA_SOURCE_TO_SINK_MAPPING_TABLE')} dm\n    ON (\n        rs.dataset = dm.source_dataset AND\n        rs.specified_database = dm.source_database AND\n        rs.specified_schema = dm.source_schema AND\n        rs.identified_table = dm.source_table\n    )\nWHERE\n    dm.sink_database = '@{pipeline().parameters.P_SINK_CATALOG}' AND\n    dm.sink_schema = '@{pipeline().parameters.P_SINK_SCHEMA}'",
                  "type": "Expression"
                },
                "queryTimeout": "02:00:00",
                "partitionOption": "None"
              },
              "dataset": {
                "referenceName": "dcsazure_Databricks_to_Databricks_metadata_mask_ds",
                "type": "DatasetReference",
                "parameters": {
                  "DS_METADATA_SCHEMA": {
                    "value": "@variables('METADATA_SCHEMA')",
                    "type": "Expression"
                  },
                  "DS_METADATA_TABLE": {
                    "value": "@variables('METADATA_RULESET_TABLE')",
                    "type": "Expression"
                  }
                }
              },
              "firstRowOnly": false
            }
          },
          {
            "name": "For Each Table To Mask",
            "type": "ForEach",
            "dependsOn": [
              {
                "activity": "Select Tables That Require Masking",
                "dependencyConditions": [
                  "Succeeded"
                ]
              }
            ],
            "userProperties": [],
            "typeProperties": {
              "items": {
                "value": "@activity('Select Tables That Require Masking').output.value",
                "type": "Expression"
              },
              "isSequential": false,
              "activities": [
                {
                  "name": "Get Sink Table Details",
                  "type": "Lookup",
                  "dependsOn": [],
                  "policy": {
                    "timeout": "0.12:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                  },
                  "userProperties": [],
                  "typeProperties": {
                    "source": {
                      "type": "AzureDatabricksDeltaLakeSource",
                      "query": {
                        "value": "DESCRIBE DETAIL @{item().sink_database}.@{item().sink_schema}.@{item().sink_table}",
                        "type": "Expression"
                      }
                    },
                    "dataset": {
                      "referenceName": "dcsazure_Databricks_to_Databricks_for_mask_query_ds",
                      "type": "DatasetReference",
                      "parameters": {}
                    }
                  }
                },
                {
                  "name": "Check Writer Version",
                  "type": "IfCondition",
                  "dependsOn": [
                    {
                      "activity": "Get Sink Table Details",
                      "dependencyConditions": [
                        "Succeeded"
                      ]
                    },
                    {
                      "activity": "Get Source Metadata Mask",
                      "dependencyConditions": [
                        "Succeeded"
                      ]
                    }
                  ],
                  "userProperties": [],
                  "typeProperties": {
                    "expression": {
                      "value": "@greater(int(activity('Get Sink Table Details').output.firstRow.minWriterVersion),variables('writerVersion'))",
                      "type": "Expression"
                    },
                    "ifTrueActivities": [
                      {
                        "name": "Failed to mask",
                        "type": "Fail",
                        "dependsOn": [],
                        "userProperties": [],
                        "typeProperties": {
                          "message": {
                            "value": "Failed to mask @{item().source_table} as sink location @{item().sink_database}.@{item().sink_schema}.@{item().sink_table} requires an unsupported writer version @{activity('Get Sink Table Details').output.firstRow.minWriterVersion}",
                            "type": "Expression"
                          },
                          "errorCode": "400"
                        }
                      }
                    ]
                  }
                },
                {
                  "name": "Get Source Metadata Mask",
                  "type": "Lookup",
                  "dependsOn": [],
                  "policy": {
                    "timeout": "0.12:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                  },
                  "userProperties": [],
                  "typeProperties": {
                    "source": {
                      "type": "AzureSqlSource",
                      "sqlReaderQuery": {
                        "value": "SELECT TOP 1 metadata\nFROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')}\nWHERE dataset = '@{variables('DATASET')}'\nAND specified_database = '@{item().source_database}'\nAND specified_schema = '@{item().source_schema}'\nAND identified_table = '@{item().source_table}'\n",
                        "type": "Expression"
                      },
                      "queryTimeout": "02:00:00",
                      "partitionOption": "None"
                    },
                    "dataset": {
                      "referenceName": "dcsazure_Databricks_to_Databricks_metadata_mask_ds",
                      "type": "DatasetReference",
                      "parameters": {
                        "DS_METADATA_SCHEMA": {
                          "value": "@variables('METADATA_SCHEMA')",
                          "type": "Expression"
                        },
                        "DS_METADATA_TABLE": {
                          "value": "@variables('METADATA_RULESET_TABLE')",
                          "type": "Expression"
                        }
                      }
                    }
                  }
                },
                {
                  "name": "Check For Conditional Masking",
                  "description": "Identify whether the table needs to be masked with conditional masking",
                  "type": "IfCondition",
                  "dependsOn": [
                    {
                      "activity": "Get Sink Table Metadata",
                      "dependencyConditions": [
                        "Succeeded"
                      ]
                    }
                  ],
                  "userProperties": [],
                  "typeProperties": {
                    "expression": {
                      "value": "@greater(length(item().filter_alias),0)",
                      "type": "Expression"
                    },
                    "ifFalseActivities": [
                      {
                        "name": "Get Masking Parameters No Filter",
                        "description": "Get parameters needed for masking this table that has no conditional masking",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_Databricks_to_Databricks_unfiltered_mask_params_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "runId": {
                                "value": "'@{pipeline().RunId}'",
                                "type": "Expression"
                              },
                              "DF_METADATA_SCHEMA": {
                                "value": "'@{variables('METADATA_SCHEMA')}'",
                                "type": "Expression"
                              },
                              "DF_METADATA_RULESET_TABLE": {
                                "value": "'@{variables('METADATA_RULESET_TABLE')}'",
                                "type": "Expression"
                              },
                              "DF_METADATA_ADF_TYPE_MAPPING_TABLE": {
                                "value": "'@{variables('METADATA_ADF_TYPE_MAPPING_TABLE')}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_DB": {
                                "value": "'@{item().source_database}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_SCHEMA": {
                                "value": "'@{item().source_schema}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_TABLE": {
                                "value": "'@{item().source_table}'",
                                "type": "Expression"
                              },
                              "DF_COLUMN_WIDTH_ESTIMATE": "1000"
                            },
                            "datasetParameters": {
                              "Ruleset": {},
                              "TypeMapping": {},
                              "MaskingParameterOutput": {}
                            }
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "None",
                          "cacheSinks": {
                            "firstRowOnly": true
                          }
                        }
                      },
                      {
                        "name": "Perform Masking Per Table No Filter",
                        "description": "Perform masking for the entire table",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [
                          {
                            "activity": "Get Masking Parameters No Filter",
                            "dependencyConditions": [
                              "Succeeded"
                            ]
                          }
                        ],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_Databricks_to_Databricks_unfiltered_mask_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "runId": {
                                "value": "'@{pipeline().RunId}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_CONTAINER": {
                                "value": "'@{json(activity('Get Source Metadata Mask').output.firstRow.metadata).container_name}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_DELTAPATH": {
                                "value": "'@{json(activity('Get Source Metadata Mask').output.firstRow.metadata).table_location}'",
                                "type": "Expression"
                              },
                              "DF_SINK_CONTAINER": {
                                "value": "'@{json(activity('Get Sink Table Metadata').output.firstRow.metadata).container_name}'",
                                "type": "Expression"
                              },
                              "DF_SINK_DELTAPATH": {
                                "value": "'@{json(activity('Get Sink Table Metadata').output.firstRow.metadata).table_location}'",
                                "type": "Expression"
                              },
                              "DF_FIELD_ALGORITHM_ASSIGNMENT": {
                                "value": "'@{activity('Get Masking Parameters No Filter').output.runStatus.output.MaskingParameterOutput.value[0].FieldAlgorithmAssignments}'",
                                "type": "Expression"
                              },
                              "DF_COLUMNS_TO_MASK": {
                                "value": "@activity('Get Masking Parameters No Filter').output.runStatus.output.MaskingParameterOutput.value[0].ColumnsToMask",
                                "type": "Expression"
                              },
                              "DF_BODY_TYPE_MAPPING": {
                                "value": "@activity('Get Masking Parameters No Filter').output.runStatus.output.MaskingParameterOutput.value[0].DataFactoryTypeMapping",
                                "type": "Expression"
                              },
                              "DF_NUMBER_OF_BATCHES": {
                                "value": "@activity('Get Masking Parameters No Filter').output.runStatus.output.MaskingParameterOutput.value[0].NumberOfBatches",
                                "type": "Expression"
                              },
                              "DF_TRIM_LENGTHS": {
                                "value": "@activity('Get Masking Parameters No Filter').output.runStatus.output.MaskingParameterOutput.value[0].TrimLengths",
                                "type": "Expression"
                              },
                              "DF_SOURCE_TABLE": {
                                "value": "'@{item().source_table}'",
                                "type": "Expression"
                              },
                              "DF_SINK_TABLE": {
                                "value": "'@{item().sink_table}'",
                                "type": "Expression"
                              },
                              "DF_FAIL_ON_NONCONFORMANT_DATA": {
                                "value": "@pipeline().parameters.P_FAIL_ON_NONCONFORMANT_DATA",
                                "type": "Expression"
                              },
                              "DF_FIELD_DATE_FORMAT": {
                                "value": "'@{activity('Get Masking Parameters No Filter').output.runStatus.output.MaskingParameterOutput.value[0].DateFormatAssignments }'",
                                "type": "Expression"
                              }
                            },
                            "datasetParameters": {
                              "Source": {},
                              "Sink": {}
                            }
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "None",
                          "cacheSinks": {
                            "firstRowOnly": true
                          }
                        }
                      }
                    ],
                    "ifTrueActivities": [
                      {
                        "name": "Get Masking Parameters With Filter",
                        "description": "Get parameters needed for masking this table with conditional masking filter",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_Databricks_to_Databricks_filtered_mask_params_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "runId": {
                                "value": "'@{pipeline().RunId}'",
                                "type": "Expression"
                              },
                              "DF_METADATA_SCHEMA": {
                                "value": "'@{variables('METADATA_SCHEMA')}'",
                                "type": "Expression"
                              },
                              "DF_METADATA_RULESET_TABLE": {
                                "value": "'@{variables('METADATA_RULESET_TABLE')}'",
                                "type": "Expression"
                              },
                              "DF_METADATA_ADF_TYPE_MAPPING_TABLE": {
                                "value": "'@{variables('METADATA_ADF_TYPE_MAPPING_TABLE')}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_DB": {
                                "value": "'@{item().source_database}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_SCHEMA": {
                                "value": "'@{item().source_schema}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_TABLE": {
                                "value": "'@{item().source_table}'",
                                "type": "Expression"
                              },
                              "DF_COLUMN_WIDTH_ESTIMATE": "1000",
                              "DF_FILTER_KEY": {
                                "value": "'@{item().filter_alias}'",
                                "type": "Expression"
                              },
                              "DF_DATASET": {
                                "value": "'@{variables('DATASET')}'",
                                "type": "Expression"
                              }
                            },
                            "datasetParameters": {
                              "Ruleset": {},
                              "TypeMapping": {},
                              "MaskingParameterOutput": {}
                            }
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "None",
                          "cacheSinks": {
                            "firstRowOnly": true
                          }
                        }
                      },
                      {
                        "name": "Perform Masking Per Table With Filter",
                        "description": "Perform masking for the part of this table that satisfies the specified filter",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [
                          {
                            "activity": "Get Masking Parameters With Filter",
                            "dependencyConditions": [
                              "Succeeded"
                            ]
                          }
                        ],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_Databricks_to_Databricks_filtered_mask_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "runId": {
                                "value": "'@{pipeline().RunId}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_CONTAINER": {
                                "value": "'@{json(activity('Get Source Metadata Mask').output.firstRow.metadata).container_name}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_DELTAPATH": {
                                "value": "'@{json(activity('Get Source Metadata Mask').output.firstRow.metadata).table_location}'",
                                "type": "Expression"
                              },
                              "DF_SINK_CONTAINER": {
                                "value": "'@{json(activity('Get Sink Table Metadata').output.firstRow.metadata).container_name}'",
                                "type": "Expression"
                              },
                              "DF_SINK_DELTAPATH": {
                                "value": "'@{json(activity('Get Sink Table Metadata').output.firstRow.metadata).table_location}'",
                                "type": "Expression"
                              },
                              "DF_FIELD_ALGORITHM_ASSIGNMENT": {
                                "value": "'@{activity('Get Masking Parameters With Filter').output.runStatus.output.MaskingParameterOutput.value[0].FieldAlgorithmAssignments}'",
                                "type": "Expression"
                              },
                              "DF_COLUMNS_TO_MASK": {
                                "value": "@activity('Get Masking Parameters With Filter').output.runStatus.output.MaskingParameterOutput.value[0].ColumnsToMask",
                                "type": "Expression"
                              },
                              "DF_BODY_TYPE_MAPPING": {
                                "value": "@activity('Get Masking Parameters With Filter').output.runStatus.output.MaskingParameterOutput.value[0].DataFactoryTypeMapping",
                                "type": "Expression"
                              },
                              "DF_TARGET_BATCH_SIZE": {
                                "value": "@variables('TARGET_BATCH_SIZE')",
                                "type": "Expression"
                              },
                              "DF_TRIM_LENGTHS": {
                                "value": "@activity('Get Masking Parameters With Filter').output.runStatus.output.MaskingParameterOutput.value[0].TrimLengths",
                                "type": "Expression"
                              },
                              "DF_SOURCE_TABLE": {
                                "value": "'@{item().source_table}'",
                                "type": "Expression"
                              },
                              "DF_SINK_TABLE": {
                                "value": "'@{item().sink_table}'",
                                "type": "Expression"
                              },
                              "DF_FAIL_ON_NONCONFORMANT_DATA": {
                                "value": "@pipeline().parameters.P_FAIL_ON_NONCONFORMANT_DATA",
                                "type": "Expression"
                              },
                              "DF_FIELD_DATE_FORMAT": {
                                "value": "'@{activity('Get Masking Parameters With Filter').output.runStatus.output.MaskingParameterOutput.value[0].DateFormatAssignments }'",
                                "type": "Expression"
                              },
                              "DF_FILTER_CONDITION": {
                                "value": "@item().filter_value",
                                "type": "Expression"
                              }
                            },
                            "datasetParameters": {
                              "Source": {},
                              "Sink": {}
                            },
                            "linkedServiceParameters": {}
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "Fine"
                        }
                      }
                    ]
                  }
                },
                {
                  "name": "Get Sink Table Metadata",
                  "type": "Lookup",
                  "dependsOn": [
                    {
                      "activity": "Check Writer Version",
                      "dependencyConditions": [
                        "Succeeded"
                      ]
                    }
                  ],
                  "policy": {
                    "timeout": "0.12:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                  },
                  "userProperties": [],
                  "typeProperties": {
                    "source": {
                      "type": "AzureSqlSource",
                      "sqlReaderQuery": {
                        "value": "SELECT JSON_OBJECT(\n'metadata_version': 2,\n'minReaderVersion': @{activity('Get Sink Table Details').output.firstRow.minReaderVersion},\n'container_name': SUBSTRING('@{activity('Get Sink Table Details').output.firstRow.location}', 9, CHARINDEX('@','@{activity('Get Sink Table Details').output.firstRow.location}')-9),\n'table_location': SUBSTRING('@{activity('Get Sink Table Details').output.firstRow.location}',  CHARINDEX('__unitystorage/','@{activity('Get Sink Table Details').output.firstRow.location}'), len('@{activity('Get Sink Table Details').output.firstRow.location}'))\n) as metadata;",
                        "type": "Expression"
                      },
                      "queryTimeout": "02:00:00",
                      "partitionOption": "None"
                    },
                    "dataset": {
                      "referenceName": "dcsazure_Databricks_to_Databricks_metadata_mask_ds",
                      "type": "DatasetReference",
                      "parameters": {
                        "DS_METADATA_SCHEMA": {
                          "value": "@variables('METADATA_SCHEMA')",
                          "type": "Expression"
                        },
                        "DS_METADATA_TABLE": {
                          "value": "@variables('METADATA_RULESET_TABLE')",
                          "type": "Expression"
                        }
                      }
                    }
                  }
                }
              ]
            }
          },
          {
            "name": "Select Tables Without Required Masking",
            "description": "Select tables with a data mapping, and no assigned algorithms or rows.",
            "type": "Lookup",
            "dependsOn": [],
            "policy": {
              "timeout": "0.12:00:00",
              "retry": 0,
              "retryIntervalInSeconds": 30,
              "secureOutput": false,
              "secureInput": false
            },
            "userProperties": [],
            "typeProperties": {
              "source": {
                "type": "AzureSqlSource",
                "sqlReaderQuery": {
                  "value": "SELECT DISTINCT source_schema, source_database, source_table, sink_schema, sink_database, sink_table\nFROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')} dd\nJOIN @{variables('METADATA_SCHEMA')}.@{variables('METADATA_SOURCE_TO_SINK_MAPPING_TABLE')} am\nON (am.source_database = dd.specified_database AND am.source_schema = dd.specified_schema AND am.source_table = dd.identified_table)\nWHERE am.source_dataset = '@{variables('DATASET')}'\nAND am.sink_dataset = '@{variables('DATASET')}'\nAND am.source_database = '@{pipeline().parameters.P_SOURCE_CATALOG}'\nAND am.sink_database = '@{pipeline().parameters.P_SINK_CATALOG}'\nAND am.source_schema = '@{pipeline().parameters.P_SOURCE_SCHEMA}'\nAND am.sink_schema = '@{pipeline().parameters.P_SINK_SCHEMA}'\nEXCEPT\nSELECT DISTINCT source_schema, source_database, source_table, sink_schema, sink_database, sink_table\nFROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')} dd\nJOIN @{variables('METADATA_SCHEMA')}.@{variables('METADATA_SOURCE_TO_SINK_MAPPING_TABLE')} am\nON (am.source_database = dd.specified_database AND am.source_schema = dd.specified_schema AND am.source_table = dd.identified_table)\nWHERE dd.dataset = '@{variables('DATASET')}'\nAND am.source_dataset = '@{variables('DATASET')}'\nAND am.sink_dataset = '@{variables('DATASET')}'\nAND dd.assigned_algorithm IS NOT NULL\nAND dd.assigned_algorithm != ''\nAND am.source_database = '@{pipeline().parameters.P_SOURCE_CATALOG}'\nAND am.sink_database = '@{pipeline().parameters.P_SINK_CATALOG}'\nAND am.source_schema = '@{pipeline().parameters.P_SOURCE_SCHEMA}'\nAND am.sink_schema = '@{pipeline().parameters.P_SINK_SCHEMA}'\nAND dd.row_count IS NOT NULL\nAND dd.row_count > 0;",
                  "type": "Expression"
                },
                "queryTimeout": "02:00:00",
                "partitionOption": "None"
              },
              "dataset": {
                "referenceName": "dcsazure_Databricks_to_Databricks_metadata_mask_ds",
                "type": "DatasetReference",
                "parameters": {
                  "DS_METADATA_SCHEMA": {
                    "value": "@variables('METADATA_SCHEMA')",
                    "type": "Expression"
                  },
                  "DS_METADATA_TABLE": {
                    "value": "@variables('METADATA_RULESET_TABLE')",
                    "type": "Expression"
                  }
                }
              },
              "firstRowOnly": false
            }
          },
          {
            "name": "Filter If Copy Unmasked Enabled",
            "type": "Filter",
            "dependsOn": [
              {
                "activity": "Select Tables Without Required Masking",
                "dependencyConditions": [
                  "Succeeded"
                ]
              }
            ],
            "userProperties": [],
            "typeProperties": {
              "items": {
                "value": "@activity('Select Tables Without Required Masking').output.value",
                "type": "Expression"
              },
              "condition": {
                "value": "@pipeline().parameters.P_COPY_UNMASKED_TABLES",
                "type": "Expression"
              }
            }
          },
          {
            "name": "For Each Table With No Masking",
            "type": "ForEach",
            "dependsOn": [
              {
                "activity": "Filter If Copy Unmasked Enabled",
                "dependencyConditions": [
                  "Succeeded"
                ]
              }
            ],
            "userProperties": [],
            "typeProperties": {
              "items": {
                "value": "@activity('Filter If Copy Unmasked Enabled').output.value",
                "type": "Expression"
              },
              "activities": [
                {
                  "name": "Get Sink Table Details No Masking",
                  "type": "Lookup",
                  "dependsOn": [],
                  "policy": {
                    "timeout": "0.12:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                  },
                  "userProperties": [],
                  "typeProperties": {
                    "source": {
                      "type": "AzureDatabricksDeltaLakeSource",
                      "query": {
                        "value": "DESCRIBE DETAIL @{item().sink_database}.@{item().sink_schema}.@{item().sink_table}",
                        "type": "Expression"
                      }
                    },
                    "dataset": {
                      "referenceName": "dcsazure_Databricks_to_Databricks_for_mask_query_ds",
                      "type": "DatasetReference",
                      "parameters": {}
                    }
                  }
                },
                {
                  "name": "Get Sink Table Metadata No Masking",
                  "type": "Lookup",
                  "dependsOn": [
                    {
                      "activity": "Get Sink Table Details No Masking",
                      "dependencyConditions": [
                        "Succeeded"
                      ]
                    }
                  ],
                  "policy": {
                    "timeout": "0.12:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                  },
                  "userProperties": [],
                  "typeProperties": {
                    "source": {
                      "type": "AzureSqlSource",
                      "sqlReaderQuery": {
                        "value": "SELECT JSON_OBJECT(\n'metadata_version': 2,\n'minReaderVersion': @{activity('Get Sink Table Details No Masking').output.firstRow.minReaderVersion},\n'container_name': SUBSTRING('@{activity('Get Sink Table Details No Masking').output.firstRow.location}', 9, CHARINDEX('@','@{activity('Get Sink Table Details No Masking').output.firstRow.location}')-9),\n'table_location': SUBSTRING('@{activity('Get Sink Table Details No Masking').output.firstRow.location}',  CHARINDEX('__unitystorage/','@{activity('Get Sink Table Details No Masking').output.firstRow.location}'), len('@{activity('Get Sink Table Details No Masking').output.firstRow.location}'))\n) as metadata;",
                        "type": "Expression"
                      },
                      "queryTimeout": "02:00:00",
                      "partitionOption": "None"
                    },
                    "dataset": {
                      "referenceName": "dcsazure_Databricks_to_Databricks_metadata_mask_ds",
                      "type": "DatasetReference",
                      "parameters": {
                        "DS_METADATA_SCHEMA": {
                          "value": "@variables('METADATA_SCHEMA')",
                          "type": "Expression"
                        },
                        "DS_METADATA_TABLE": {
                          "value": "@variables('METADATA_RULESET_TABLE')",
                          "type": "Expression"
                        }
                      }
                    }
                  }
                },
                {
                  "name": "If Copy Write Supported",
                  "type": "IfCondition",
                  "dependsOn": [
                    {
                      "activity": "Get Sink Table Metadata No Masking",
                      "dependencyConditions": [
                        "Succeeded"
                      ]
                    },
                    {
                      "activity": "Get Source Metadata No Masking",
                      "dependencyConditions": [
                        "Succeeded"
                      ]
                    }
                  ],
                  "userProperties": [],
                  "typeProperties": {
                    "expression": {
                      "value": "@lessOrEquals(int(activity('Get Sink Table Details No Masking').output.firstRow.minWriterVersion),variables('writerVersion'))",
                      "type": "Expression"
                    },
                    "ifFalseActivities": [
                      {
                        "name": "Unable To Copy",
                        "type": "Fail",
                        "dependsOn": [],
                        "userProperties": [],
                        "typeProperties": {
                          "message": {
                            "value": "@concat('Unable to copy ', item().source_table, ' with minimum writer version ', item().minWriterVersion, ' when supported writer version is ', variables('writerVersion'))",
                            "type": "Expression"
                          },
                          "errorCode": "500"
                        }
                      }
                    ],
                    "ifTrueActivities": [
                      {
                        "name": "Call Copy Dataflow",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_Databricks_to_Databricks_copy_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "runId": {
                                "value": "'@{pipeline().RunId}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_CONTAINER": {
                                "value": "'@{json(activity('Get Source Metadata No Masking').output.firstRow.metadata).container_name}'",
                                "type": "Expression"
                              },
                              "DF_SINK_CONTAINER": {
                                "value": "'@{json(activity('Get Sink Table Metadata No Masking').output.firstRow.metadata).container_name}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_DELTAPATH": {
                                "value": "'@{json(activity('Get Source Metadata No Masking').output.firstRow.metadata).table_location}'",
                                "type": "Expression"
                              },
                              "DF_SINK_DELTAPATH": {
                                "value": "'@{json(activity('Get Sink Table Metadata No Masking').output.firstRow.metadata).table_location}'",
                                "type": "Expression"
                              }
                            },
                            "datasetParameters": {
                              "SourceData": {},
                              "SinkData": {}
                            }
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "Fine"
                        }
                      }
                    ]
                  }
                },
                {
                  "name": "Get Source Metadata No Masking",
                  "type": "Lookup",
                  "dependsOn": [],
                  "policy": {
                    "timeout": "0.12:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                  },
                  "userProperties": [],
                  "typeProperties": {
                    "source": {
                      "type": "AzureSqlSource",
                      "sqlReaderQuery": {
                        "value": "SELECT TOP 1 metadata\nFROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')}\nWHERE dataset = 'DATABRICKS'\nAND specified_database = '@{item().source_database}'\nAND specified_schema = '@{item().source_schema}'\nAND identified_table = '@{item().source_table}'",
                        "type": "Expression"
                      },
                      "queryTimeout": "02:00:00",
                      "partitionOption": "None"
                    },
                    "dataset": {
                      "referenceName": "dcsazure_Databricks_to_Databricks_metadata_mask_ds",
                      "type": "DatasetReference",
                      "parameters": {
                        "DS_METADATA_SCHEMA": {
                          "value": "@variables('METADATA_SCHEMA')",
                          "type": "Expression"
                        },
                        "DS_METADATA_TABLE": {
                          "value": "@variables('METADATA_RULESET_TABLE')",
                          "type": "Expression"
                        }
                      }
                    }
                  }
                }
              ]
            }
          },
          {
            "name": "Test Filter Conditions",
            "description": "Use data preview on Lookup \"Select Tables That Require Masking\" activity to confirm what filter conditions are to be applied. Leverage data preview on this data flow to confirm your filter is working as expected.",
            "type": "ExecuteDataFlow",
            "state": "Inactive",
            "onInactiveMarkAs": "Succeeded",
            "dependsOn": [],
            "policy": {
              "timeout": "0.12:00:00",
              "retry": 0,
              "retryIntervalInSeconds": 30,
              "secureOutput": false,
              "secureInput": false
            },
            "userProperties": [],
            "typeProperties": {
              "dataflow": {
                "referenceName": "dcsazure_Databricks_to_Databricks_filter_test_utility_df",
                "type": "DataFlowReference",
                "parameters": {
                  "runId": "''",
                  "DF_SOURCE_CONTAINER": "''",
                  "DF_SINK_CONTAINER": "''",
                  "DF_SOURCE_DELTAPATH": "''",
                  "DF_SINK_DELTAPATH": "''",
                  "DF_FILTER_CONDITION": "true()"
                },
                "datasetParameters": {
                  "SourceData": {},
                  "SinkData": {}
                }
              },
              "staging": {},
              "compute": {
                "coreCount": 8,
                "computeType": "General"
              },
              "traceLevel": "Fine"
            }
          }
        ],
        "policy": {
          "elapsedTimeMetric": {}
        },
        "parameters": {
          "P_COPY_UNMASKED_TABLES": {
            "type": "bool",
            "defaultValue": false
          },
          "P_FAIL_ON_NONCONFORMANT_DATA": {
            "type": "bool",
            "defaultValue": true
          },
          "P_SOURCE_CATALOG": {
            "type": "string"
          },
          "P_SINK_CATALOG": {
            "type": "string"
          },
          "P_SOURCE_SCHEMA": {
            "type": "string"
          },
          "P_SINK_SCHEMA": {
            "type": "string"
          }
        },
        "variables": {
          "writerVersion": {
            "type": "Integer",
            "defaultValue": 5
          },
          "readerVersion": {
            "type": "Integer",
            "defaultValue": 2
          },
          "METADATA_SCHEMA": {
            "type": "String",
            "defaultValue": "dbo"
          },
          "METADATA_RULESET_TABLE": {
            "type": "String",
            "defaultValue": "discovered_ruleset"
          },
          "METADATA_SOURCE_TO_SINK_MAPPING_TABLE": {
            "type": "String",
            "defaultValue": "adf_data_mapping"
          },
          "METADATA_ADF_TYPE_MAPPING_TABLE": {
            "type": "String",
            "defaultValue": "adf_type_mapping"
          },
          "BLOB_STORE_STAGING_STORAGE_PATH": {
            "type": "String",
            "defaultValue": "staging-container"
          },
          "DATASET": {
            "type": "String",
            "defaultValue": "DATABRICKS"
          },
          "CONDITIONAL_MASKING_RESERVED_CHARACTER": {
            "type": "String",
            "defaultValue": "%"
          },
          "TARGET_BATCH_SIZE": {
            "type": "Integer",
            "defaultValue": 2000
          }
        },
        "folder": {
          "name": "dcsazure_Databricks_to_Databricks"
        },
        "annotations": [],
        "lastPublishTime": "2024-08-08T21:06:03Z"
      },
      "dependsOn": [
        "[concat(variables('factoryId'), '/datasets/dcsazure_Databricks_to_Databricks_metadata_mask_ds')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_Databricks_to_Databricks_filter_test_utility_df')]",
        "[concat(variables('factoryId'), '/datasets/dcsazure_Databricks_to_Databricks_for_mask_query_ds')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_Databricks_to_Databricks_unfiltered_mask_params_df')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_Databricks_to_Databricks_unfiltered_mask_df')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_Databricks_to_Databricks_filtered_mask_params_df')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_Databricks_to_Databricks_filtered_mask_df')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_Databricks_to_Databricks_copy_df')]"
      ]
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_Databricks_to_Databricks_metadata_mask_ds')]",
      "type": "Microsoft.DataFactory/factories/datasets",
      "apiVersion": "2018-06-01",
      "properties": {
        "linkedServiceName": {
          "referenceName": "[parameters('Metadata Datastore')]",
          "type": "LinkedServiceReference"
        },
        "parameters": {
          "DS_METADATA_SCHEMA": {
            "type": "string"
          },
          "DS_METADATA_TABLE": {
            "type": "string"
          }
        },
        "folder": {
          "name": "dcsazure_Databricks_to_Databricks"
        },
        "annotations": [],
        "type": "AzureSqlTable",
        "schema": [
          {
            "name": "source_database",
            "type": "varchar"
          },
          {
            "name": "source_schema",
            "type": "varchar"
          },
          {
            "name": "source_table",
            "type": "varchar"
          },
          {
            "name": "source_column",
            "type": "varchar"
          },
          {
            "name": "destination_database",
            "type": "varchar"
          },
          {
            "name": "destination_schema",
            "type": "varchar"
          },
          {
            "name": "destination_table",
            "type": "varchar"
          }
        ],
        "typeProperties": {
          "schema": {
            "value": "@dataset().DS_METADATA_SCHEMA",
            "type": "Expression"
          },
          "table": {
            "value": "@dataset().DS_METADATA_TABLE",
            "type": "Expression"
          }
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_Databricks_to_Databricks_filter_test_utility_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "folder": {
          "name": "dcsazure_Databricks_to_Databricks"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Source')]",
                "type": "LinkedServiceReference"
              },
              "name": "SourceData",
              "description": "Using an inline data set, specify the source as DF_SOURCE_CONTAINER/DF_SOURCE_DELTAPATH"
            }
          ],
          "sinks": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Sink')]",
                "type": "LinkedServiceReference"
              },
              "name": "SinkData",
              "description": "Using an inline data set, specify the sink as DF_SINK_CONTAINER/DF_SINK_DELTAPATH"
            }
          ],
          "transformations": [
            {
              "name": "FilterToAppropriateRows",
              "description": "Filter base table based on supplied filter"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SINK_CONTAINER as string (''),",
            "     DF_SOURCE_DELTAPATH as string (''),",
            "     DF_SINK_DELTAPATH as string (''),",
            "     DF_FILTER_CONDITION as boolean (true())",
            "}",
            "source(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     ignoreNoFilesFound: false,",
            "     format: 'delta',",
            "     fileSystem: ($DF_SOURCE_CONTAINER),",
            "     folderPath: ($DF_SOURCE_DELTAPATH)) ~> SourceData",
            "SourceData filter($DF_FILTER_CONDITION) ~> FilterToAppropriateRows",
            "FilterToAppropriateRows sink(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'delta',",
            "     fileSystem: ($DF_SINK_CONTAINER),",
            "     folderPath: ($DF_SINK_DELTAPATH),",
            "     mergeSchema: false,",
            "     autoCompact: false,",
            "     optimizedWrite: false,",
            "     vacuum: 0,",
            "     deletable: false,",
            "     insertable: true,",
            "     updateable: false,",
            "     upsertable: false,",
            "     umask: 0022,",
            "     preCommands: [],",
            "     postCommands: [],",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SinkData"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_Databricks_to_Databricks_for_mask_query_ds')]",
      "type": "Microsoft.DataFactory/factories/datasets",
      "apiVersion": "2018-06-01",
      "properties": {
        "linkedServiceName": {
          "referenceName": "[parameters('AzureDatabricksDeltaLake_Source')]",
          "type": "LinkedServiceReference"
        },
        "folder": {
          "name": "dcsazure_Databricks_to_Databricks"
        },
        "annotations": [],
        "type": "AzureDatabricksDeltaLakeDataset",
        "typeProperties": {},
        "schema": []
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_Databricks_to_Databricks_unfiltered_mask_params_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "folder": {
          "name": "dcsazure_Databricks_to_Databricks"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('Metadata Datastore')]",
                "type": "LinkedServiceReference"
              },
              "name": "Ruleset",
              "description": "Get the ruleset table from the metadata store at DF_METADATA_SCHEMA.DF_METADATA_RULESET_TABLE"
            },
            {
              "linkedService": {
                "referenceName": "[parameters('Metadata Datastore')]",
                "type": "LinkedServiceReference"
              },
              "name": "TypeMapping",
              "description": "Get the type mapping table from the metadata store at DF_METADATA_STORE.DF_METADATA_ADF_TYPE_MAPPING_TABLE"
            }
          ],
          "sinks": [
            {
              "name": "MaskingParameterOutput",
              "description": "Sink results of computing masking parameters to activity output cache"
            }
          ],
          "transformations": [
            {
              "name": "FilterToSingleTable",
              "description": "Filter ruleset table down to the table in question by specifying dataset, specified_database, specified_schema, identified_table, and assigned_algorithm - making sure they match the dataset associated with each version of the dataflow, DF_SOURCE_DATABASE, DF_SOURCE_SCHEMA, DF_SOURCE_TABLE, and not empty or null (respectively). This filters the ruleset down to only the rules that need to be applied for masking this particular table"
            },
            {
              "name": "RulesetWithTypes",
              "description": "Join the ruleset table with the type mapping table based on the type of the column and the translation of that type to an ADF type"
            },
            {
              "name": "RulesetWithAlgorithmTypeMapping",
              "description": "Generate several columns:\n\n    output_row that always contains 1 (used later for Aggregate and Join operations)\n    adf_type_conversion that contains a string like <column_name> as <adf_type>\n    column_width_estimate that contains an integer that uses DF_COLUMN_WIDTH_ESTIMATE as the width for any column where identified_column_max_length is not positive, and identified_column_max_length plus some padding otherwise"
            },
            {
              "name": "GenerateMaskParameters",
              "description": "Grouped by output_row produce the following aggregates\n\n    FieldAlgorithmAssignments - a JSON string that maps a column name to its assigned algorithm\n    ColumnsToMask - a list of the column names that have an algorithm assigned\n    DataFactoryTypeMapping - a string that can be used by ADF to parse the output of a call to the Delphix masking endpoint, leveraging the adf_type_conversion column derived previously\n    NumberOfBatches - an integer value determined by computing the number of batches leveraging the max row_count as specified in the ruleset table, and the sum of column_width_estimate column derived previously\n    TrimLengths - a list of the actual widths of the columns so that will be used by the masking data flow to trim output before sinking"
            },
            {
              "name": "ModifyNumberOfBatches",
              "description": "Modifies the number of batches to be at least 1"
            },
            {
              "name": "FilterToDataSourceType",
              "description": "Filter type mapping table down to only the dataset in question"
            },
            {
              "name": "ParseMetadata",
              "description": "Parse the content from the metadata column that contains JSON, specifically handling parsing of known keys (i.e. date_format)"
            },
            {
              "name": "DateFormatString",
              "description": "Derive columns as necessary for handling the parsed data (i.e. consume parsed_metadata.date_format and put it in a column date_format_string), and add an output_row column that always contains 1 (used later for Aggregate and Join operations)"
            },
            {
              "name": "DateFormatHeader",
              "description": "Create DateFormatAssignment, grouped by output_row (which is always 1), generating a JSON string that maps a column to its specified date format"
            },
            {
              "name": "AllMaskingParameters",
              "description": "Perform an inner join on output_row with the output of RemoveUnnecessaryColumns - combining all computed masking parameters into the same output stream"
            },
            {
              "name": "DateFormatHeaderHandlingNulls",
              "description": "Update column DateFormatAssignments to coalesce DateFormatAssignments, and NoFormatHeader (i.e. if DateFormatAssignments is null, take NoFormatHeader, which won't be null)"
            },
            {
              "name": "RemoveUnnecessaryColumns",
              "description": "Remove intermediate columns"
            },
            {
              "name": "SplitOnDateFormat",
              "description": "Split the data into two streams, data that contains a specified date\nformat, and data that does not"
            },
            {
              "name": "NoFormatHeader",
              "description": "Create NoFormatHeader, that generates a JSON string containing an empty map when all values are null, grouped by output_row (which is always 1)"
            },
            {
              "name": "JoinDateHeaders",
              "description": "Full outer join both DateFormatHeader and NoFormatHeader where output_row = output_row"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_METADATA_SCHEMA as string ('dbo'),",
            "     DF_METADATA_RULESET_TABLE as string ('discovered_ruleset'),",
            "     DF_METADATA_ADF_TYPE_MAPPING_TABLE as string ('adf_type_mapping'),",
            "     DF_SOURCE_DB as string (''),",
            "     DF_SOURCE_SCHEMA as string (''),",
            "     DF_SOURCE_TABLE as string (''),",
            "     DF_COLUMN_WIDTH_ESTIMATE as integer (1000)",
            "}",
            "source(output(",
            "          dataset as string,",
            "          specified_database as string,",
            "          specified_schema as string,",
            "          identified_table as string,",
            "          identified_column as string,",
            "          identified_column_type as string,",
            "          identified_column_max_length as integer,",
            "          ordinal_position as integer,",
            "          row_count as long,",
            "          metadata as string,",
            "          profiled_domain as string,",
            "          profiled_algorithm as string,",
            "          confidence_score as decimal(6,5),",
            "          rows_profiled as long,",
            "          assigned_algorithm as string,",
            "          last_profiled_updated_timestamp as timestamp",
            "     ),",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'table',",
            "     store: 'sqlserver',",
            "     schemaName: ($DF_METADATA_SCHEMA),",
            "     tableName: ($DF_METADATA_RULESET_TABLE),",
            "     isolationLevel: 'READ_UNCOMMITTED') ~> Ruleset",
            "source(output(",
            "          dataset as string,",
            "          dataset_type as string,",
            "          adf_type as string",
            "     ),",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'table',",
            "     store: 'sqlserver',",
            "     schemaName: ($DF_METADATA_SCHEMA),",
            "     tableName: ($DF_METADATA_ADF_TYPE_MAPPING_TABLE),",
            "     isolationLevel: 'READ_UNCOMMITTED') ~> TypeMapping",
            "Ruleset filter(equalsIgnoreCase(dataset, 'DATABRICKS') ",
            "&& equalsIgnoreCase(specified_database, $DF_SOURCE_DB) ",
            "&& equalsIgnoreCase(specified_schema, $DF_SOURCE_SCHEMA) ",
            "&& equalsIgnoreCase(identified_table, $DF_SOURCE_TABLE)",
            "&& !equalsIgnoreCase(assigned_algorithm, '')",
            "&& !isNull(assigned_algorithm)) ~> FilterToSingleTable",
            "FilterToSingleTable, FilterToDataSourceType join(identified_column_type <=> dataset_type,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'left')~> RulesetWithTypes",
            "RulesetWithTypes derive(output_row = 1,",
            "          adf_type_conversion = concat(identified_column, ' as ', adf_type),",
            "          column_width_estimate = iif(identified_column_max_length > 0, identified_column_max_length+4, $DF_COLUMN_WIDTH_ESTIMATE)) ~> RulesetWithAlgorithmTypeMapping",
            "RulesetWithAlgorithmTypeMapping aggregate(groupBy(output_row),",
            "     FieldAlgorithmAssignments = regexReplace(reduce(mapAssociation(keyValues(collect(identified_column), collect(assigned_algorithm)), '\"' + #key + '\":\"' + #value + '\"'), '{', #acc + #item + ',', #result + '}'), ',}', '}'),",
            "          ColumnsToMask = regexReplace(reduce(collect(identified_column), '[',  #acc + '\"' + #item + '\",', #result + ']'), ',]', ']'),",
            "          DataFactoryTypeMapping = concat(\"'\", '(timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long, ',",
            "    regexReplace(reduce(collect(identified_column + ' as ' + adf_type), '', #acc + #item + ', ', #result + ')'), ', \\\\)', ')'),",
            "    '[])', \"'\"),",
            "          NumberOfBatches = toInteger(ceil(((max(row_count) * (sum(column_width_estimate) + log10(max(row_count)) +1)) / (2000000 * .9)))),",
            "          TrimLengths = regexReplace(reduce(collect(identified_column_max_length), '[',  #acc + toString(#item) + ',', toString(#result) + ']'), ',]', ']')) ~> GenerateMaskParameters",
            "GenerateMaskParameters derive(NumberOfBatches = iif(NumberOfBatches > 0, NumberOfBatches, 1)) ~> ModifyNumberOfBatches",
            "TypeMapping filter(equalsIgnoreCase(dataset, 'DATABRICKS')) ~> FilterToDataSourceType",
            "FilterToSingleTable parse(parsed_metadata = metadata ? (date_format as string),",
            "     format: 'json',",
            "     documentForm: 'singleDocument') ~> ParseMetadata",
            "ParseMetadata derive(output_row = 1,",
            "          date_format_string = parsed_metadata.date_format) ~> DateFormatString",
            "SplitOnDateFormat@ContainsDateFormat aggregate(groupBy(output_row),",
            "     DateFormatAssignments = regexReplace(reduce(mapAssociation(keyValues(collect(identified_column), collect(date_format_string)), '\"' + #key + '\":\"' + #value + '\"'), '{', #acc + #item + ',', #result + '}'), ',}', '}')) ~> DateFormatHeader",
            "ModifyNumberOfBatches, RemoveUnnecessaryColumns join(GenerateMaskParameters@output_row == RemoveUnnecessaryColumns@output_row,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> AllMaskingParameters",
            "JoinDateHeaders derive(DateFormatAssignments = coalesce(DateFormatAssignments,NoFormatHeader),",
            "          output_row = coalesce(DateFormatHeader@output_row, NoFormatHeader@output_row)) ~> DateFormatHeaderHandlingNulls",
            "DateFormatHeaderHandlingNulls select(mapColumn(",
            "          output_row,",
            "          DateFormatAssignments",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> RemoveUnnecessaryColumns",
            "DateFormatString split(not(isNull(date_format_string)),",
            "     disjoint: false) ~> SplitOnDateFormat@(ContainsDateFormat, DoesNotContainDateFormat)",
            "SplitOnDateFormat@DoesNotContainDateFormat aggregate(groupBy(output_row),",
            "     NoFormatHeader = reduce(collect(date_format_string), '{', #acc + #item + ',', #result + '}')) ~> NoFormatHeader",
            "DateFormatHeader, NoFormatHeader join(DateFormatHeader@output_row == NoFormatHeader@output_row,",
            "     joinType:'outer',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> JoinDateHeaders",
            "AllMaskingParameters sink(validateSchema: false,",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     store: 'cache',",
            "     format: 'inline',",
            "     output: true,",
            "     saveOrder: 1) ~> MaskingParameterOutput"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_Databricks_to_Databricks_unfiltered_mask_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "folder": {
          "name": "dcsazure_Databricks_to_Databricks"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Source')]",
                "type": "LinkedServiceReference"
              },
              "name": "Source",
              "description": "Select source data at DF_SOURCE_CONTAINER/DF_SOURCE_DELTAPATH using an inline dataset"
            }
          ],
          "sinks": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Sink')]",
                "type": "LinkedServiceReference"
              },
              "name": "Sink",
              "description": "Sink results of masking to data store by sinking the unrolled results of the masking call to the columns of the same name in the data sink"
            }
          ],
          "transformations": [
            {
              "name": "DCSForAzureAPI",
              "linkedService": {
                "referenceName": "[parameters('DCSForAzureProd')]",
                "type": "LinkedServiceReference"
              }
            },
            {
              "name": "AddSortKey",
              "description": "Create column DELPHIX_COMPLIANCE_SERVICE_SORT_ID that consists of SHA of the data across all columns in the table - every row will have this value and it cannot be null"
            },
            {
              "name": "SortBySortKey",
              "description": "Sort the table by the value in DELPHIX_COMPLIANCE_SERVICE_SORT_ID, as we need the table to be in a particular order before we apply a surrogate key"
            },
            {
              "name": "CreateSurrogateKey",
              "description": "Add a DELPHIX_COMPLIANCE_SERVICE_BATCH_ID column that increments by 1 and starts at 1 after applying the sorting"
            },
            {
              "name": "WrapValuesInArray",
              "description": "For each column we wish to mask, convert the value into an array, this is needed to preserve null values as null when using collect, as null values become []"
            },
            {
              "name": "SelectColumnsUnmasked",
              "description": "Select only columns that don't require masking"
            },
            {
              "name": "AggregateColumnsByBatch",
              "description": "For each column we wish to mask, aggregate to a list using collect, grouped by DELPHIX_COMPLIANCE_SERVICE_BATCH_ID modulo DF_NUMBER_OF_BATCHES - so there will be DF_NUMBER_OF_BATCHES total such aggregations, name the group as DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP"
            },
            {
              "name": "FlattenValuesOutOfArray",
              "description": "For each column we wish to mask, flatten the value out of the array, in the case where the value was previously [], it becomes null"
            },
            {
              "name": "AssertNoFailures",
              "description": "Confirm that we received a 200 response status from the API request"
            },
            {
              "name": "FlattenAggregateData",
              "description": "Unroll the API response body into named columns"
            },
            {
              "name": "JoinMaskedAndUnmaskedData",
              "description": "Inner join on SelectColumnsUnmasked and TrimMaskedStrings based on matching DELPHIX_COMPLIANCE_SERVICE_BATCH_ID"
            },
            {
              "name": "TrimMaskedStrings",
              "description": "For each column with a string type, trim the string to length based on the value in DF_TRIM_LENGTHS - this is needed as masking a string may produce a longer string that exceeds the column width in the sink"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SOURCE_DELTAPATH as string (''),",
            "     DF_SINK_CONTAINER as string (''),",
            "     DF_SINK_DELTAPATH as string (''),",
            "     DF_FIELD_ALGORITHM_ASSIGNMENT as string ('{}'),",
            "     DF_COLUMNS_TO_MASK as string[] ([\"\"]),",
            "     DF_BODY_TYPE_MAPPING as string ('(timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long)[])'),",
            "     DF_NUMBER_OF_BATCHES as integer (100),",
            "     DF_TRIM_LENGTHS as integer[] ([1000]),",
            "     DF_SOURCE_TABLE as string (''),",
            "     DF_SINK_TABLE as string (''),",
            "     DF_FAIL_ON_NONCONFORMANT_DATA as boolean (true()),",
            "     DF_FIELD_DATE_FORMAT as string ('{}')",
            "}",
            "source(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     ignoreNoFilesFound: false,",
            "     format: 'delta',",
            "     fileSystem: ($DF_SOURCE_CONTAINER),",
            "     folderPath: ($DF_SOURCE_DELTAPATH),",
            "     partitionBy('roundRobin', 32)) ~> Source",
            "FlattenValuesOutOfArray call(mapColumn(",
            "          each(match(name!='DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP'))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     output(",
            "          headers as [string,string],",
            "          body as (timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long, LAST_NAME as string, FIRST_NAME as string, EMAIL as string, PHONE as string, STAFF_ID as integer, ACTIVE as integer, STORE_ID as integer, MANAGER_ID as integer)[]),",
            "          status as string",
            "     ),",
            "     allowSchemaDrift: true,",
            "     format: 'rest',",
            "     store: 'restservice',",
            "     timeout: 300,",
            "     requestInterval: 0,",
            "     headers = ['Run-Id' -> $runId, 'Field-Algorithm-Assignment' -> $DF_FIELD_ALGORITHM_ASSIGNMENT, 'Fail-On-Non-Conformant-Data' -> iif($DF_FAIL_ON_NONCONFORMANT_DATA,'true','false'), 'Field-Date-Format' -> $DF_FIELD_DATE_FORMAT],",
            "     httpMethod: 'POST',",
            "     entity: '/v1/masking/batchMaskByColumn',",
            "     headerColumnName: 'headers',",
            "     bodyColumnName: 'body',",
            "     statusColumnName: 'status',",
            "     addResponseCode: true,",
            "     requestFormat: ['type' -> 'json'],",
            "     responseFormat: ['type' -> 'json', 'documentForm' -> 'documentPerLine'],",
            "     columnTypeMap: ['body'->'$DF_BODY_TYPE_MAPPING']) ~> DCSForAzureAPI",
            "Source derive(DELPHIX_COMPLIANCE_SERVICE_SORT_ID = sha2(256, columns())) ~> AddSortKey",
            "AddSortKey sort(asc(DELPHIX_COMPLIANCE_SERVICE_SORT_ID, false)) ~> SortBySortKey",
            "SortBySortKey keyGenerate(output(DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long),",
            "     startAt: 1L,",
            "     stepValue: 1L) ~> CreateSurrogateKey",
            "CreateSurrogateKey derive(each(match(contains($DF_COLUMNS_TO_MASK,#item==name)), $$ = array($$))) ~> WrapValuesInArray",
            "CreateSurrogateKey select(mapColumn(",
            "          each(match(!contains($DF_COLUMNS_TO_MASK,#item==name)))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SelectColumnsUnmasked",
            "WrapValuesInArray aggregate(groupBy(DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP = DELPHIX_COMPLIANCE_SERVICE_BATCH_ID%$DF_NUMBER_OF_BATCHES),",
            "     each(match(contains($DF_COLUMNS_TO_MASK,#item==name)||(name==\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\")), $$ = collect($$))) ~> AggregateColumnsByBatch",
            "AggregateColumnsByBatch derive(each(match(contains($DF_COLUMNS_TO_MASK,#item==name)||(name==\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\")), $$ = flatten($$))) ~> FlattenValuesOutOfArray",
            "DCSForAzureAPI assert(expectTrue(toInteger(regexExtract(status, '(\\\\d+)', 1)) == 200, false, 'Failed_request', null, iif(isNull(body.message), status, concatWS(', ', 'timestamp: ' + toString(body.timestamp), 'status: ' + body.status, 'message: ' + body.message, 'trace_id: ' + body.trace_id))),",
            "     abort: true) ~> AssertNoFailures",
            "AssertNoFailures foldDown(unroll(body.items),",
            "     mapColumn(",
            "          every(body.items,match(true()))",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenAggregateData",
            "SelectColumnsUnmasked, TrimMaskedStrings join(SelectColumnsUnmasked@DELPHIX_COMPLIANCE_SERVICE_BATCH_ID == FlattenAggregateData@DELPHIX_COMPLIANCE_SERVICE_BATCH_ID,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'off')~> JoinMaskedAndUnmaskedData",
            "FlattenAggregateData derive(each(match(type=='string'), $$ = substring($$, 1, $DF_TRIM_LENGTHS[toInteger($# - 1)]))) ~> TrimMaskedStrings",
            "JoinMaskedAndUnmaskedData sink(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'delta',",
            "     fileSystem: ($DF_SINK_CONTAINER),",
            "     folderPath: ($DF_SINK_DELTAPATH),",
            "     mergeSchema: false,",
            "     autoCompact: false,",
            "     optimizedWrite: false,",
            "     vacuum: 0,",
            "     deletable: false,",
            "     insertable: true,",
            "     updateable: false,",
            "     upsertable: false,",
            "     umask: 0022,",
            "     preCommands: [],",
            "     postCommands: [],",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     mapColumn(",
            "          each(match(name!=\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\"&&name!=\"DELPHIX_COMPLIANCE_SERVICE_SORT_ID\"))",
            "     )) ~> Sink"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_Databricks_to_Databricks_filtered_mask_params_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "description": "Generate masking parameters when conditional algorithms are defined",
        "folder": {
          "name": "dcsazure_Databricks_to_Databricks"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('Metadata Datastore')]",
                "type": "LinkedServiceReference"
              },
              "name": "Ruleset",
              "description": "Get the ruleset table from the metadata store at DF_METADATA_SCHEMA.DF_METADATA_RULESET_TABLE"
            },
            {
              "linkedService": {
                "referenceName": "[parameters('Metadata Datastore')]",
                "type": "LinkedServiceReference"
              },
              "name": "TypeMapping",
              "description": "Get the type mapping table from the metadata store at DF_METADATA_STORE.DF_METADATA_ADF_TYPE_MAPPING_TABLE"
            }
          ],
          "sinks": [
            {
              "name": "MaskingParameterOutput",
              "description": "Sink results of computing masking parameters to activity output cache"
            }
          ],
          "transformations": [
            {
              "name": "FilterToSingleTable",
              "description": "Filter ruleset table down to the table in question by specifying dataset, specified_database, specified_schema, identified_table, and assigned_algorithm - making sure they match the dataset associated with each version of the dataflow, DF_SOURCE_DATABASE, DF_SOURCE_SCHEMA, DF_SOURCE_TABLE, and not empty or null (respectively). This filters the ruleset down to only the rules that need to be applied for masking this particular table"
            },
            {
              "name": "RulesetWithTypes",
              "description": "Join the ruleset table with the type mapping table based on the type of the column and the translation of that type to an ADF type"
            },
            {
              "name": "RulesetWithAlgorithmTypeMapping",
              "description": "Generate several columns:\n\n    output_row that always contains 1 (used later for Aggregate and Join operations)\n    adf_type_conversion that contains a string like <column_name> as <adf_type>\n    column_width_estimate that contains an integer that uses DF_COLUMN_WIDTH_ESTIMATE as the width for any column where identified_column_max_length is not positive, and identified_column_max_length plus some padding otherwise"
            },
            {
              "name": "GenerateMaskParameters",
              "description": "Grouped by output_row produce the following aggregates\n\n    FieldAlgorithmAssignments - a JSON string that maps a column name to its assigned algorithm\n    ColumnsToMask - a list of the column names that have an algorithm assigned\n    DataFactoryTypeMapping - a string that can be used by ADF to parse the output of a call to the Delphix masking endpoint, leveraging the adf_type_conversion column derived previously\n    NumberOfBatches - an integer value determined by computing the number of batches leveraging the max row_count as specified in the ruleset table, and the sum of column_width_estimate column derived previously\n    TrimLengths - a list of the actual widths of the columns so that will be used by the masking data flow to trim output before sinking"
            },
            {
              "name": "FilterToDataSourceType",
              "description": "Filter type mapping table down to only the dataset in question"
            },
            {
              "name": "ParseMetadata",
              "description": "Parse the content from the metadata column that contains JSON, specifically handling parsing of known keys (i.e. date_format)"
            },
            {
              "name": "DateFormatString",
              "description": "Derive columns as necessary for handling the parsed data (i.e. consume parsed_metadata.date_format and put it in a column date_format_string), and add an output_row column that always contains 1 (used later for Aggregate and Join operations)"
            },
            {
              "name": "DateFormatHeader",
              "description": "Create DateFormatAssignment, grouped by output_row (which is always 1), generating a JSON string that maps a column to its specified date format"
            },
            {
              "name": "AllMaskingParameters",
              "description": "Perform an inner join on output_row with the output of RemoveUnnecessaryColumns - combining all computed masking parameters into the same output stream"
            },
            {
              "name": "DateFormatHeaderHandlingNulls",
              "description": "Update column DateFormatAssignments to coalesce DateFormatAssignments, and NoFormatHeader (i.e. if DateFormatAssignments is null, take NoFormatHeader, which won't be null)"
            },
            {
              "name": "RemoveUnnecessaryColumns",
              "description": "Remove intermediate columns"
            },
            {
              "name": "SplitOnDateFormat",
              "description": "Split the data into two streams, data that contains a specified date\nformat, and data that does not"
            },
            {
              "name": "NoFormatHeader",
              "description": "Create NoFormatHeader, that generates a JSON string containing an empty map when all values are null, grouped by output_row (which is always 1)"
            },
            {
              "name": "JoinDateHeaders",
              "description": "Full outer join both DateFormatHeader and NoFormatHeader where output_row = output_row"
            },
            {
              "name": "HandleConditionalAlgorithms",
              "description": "Conditionally distributing the data in assigned_algorithm groups, based on the type of data in assigned_algorithm"
            },
            {
              "name": "SimplifySimpleRulesetTable",
              "description": "Simplify the columns of the ruleset table for algorithms that are always applied"
            },
            {
              "name": "ParseAlgorithm",
              "description": "Parse conditional algorithm assignment"
            },
            {
              "name": "FlattenAlgorithmAssignments",
              "description": "Unroll the conditions from the conditional algorithm assignment"
            },
            {
              "name": "ParseKeyColumn",
              "description": "Parse key column conditions"
            },
            {
              "name": "FlattenKeyConditions",
              "description": "Unroll the aliases and conditions from the key column"
            },
            {
              "name": "JoinConditionalAlgorithms",
              "description": "Join the key column, its conditions, and the algorithms assigned to those conditions"
            },
            {
              "name": "FilterToConditionKey",
              "description": "Filter out rows that don't apply to this condition key, and rows that don't include an assigned algorithm"
            },
            {
              "name": "SimplifyConditionalRulesetTable",
              "description": "Rename columns in the conditional ruleset table to match the simplified ruleset table"
            },
            {
              "name": "UnionAllRules",
              "description": "Combining rows from conditional and non-conditional ruleset tables"
            },
            {
              "name": "FlattenConditionalFormatting",
              "description": "Unroll the conditions from the conditional date_format assignment"
            },
            {
              "name": "FilterUnmatchingAlias",
              "description": "Filter down to only this filer alias, as necessary"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_METADATA_SCHEMA as string ('dbo'),",
            "     DF_METADATA_RULESET_TABLE as string ('discovered_ruleset'),",
            "     DF_METADATA_ADF_TYPE_MAPPING_TABLE as string ('adf_type_mapping'),",
            "     DF_SOURCE_DB as string (''),",
            "     DF_SOURCE_SCHEMA as string (''),",
            "     DF_SOURCE_TABLE as string (''),",
            "     DF_COLUMN_WIDTH_ESTIMATE as integer (1000),",
            "     DF_FILTER_KEY as string (''),",
            "     DF_DATASET as string ('DATABRICKS')",
            "}",
            "source(output(",
            "          dataset as string,",
            "          specified_database as string,",
            "          specified_schema as string,",
            "          identified_table as string,",
            "          identified_column as string,",
            "          identified_column_type as string,",
            "          identified_column_max_length as integer,",
            "          ordinal_position as integer,",
            "          row_count as long,",
            "          metadata as string,",
            "          profiled_domain as string,",
            "          profiled_algorithm as string,",
            "          confidence_score as decimal(6,5),",
            "          rows_profiled as long,",
            "          assigned_algorithm as string,",
            "          last_profiled_updated_timestamp as timestamp",
            "     ),",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'table',",
            "     store: 'sqlserver',",
            "     schemaName: ($DF_METADATA_SCHEMA),",
            "     tableName: ($DF_METADATA_RULESET_TABLE),",
            "     isolationLevel: 'READ_UNCOMMITTED') ~> Ruleset",
            "source(output(",
            "          dataset as string,",
            "          dataset_type as string,",
            "          adf_type as string",
            "     ),",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'table',",
            "     store: 'sqlserver',",
            "     schemaName: ($DF_METADATA_SCHEMA),",
            "     tableName: ($DF_METADATA_ADF_TYPE_MAPPING_TABLE),",
            "     isolationLevel: 'READ_UNCOMMITTED') ~> TypeMapping",
            "Ruleset filter(equalsIgnoreCase(dataset, $DF_DATASET) ",
            "&& equalsIgnoreCase(specified_database, $DF_SOURCE_DB) ",
            "&& equalsIgnoreCase(specified_schema, $DF_SOURCE_SCHEMA) ",
            "&& equalsIgnoreCase(identified_table, $DF_SOURCE_TABLE)",
            "&& !equalsIgnoreCase(assigned_algorithm, '')",
            "&& !isNull(assigned_algorithm)) ~> FilterToSingleTable",
            "UnionAllRules, FilterToDataSourceType join(identified_column_type <=> dataset_type,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'left')~> RulesetWithTypes",
            "RulesetWithTypes derive(output_row = 1,",
            "          adf_type_conversion = concat(identified_column, ' as ', adf_type),",
            "          column_width_estimate = iif(identified_column_max_length > 0, identified_column_max_length+4, $DF_COLUMN_WIDTH_ESTIMATE)) ~> RulesetWithAlgorithmTypeMapping",
            "RulesetWithAlgorithmTypeMapping aggregate(groupBy(output_row),",
            "     FieldAlgorithmAssignments = regexReplace(reduce(mapAssociation(keyValues(collect(identified_column), collect(assigned_algorithm)), '\"' + #key + '\":\"' + #value + '\"'), '{', #acc + #item + ',', #result + '}'), ',}', '}'),",
            "          ColumnsToMask = regexReplace(reduce(collect(identified_column), '[',  #acc + '\"' + #item + '\",', #result + ']'), ',]', ']'),",
            "          DataFactoryTypeMapping = concat(\"'\", '(timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long, ',",
            "    regexReplace(reduce(collect(identified_column + ' as ' + adf_type), '', #acc + #item + ', ', #result + ')'), ', \\\\)', ')'),",
            "    '[])', \"'\"),",
            "          TrimLengths = regexReplace(reduce(collect(identified_column_max_length), '[',  #acc + toString(#item) + ',', toString(#result) + ']'), ',]', ']')) ~> GenerateMaskParameters",
            "TypeMapping filter(equalsIgnoreCase(dataset, $DF_DATASET)) ~> FilterToDataSourceType",
            "FilterToSingleTable parse(parsed_metadata = metadata ? (date_format as string),",
            "          conditional_formatting = metadata ? (key_column as string,",
            "          conditions as (alias as string, date_format as string)[]),",
            "     format: 'json',",
            "     documentForm: 'singleDocument') ~> ParseMetadata",
            "FilterUnmatchingAlias derive(output_row = 1,",
            "          date_format_string = coalesce(conditional_date_format, parsed_metadata.date_format)) ~> DateFormatString",
            "SplitOnDateFormat@ContainsDateFormat aggregate(groupBy(output_row),",
            "     DateFormatAssignments = regexReplace(reduce(mapAssociation(keyValues(collect(identified_column), collect(date_format_string)), '\"' + #key + '\":\"' + #value + '\"'), '{', #acc + #item + ',', #result + '}'), ',}', '}')) ~> DateFormatHeader",
            "GenerateMaskParameters, RemoveUnnecessaryColumns join(GenerateMaskParameters@output_row == RemoveUnnecessaryColumns@output_row,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> AllMaskingParameters",
            "JoinDateHeaders derive(DateFormatAssignments = coalesce(DateFormatAssignments,NoFormatHeader),",
            "          output_row = coalesce(DateFormatHeader@output_row, NoFormatHeader@output_row)) ~> DateFormatHeaderHandlingNulls",
            "DateFormatHeaderHandlingNulls select(mapColumn(",
            "          output_row,",
            "          DateFormatAssignments",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> RemoveUnnecessaryColumns",
            "DateFormatString split(not(isNull(date_format_string)),",
            "     disjoint: false) ~> SplitOnDateFormat@(ContainsDateFormat, DoesNotContainDateFormat)",
            "SplitOnDateFormat@DoesNotContainDateFormat aggregate(groupBy(output_row),",
            "     NoFormatHeader = reduce(collect(date_format_string), '{', #acc + #item + ',', #result + '}')) ~> NoFormatHeader",
            "DateFormatHeader, NoFormatHeader join(DateFormatHeader@output_row == NoFormatHeader@output_row,",
            "     joinType:'outer',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> JoinDateHeaders",
            "FilterToSingleTable split(like(assigned_algorithm, '[%]'),",
            "     like(assigned_algorithm, '{%}'),",
            "     disjoint: false) ~> HandleConditionalAlgorithms@(KeyColumn, ConditionalAlgorithm, StandardAlgorithm)",
            "HandleConditionalAlgorithms@StandardAlgorithm select(mapColumn(",
            "          dataset,",
            "          specified_database,",
            "          specified_schema,",
            "          identified_table,",
            "          identified_column,",
            "          identified_column_type,",
            "          identified_column_max_length,",
            "          row_count,",
            "          rows_profiled,",
            "          assigned_algorithm",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SimplifySimpleRulesetTable",
            "HandleConditionalAlgorithms@ConditionalAlgorithm parse(conditional_algorithm = assigned_algorithm ? (key_column as string,",
            "          conditions as (alias as string, algorithm as string)[]),",
            "     format: 'json',",
            "     documentForm: 'singleDocument') ~> ParseAlgorithm",
            "ParseAlgorithm foldDown(unroll(conditional_algorithm.conditions),",
            "     mapColumn(",
            "          dataset,",
            "          specified_database,",
            "          specified_schema,",
            "          identified_table,",
            "          identified_column,",
            "          identified_column_type,",
            "          identified_column_max_length,",
            "          row_count,",
            "          assigned_algorithm,",
            "          key_column = conditional_algorithm.key_column,",
            "          alias = conditional_algorithm.conditions.alias,",
            "          algorithm = conditional_algorithm.conditions.algorithm",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenAlgorithmAssignments",
            "HandleConditionalAlgorithms@KeyColumn parse(conditions_set = assigned_algorithm ? (alias as string,          condition as string)[],",
            "     format: 'json',",
            "     documentForm: 'arrayOfDocuments') ~> ParseKeyColumn",
            "ParseKeyColumn foldDown(unroll(conditions_set),",
            "     mapColumn(",
            "          dataset,",
            "          specified_database,",
            "          specified_schema,",
            "          identified_table,",
            "          identified_column,",
            "          identified_column_type,",
            "          identified_column_max_length,",
            "          row_count,",
            "          assigned_algorithm,",
            "          alias = conditions_set.alias,",
            "          condition = conditions_set.condition",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenKeyConditions",
            "FlattenKeyConditions, FlattenAlgorithmAssignments join(FlattenKeyConditions@identified_column == key_column",
            "     && FlattenKeyConditions@alias == FlattenAlgorithmAssignments@alias,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> JoinConditionalAlgorithms",
            "JoinConditionalAlgorithms filter(equalsIgnoreCase(FlattenAlgorithmAssignments@alias, $DF_FILTER_KEY) && not(isNull(algorithm)) && algorithm != '') ~> FilterToConditionKey",
            "FilterToConditionKey select(mapColumn(",
            "          dataset = FlattenAlgorithmAssignments@dataset,",
            "          specified_database = FlattenAlgorithmAssignments@specified_database,",
            "          specified_schema = FlattenAlgorithmAssignments@specified_schema,",
            "          identified_table = FlattenAlgorithmAssignments@identified_table,",
            "          identified_column = FlattenAlgorithmAssignments@identified_column,",
            "          identified_column_type = FlattenAlgorithmAssignments@identified_column_type,",
            "          identified_column_max_length = FlattenAlgorithmAssignments@identified_column_max_length,",
            "          row_count = FlattenAlgorithmAssignments@row_count,",
            "          assigned_algorithm = algorithm",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SimplifyConditionalRulesetTable",
            "SimplifyConditionalRulesetTable, SimplifySimpleRulesetTable union(byName: true)~> UnionAllRules",
            "ParseMetadata foldDown(unroll(conditional_formatting.conditions),",
            "     mapColumn(",
            "          dataset,",
            "          specified_database,",
            "          specified_schema,",
            "          identified_table,",
            "          identified_column,",
            "          identified_column_type,",
            "          identified_column_max_length,",
            "          ordinal_position,",
            "          row_count,",
            "          metadata,",
            "          profiled_domain,",
            "          profiled_algorithm,",
            "          confidence_score,",
            "          rows_profiled,",
            "          assigned_algorithm,",
            "          last_profiled_updated_timestamp,",
            "          parsed_metadata,",
            "          conditional_formatting_key_column = conditional_formatting.key_column,",
            "          alias = conditional_formatting.conditions.alias,",
            "          conditional_date_format = conditional_formatting.conditions.date_format",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenConditionalFormatting",
            "FlattenConditionalFormatting filter(alias == $DF_FILTER_KEY || isNull(alias)) ~> FilterUnmatchingAlias",
            "AllMaskingParameters sink(validateSchema: false,",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     store: 'cache',",
            "     format: 'inline',",
            "     output: true,",
            "     saveOrder: 1) ~> MaskingParameterOutput"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_Databricks_to_Databricks_filtered_mask_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "description": "Perform masking on a subset of the data that matches the specified filter",
        "folder": {
          "name": "dcsazure_Databricks_to_Databricks"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Source')]",
                "type": "LinkedServiceReference"
              },
              "name": "Source",
              "description": "Select source data at DF_SOURCE_CONTAINER/DF_SOURCE_DELTAPATH using an inline dataset"
            }
          ],
          "sinks": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Sink')]",
                "type": "LinkedServiceReference"
              },
              "name": "Sink",
              "description": "Sink results of masking to data store by sinking the unrolled results of the masking call to the columns of the same name in the data sink"
            }
          ],
          "transformations": [
            {
              "name": "DCSForAzureAPI",
              "linkedService": {
                "referenceName": "[parameters('DCSForAzureProd')]",
                "type": "LinkedServiceReference"
              }
            },
            {
              "name": "AddSortKey",
              "description": "Create column DELPHIX_COMPLIANCE_SERVICE_SORT_ID that consists of SHA of the data across all columns in the table - every row will have this value and it cannot be null"
            },
            {
              "name": "SortBySortKey",
              "description": "Sort the table by the value in DELPHIX_COMPLIANCE_SERVICE_SORT_ID, as we need the table to be in a particular order before we apply a surrogate key"
            },
            {
              "name": "CreateSurrogateKey",
              "description": "Add a DELPHIX_COMPLIANCE_SERVICE_BATCH_ID column that increments by 1 and starts at 1 after applying the sorting"
            },
            {
              "name": "WrapValuesInArray",
              "description": "For each column we wish to mask, convert the value into an array, this is needed to preserve null values as null when using collect, as null values become []"
            },
            {
              "name": "SelectColumnsUnmasked",
              "description": "Select only columns that don't require masking"
            },
            {
              "name": "AggregateColumnsByBatch",
              "description": "For each column we wish to mask, aggregate to a list using collect, grouped by DELPHIX_COMPLIANCE_SERVICE_BATCH_ID modulo DF_NUMBER_OF_BATCHES - so there will be DF_NUMBER_OF_BATCHES total such aggregations, name the group as DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP"
            },
            {
              "name": "FlattenValuesOutOfArray",
              "description": "For each column we wish to mask, flatten the value out of the array, in the case where the value was previously [], it becomes null"
            },
            {
              "name": "AssertNoFailures",
              "description": "Confirm that we received a 200 response status from the API request"
            },
            {
              "name": "FlattenAggregateData",
              "description": "Unroll the API response body into named columns"
            },
            {
              "name": "JoinMaskedAndUnmaskedData",
              "description": "Inner join on SelectColumnsUnmasked and TrimMaskedStrings based on matching DELPHIX_COMPLIANCE_SERVICE_BATCH_ID"
            },
            {
              "name": "TrimMaskedStrings",
              "description": "For each column with a string type, trim the string to length based on the value in DF_TRIM_LENGTHS - this is needed as masking a string may produce a longer string that exceeds the column width in the sink"
            },
            {
              "name": "ApplyTableFilter",
              "description": "Filter base table based on supplied filter"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SOURCE_DELTAPATH as string (''),",
            "     DF_SINK_CONTAINER as string (''),",
            "     DF_SINK_DELTAPATH as string (''),",
            "     DF_FIELD_ALGORITHM_ASSIGNMENT as string ('{}'),",
            "     DF_COLUMNS_TO_MASK as string[] ([\"\"]),",
            "     DF_BODY_TYPE_MAPPING as string ('(timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long)[])'),",
            "     DF_TARGET_BATCH_SIZE as integer (2000),",
            "     DF_TRIM_LENGTHS as integer[] ([1000]),",
            "     DF_SOURCE_TABLE as string (''),",
            "     DF_SINK_TABLE as string (''),",
            "     DF_FAIL_ON_NONCONFORMANT_DATA as boolean (true()),",
            "     DF_FIELD_DATE_FORMAT as string ('{}'),",
            "     DF_FILTER_CONDITION as boolean (true())",
            "}",
            "source(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     ignoreNoFilesFound: false,",
            "     format: 'delta',",
            "     fileSystem: ($DF_SOURCE_CONTAINER),",
            "     folderPath: ($DF_SOURCE_DELTAPATH),",
            "     partitionBy('roundRobin', 32)) ~> Source",
            "FlattenValuesOutOfArray call(mapColumn(",
            "          each(match(name!='DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP'))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     output(",
            "          headers as [string,string],",
            "          body as (timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long, LAST_NAME as string, FIRST_NAME as string, EMAIL as string, PHONE as string, STAFF_ID as integer, ACTIVE as integer, STORE_ID as integer, MANAGER_ID as integer)[]),",
            "          status as string",
            "     ),",
            "     allowSchemaDrift: true,",
            "     format: 'rest',",
            "     store: 'restservice',",
            "     timeout: 300,",
            "     requestInterval: 0,",
            "     headers = ['Run-Id' -> $runId, 'Field-Algorithm-Assignment' -> $DF_FIELD_ALGORITHM_ASSIGNMENT, 'Fail-On-Non-Conformant-Data' -> iif($DF_FAIL_ON_NONCONFORMANT_DATA,'true','false'), 'Field-Date-Format' -> $DF_FIELD_DATE_FORMAT],",
            "     httpMethod: 'POST',",
            "     entity: '/v1/masking/batchMaskByColumn',",
            "     headerColumnName: 'headers',",
            "     bodyColumnName: 'body',",
            "     statusColumnName: 'status',",
            "     addResponseCode: true,",
            "     requestFormat: ['type' -> 'json'],",
            "     responseFormat: ['type' -> 'json', 'documentForm' -> 'documentPerLine'],",
            "     columnTypeMap: ['body'->'$DF_BODY_TYPE_MAPPING']) ~> DCSForAzureAPI",
            "ApplyTableFilter derive(DELPHIX_COMPLIANCE_SERVICE_SORT_ID = sha2(256, columns())) ~> AddSortKey",
            "AddSortKey sort(asc(DELPHIX_COMPLIANCE_SERVICE_SORT_ID, false)) ~> SortBySortKey",
            "SortBySortKey keyGenerate(output(DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long),",
            "     startAt: 1L,",
            "     stepValue: 1L) ~> CreateSurrogateKey",
            "CreateSurrogateKey derive(each(match(contains($DF_COLUMNS_TO_MASK,#item==name)), $$ = array($$))) ~> WrapValuesInArray",
            "CreateSurrogateKey select(mapColumn(",
            "          each(match(!contains($DF_COLUMNS_TO_MASK,#item==name)))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SelectColumnsUnmasked",
            "WrapValuesInArray aggregate(groupBy(DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP = toInteger(ceil(DELPHIX_COMPLIANCE_SERVICE_BATCH_ID/($DF_TARGET_BATCH_SIZE/size($DF_COLUMNS_TO_MASK))))),",
            "     each(match(contains($DF_COLUMNS_TO_MASK,#item==name)||(name==\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\")), $$ = collect($$))) ~> AggregateColumnsByBatch",
            "AggregateColumnsByBatch derive(each(match(contains($DF_COLUMNS_TO_MASK,#item==name)||(name==\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\")), $$ = flatten($$))) ~> FlattenValuesOutOfArray",
            "DCSForAzureAPI assert(expectTrue(toInteger(regexExtract(status, '(\\\\d+)', 1)) == 200, false, 'Failed_request', null, iif(isNull(body.message), status, concatWS(', ', 'timestamp: ' + toString(body.timestamp), 'status: ' + body.status, 'message: ' + body.message, 'trace_id: ' + body.trace_id))),",
            "     abort: true) ~> AssertNoFailures",
            "AssertNoFailures foldDown(unroll(body.items),",
            "     mapColumn(",
            "          every(body.items,match(true()))",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenAggregateData",
            "SelectColumnsUnmasked, TrimMaskedStrings join(SelectColumnsUnmasked@DELPHIX_COMPLIANCE_SERVICE_BATCH_ID == FlattenAggregateData@DELPHIX_COMPLIANCE_SERVICE_BATCH_ID,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'off')~> JoinMaskedAndUnmaskedData",
            "FlattenAggregateData derive(each(match(type=='string'), $$ = substring($$, 1, $DF_TRIM_LENGTHS[toInteger($# - 1)]))) ~> TrimMaskedStrings",
            "Source filter($DF_FILTER_CONDITION) ~> ApplyTableFilter",
            "JoinMaskedAndUnmaskedData sink(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'delta',",
            "     fileSystem: ($DF_SINK_CONTAINER),",
            "     folderPath: ($DF_SINK_DELTAPATH),",
            "     mergeSchema: false,",
            "     autoCompact: false,",
            "     optimizedWrite: false,",
            "     vacuum: 0,",
            "     deletable: false,",
            "     insertable: true,",
            "     updateable: false,",
            "     upsertable: false,",
            "     umask: 0022,",
            "     preCommands: [],",
            "     postCommands: [],",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     mapColumn(",
            "          each(match(name!=\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\"&&name!=\"DELPHIX_COMPLIANCE_SERVICE_SORT_ID\"))",
            "     )) ~> Sink"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_Databricks_to_Databricks_copy_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "folder": {
          "name": "dcsazure_Databricks_to_Databricks"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Source')]",
                "type": "LinkedServiceReference"
              },
              "name": "SourceData",
              "description": "Using an inline data set, specify the source as DF_SOURCE_CONTAINER/DF_SOURCE_DELTAPATH"
            }
          ],
          "sinks": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Sink')]",
                "type": "LinkedServiceReference"
              },
              "name": "SinkData",
              "description": "Using an inline data set, specify the sink as DF_SINK_CONTAINER/DF_SINK_DELTAPATH"
            }
          ],
          "transformations": [],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SINK_CONTAINER as string (''),",
            "     DF_SOURCE_DELTAPATH as string (''),",
            "     DF_SINK_DELTAPATH as string ('')",
            "}",
            "source(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     ignoreNoFilesFound: false,",
            "     format: 'delta',",
            "     fileSystem: ($DF_SOURCE_CONTAINER),",
            "     folderPath: ($DF_SOURCE_DELTAPATH)) ~> SourceData",
            "SourceData sink(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'delta',",
            "     fileSystem: ($DF_SINK_CONTAINER),",
            "     folderPath: ($DF_SINK_DELTAPATH),",
            "     mergeSchema: false,",
            "     autoCompact: false,",
            "     optimizedWrite: false,",
            "     vacuum: 0,",
            "     deletable: false,",
            "     insertable: true,",
            "     updateable: false,",
            "     upsertable: false,",
            "     umask: 0022,",
            "     preCommands: [],",
            "     postCommands: [],",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SinkData"
          ]
        }
      },
      "dependsOn": []
    }
  ]
}
