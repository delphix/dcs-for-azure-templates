# dcsazure_Dataverse_to_Dataverse_in_place_mask_pl
## Delphix Compliance Services (DCS) for Azure - Dataverse to Dataverse in place Masking Pipeline

This pipeline will perform masking of your Dataverse environment.

### Prerequisites

1. Configure the hosted metadata database and associated Azure SQL service.
2. Configure the DCS for Azure REST service.
3. Configure the Dataverse linked service. 
   * It is helpful for the linked service to be parameterized with the following parameter:
      * `LS_ORG_NAME` - Environment name in the linked service.

### Importing
There are several linked services that will need to be selected in order to perform the masking of your Dataverse environment.

These linked services types are needed for the following steps:

`Dataverse` (source/sink) - Linked service associated with Dataverse data. This will be used for the
following steps:
*	dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_df/Source (DataFlow)
*	dcsazure_Dataverse_to_Dataverse_in_place_filtered_mask_df/Source (DataFlow)
*	dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_df/Sink (DataFlow)
*	dcsazure_Dataverse_to_Dataverse_in_place_filtered_mask_df/Sink (DataFlow)

`Azure SQL` (metadata) - Linked service associated with your hosted metadata store. This will be used for the following steps:
* Check For Conditional Masking (If Condition activity)
* Check If We Should Reapply Mapping (If Condition activity)
* Configure Masked Status (Script activity)
* dcsazure_Dataverse_to_Dataverse_in_place_mask_metadata_ds (Azure SQL Database Dataset)
* dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_params_df/Ruleset (DataFlow)
* dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_params_df/TypeMapping (DataFlow)
* dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_params_df/RemoveColumns (DataFlow)
* dcsazure_Dataverse_to_Dataverse_in_place_filtered_params_df/Ruleset (DataFlow)
* dcsazure_Dataverse_to_Dataverse_in_place_filtered_params_df/TypeMapping (DataFlow)
* dcsazure_Dataverse_to_Dataverse_in_place_filtered_params_df/RemoveColumns (DataFlow)


`REST` (DCS for Azure) - Linked service associated with calling DCS for Azure. This will be used for the following steps:
* dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_df (DataFlow)
* dcsazure_Dataverse_to_Dataverse_in_place_filtered_mask_df (DataFlow)


### How It Works

* Check If We Should Reapply Mapping
  * If we should, Mark Table Mapping Incomplete. This is done by updating the metadata store to indicate that tables have not had their mapping applied
* Select Tables That Require Masking
  * This is done by querying the metadata store. This will provided a list of tables that need masking, and if they need to be masked leveraging conditional algorithms, the set of required filters.
* Configure Masked Status
  * Initializes the `masked_status` column for each table with available filters.
* For Each Table To Mask
    * Check if the table must be masked with a filter condition
      * If no filter needs to be applied:
        * Call the `dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_params_df` data flow to generate masking parameters
        * Call the `dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_df` data flow, passing in parameters as generated by the generate masking parameters dataflow
        * Update the mapped status based on the success of this dataflow, and fail accordingly
      * If a filter needs to be applied:
        * Call the `dcsazure_Dataverse_to_Dataverse_in_place_filtered_mask_params_df` data flow to generate masking parameters using the filter alias
        * Call the `dcsazure_Dataverse_to_Dataverse_in_place_filtered_mask_df` data flow, passing in parameters as generated by the generate masking parameters dataflow and the filter as determined by the output of For Each Table To Mask
        * Update the mapped status based on the success of this dataflow, and fail accordingly

### Variables

If you have configured your database using the metadata store scripts, these variables will not need editing. If you have customized your metadata store, then these variables may need editing.

* `METADATA_SCHEMA` - This is the schema to be used for in the self-hosted AzureSQL database for storing metadata (default `dbo`)
* `METADATA_RULESET_TABLE` - This is the table to be used for storing the discovered ruleset (default `discovered_ruleset`)
* `METADATA_SOURCE_TO_SINK_MAPPING_TABLE` - This is the table in the metadata schema that will contain the data
  mapping, defining where unmasked data lives, and where masked data should go (default `adf_data_mapping`)
* `METADATA_ADF_TYPE_MAPPING_TABLE` - This is the table that maps from data types in various datasets to the
  associated datatype required in ADF as needed for the pipeline (default `adf_type_mapping`)
* `BLOB_STORE_STAGING_STORAGE_PATH` - This is a path that specifies where we should stage data as it moves through the pipeline and should reference a storage container in a storage account (default `staging-container`)
* `DATASET` - This is used to identify data that belongs to this pipeline in the metadata store (default `DATAVERSE`)
* `CONDITIONAL_MASKING_RESERVED_CHARACTER` - This is a string (preferably a character) reserved as for shorthand for when referring to the key column when defining filter conditions, in the pipeline this will be expanded out to use the ADF syntax for referencing the key column (default `%`)
* `TARGET_BATCH_SIZE` - This is the target number of rows per batch (default `2000`)
* `METADATA_EVENT_PROCEDURE_NAME` - This is the name of the procedure used to capture pipeline information in the metadata data store and sets the masked and mapping states on the items processed during execution (default `insert_adf_masking_event`).
* `METADATA_EVENTS_LOG_TABLE` - This is the table to log pipeline run information in the metadata data store (default `adf_events_log`)

### Parameters

* `P_FAIL_ON_NONCONFORMANT_DATA` - Bool - This will fail the pipeline if non-conformant data is encountered (default `true`)
* `P_ORG_NAME` - String - Dataverse environment name (used for both source and sink)
* `P_REAPPLY_MAPPING`- Bool - If `true`, reset previous mapping/masking results (default: `true`)

### Notes

In the dataflow `dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_df`, the parameters `DF_CREATED_BEFORE` and `DF_CREATED_AFTER` are set by default to include the entire table, with `DF_CREATED_BEFORE` resolving to the current date at runtime `(toDate(toString(currentDate()), 'yyyy-MM-dd'))` and `DF_CREATED_AFTER` set to `1753-01-01`. This configuration ensures that all records in the table are masked.

### Performance and Scalability

* For tables with millions of records, it is recommended to split the data into smaller batches by adjusting `DF_CREATED_BEFORE` and `DF_CREATED_AFTER`.

* Automatic batching is not supported. Users must manually define date ranges to manage batch sizes effectively.

* There is no strict row limit while splitting the data, but to ensure manageable runtime, it is recommended to partition the dataset into chunks of up to 1 million records each by `createdon` date ranges and run multiple masking jobs for very large tables.

* Ensure the date ranges do not overlap while splitting to avoid re-masking.

* When adjusting `DF_CREATED_BEFORE` and `DF_CREATED_AFTER`, ensure correct date formats (yyyy-MM-dd) to prevent unexpected behavior or missing rows.

* For partitioned runs, execute the pipeline multiple times for the same table, updating the `DF_CREATED_BEFORE` and `DF_CREATED_AFTER` parameters in `dcsazure_Dataverse_to_Dataverse_in_place_unfiltered_mask_df` for each batch.