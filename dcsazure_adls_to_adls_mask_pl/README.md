# dcsazure_adls_to_adls_mask_pl
## Delphix Compliance Services (DCS) for Azure - ADLS to ADLS Masking Pipeline

This pipeline will perform masking of data from your Azure Data Lake (ADLS) Data from one location to another.

### Prerequisites

1. Configure the hosted metadata database and associated Azure SQL service (version `V2024.04.18.0`+).
1. Configure the DCS for Azure REST service.
1. Configure the Azure Data Lake Storage service associated with your ADLS source data.
1. Configure the Azure Data Lake Storage service associated with your ADLS sink data.


### Importing
There are several linked services that will need to be selected in order to perform the masking of your ADLS
instance.

These linked services types are needed for the following steps:

`Azure Data Lake Storage` (source) - Linked service associated with ADLS source data. This will be used for the
following steps:
* Switch On Copy Methodology And Supported Write (Switch activity)
* dcsazure_adls_to_adls_mask_df/adlsSource (dataFlow)
* dcsazure_adls_to_adls_copy_df/adlsSource (dataFlow)

`Azure Data Lake Storage` (sink) - Linked service associated with ADLS sink data. This will be used for the
following steps:
* dcsazure_adls_to_adls_mask_df/adlsSink (dataFlow)
* dcsazure_adls_to_adls_copy_df/adlsSink (dataFlow)

`Azure ADLS ` (source) - Linked service associated with ADLS. This will be used for the
following steps:
* dcsazure_adls_to_adls_for_mask_query_ds (Azure adls  dataset)
* dcsazure_adls_to_adls_copy_ds (Azure adls  dataset)

`Azure SQL` (metadata) - Linked service associated with your hosted metadata store. This will be used for the following
steps:
* dcsazure_adls_to_adls_metadata_mask_ds (Azure SQL Database dataset)
* dcsazure_adls_to_adls_mask_params_df/Ruleset (dataFlow)
* dcsazure_adls_to_adls_mask_params_df/TypeMapping (dataFlow)

`REST` (DCS for Azure) - Linked service associated with calling DCS for Azure. This will be used for the following
steps:
* dcsazure_adls_to_adls_mask_df (dataFlow)

### How It Works
* Select Tables That Require Masking
    * Choose, based on what is present in the metadata data store, which tables require masking
      * For each table that requires masking:
        * Call the `dcsazure_adls_to_adls_mask_params_df` data flow to generate masking parameters
        * Call the `dcsazure_adls_to_adls_mask_df` data flow, passing in parameters as generated by
          the generate masking parameters dataflow
* Select Tables Without Required Masking
    * Choose, based on what is present in the metadata data store, which tables do not require masking
      * For each table that does not require masking, perform a copy activity.

### Notes
ADLS does not provide us the number of rows in files and as the files are text based, there is also no restriction on
the size of each of the columns. Normally, we would use both of those pieces of information to compute the number of
batches that are required in order to mask a table. As a result of this information being unavailable, sometimes the
computed number of batches (which defaults to `1`) will be too low and masking requests will hit request size limits.

To remedy this, you can define a number of rows for the row count in `discovered_ruleset` for tables that won't mask
due to request size limits.

The computation for the batch count is as follows:
```
ceiling (((maximum row count for this table)*(number of columns in this table with a masking algorithm)) / 100))
```
For any computation that produces a batch count less than `1`, `1` will be used.

This means, that the number of batches will be defined based on the number of columns that need to be masked and the
number of rows in the table.

For example, consider a table with 10 columns. The number of batches will depend on how many rows there are in the table
and how many of those 10 columns are to be masked (note that the number of rows represents an inclusive range).

| number of rows | number of masked columns | number of batches |
|----------------|--------------------------|-------------------|
| [0-100]        | 1                        | 1                 |
| [101-200]      | 1                        | 2                 |
| [201-300]      | 1                        | 3                 |
| [301-400]      | 1                        | 4                 |
| [401-500]      | 1                        | 5                 |
| [0-50]         | 2                        | 1                 |
| [51-100]       | 2                        | 2                 |
| [101-150]      | 2                        | 3                 |
| [151-200]      | 2                        | 4                 |
| [201-250]      | 2                        | 5                 |
| [0-20]         | 5                        | 1                 |
| [21-40]        | 5                        | 2                 |
| [41-60]        | 5                        | 3                 |
| [61-80]        | 5                        | 4                 |
| [81-100]       | 5                        | 5                 |

Note that while this will take the maximum row count, it is recommended to keep the row counts for individual file types
the same. If, for example, you found that one batch was insufficient for masking a particular table, then you can update
the row count in the data store using a query similar to the following:

```sql
UPDATE discovered_ruleset
SET row_count = <desired-row-count>
WHERE dataset= 'ADLS'
	AND specified_database = '<adls-container>'
	AND specified_schema = '<subdirectory-and-optional-prefix>'
	AND identified_table = '<suffix>';
```


### Parameters

* `P_METADATA_SCHEMA` - String - This is the schema to be used for in the self-hosted AzureSQL database for storing metadata (default `dbo`)
* `P_METADATA_RULESET_TABLE` - String - This is the table to be used for storing the discovered ruleset (default `discovered_ruleset`)
* `P_METADATA_SOURCE_TO_SINK_MAPPING_TABLE` - String - This is the table used for determining where data that is run through the pipeline starts and ends (default `adf_data_mapping`)
* `P_METADATA_ADF_TYPE_MAPPING_TABLE` - String - This is the table used for determining how Azure Data Factory should interpret data that flows through the pipeline (default `adf_type_mapping`)
* `P_COPY_UNMASKED_TABLES` - Bool - This enables the pipeline to copy data from source to destination when a mapping exists, but no algorithms have been defined (default `false`)
* `P_SOURCE_DB` - String - This is the source directory in ADLS that contains the unmasked data
* `P_SINK_DB` - String - This is the sink directory in ADLS that will serve as a destination for masked data
* `P_SOURCE_SCHEMA` - String - This is the source schema in ADLS and was discovered during the profiling pipeline run - it will have a format that defines the prefix of a file name in ADLS (with its full location) and that contains the unmasked data
* `P_SINK_SCHEMA` - String - This is the sink schema in ADLS that will have a structure similar to the source schema name, except that anything after the last `/` is ignored and that will serve as a destination for masked data (file names will be preserved)
* `P_BLOB_STORE_STAGING_STORAGE_PATH` - String - This is a storage container that can be used by ADLS as a staging area
* `P_COPY_USE_DATAFLOW` - Bool - This enables the pipeline to use a data flow to copy data from source to sink when there is no data to mask (this value does not matter if `P_COPY_UNMASKED_TABLES` is `false`)

