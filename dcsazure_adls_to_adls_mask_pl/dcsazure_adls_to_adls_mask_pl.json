{
  "$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "factoryName": {
      "type": "string",
      "metadata": "Data Factory name"
    },
    "Metadata Datastore": {
      "type": "string"
    },
    "AzureDataLakeStorage_Source": {
      "type": "string"
    },
    "AzureDataLakeStorage_Sink": {
      "type": "string"
    },
    "ProdDCSForAzureService": {
      "type": "string"
    }
  },
  "variables": {
    "factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
  },
  "resources": [
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_adls_to_adls_mask_pl')]",
      "type": "Microsoft.DataFactory/factories/pipelines",
      "apiVersion": "2018-06-01",
      "properties": {
        "activities": [
          {
            "name": "Select Tables That Require Masking",
            "description": "Select tables with a data mapping and assigned algorithms, as well as filters that may be needed for conditional masking.",
            "type": "Lookup",
            "dependsOn": [],
            "policy": {
              "timeout": "0.12:00:00",
              "retry": 0,
              "retryIntervalInSeconds": 30,
              "secureOutput": false,
              "secureInput": false
            },
            "userProperties": [],
            "typeProperties": {
              "source": {
                "type": "AzureSqlSource",
                "sqlReaderQuery": {
                  "value": "WITH\ntables_requiring_filters AS\n(\n    SELECT DISTINCT\n        dataset, specified_database, specified_schema, identified_table\n    FROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')}\n    WHERE ISJSON(assigned_algorithm) = 1\n    AND dataset = '@{variables('DATASET')}'\n    AND specified_database = '@{pipeline().parameters.P_SOURCE_CONTAINER}'\n    AND specified_schema LIKE '@{pipeline().parameters.P_SOURCE_DIRECTORY}%'\n),\ntables_not_requiring_filters AS\n(\n    SELECT DISTINCT\n        dataset, specified_database, specified_schema, identified_table\n    FROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')}\n    WHERE ISJSON(assigned_algorithm) = 0\n    AND dataset = '@{variables('DATASET')}'\n    AND specified_database = '@{pipeline().parameters.P_SOURCE_CONTAINER}'\n    AND specified_schema LIKE '@{pipeline().parameters.P_SOURCE_DIRECTORY}%'\n    EXCEPT\n    SELECT dataset, specified_database, specified_schema, identified_table\n    FROM tables_requiring_filters\n    WHERE dataset = '@{variables('DATASET')}'\n    AND specified_database = '@{pipeline().parameters.P_SOURCE_CONTAINER}'\n    AND specified_schema LIKE '@{pipeline().parameters.P_SOURCE_DIRECTORY}%'\n),\nall_filters AS\n(\n    SELECT DISTINCT\n        JSON_VALUE(kc.value, '$.alias') as filter_alias,\n        replace(JSON_VALUE(kc.value, '$.condition'), '@{variables('CONDITIONAL_MASKING_RESERVED_CHARACTER')}', concat('byName(''', d.identified_column, ''') ')) as filter_value\n    FROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')} d \n        CROSS APPLY OPENJSON(d.assigned_algorithm) kc\n    WHERE\n        ISJSON(assigned_algorithm,ARRAY) = 1\n        AND\n        identified_table in (\n            SELECT identified_table FROM tables_requiring_filters\n        )\n        AND JSON_VALUE(kc.value,'$.alias') != 'default'\n    UNION\n    SELECT DISTINCT\n        'default' as filter_alias,\n        STRING_AGG(concat('not(', replace(JSON_VALUE(kc.value, '$.condition'), '@{variables('CONDITIONAL_MASKING_RESERVED_CHARACTER')}', concat('byName(''', d.identified_column, ''')')),')'), ' && ') as filter_value\n    FROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')} d \n        CROSS APPLY OPENJSON(d.assigned_algorithm) kc\n    WHERE\n        ISJSON(assigned_algorithm,ARRAY) = 1\n        AND\n        identified_table in (\n            SELECT identified_table FROM tables_requiring_filters\n        )\n        AND JSON_VALUE(kc.value,'$.alias') != 'default'\n), sources_and_filters AS (\n    SELECT\n        dataset,\n        specified_database,\n        specified_schema,\n        identified_table,\n        filter_alias,\n        filter_value\n    FROM tables_requiring_filters CROSS JOIN all_filters\n    UNION\n    SELECT\n        dataset,\n        specified_database,\n        specified_schema,\n        identified_table,\n        '',\n        ''\n    FROM tables_not_requiring_filters\n)\nSELECT DISTINCT\n    dm.source_dataset,\n    dm.source_database,\n    dm.source_schema,\n    REVERSE(SUBSTRING(REVERSE(dm.source_schema), CHARINDEX('/', REVERSE(dm.source_schema)), LEN(dm.source_schema))) AS source_directory,\n    REVERSE(SUBSTRING(REVERSE(dm.source_schema),0, CHARINDEX('/', REVERSE(dm.source_schema)))) AS source_prefix,\n    dm.source_table,\n    dm.sink_database,\n    dm.sink_schema,\n    REVERSE(SUBSTRING(REVERSE(dm.sink_schema), CHARINDEX('/', REVERSE(dm.sink_schema)), LEN(dm.sink_schema))) AS sink_directory,\n    REVERSE(SUBSTRING(REVERSE(dm.source_schema),0, CHARINDEX('/', REVERSE(dm.source_schema)))) AS sink_prefix,\n    dm.sink_table,\n    rs.filter_alias,\n    rs.filter_value\nFROM\n    sources_and_filters rs\n    JOIN @{variables('METADATA_SCHEMA')}.@{variables('METADATA_SOURCE_TO_SINK_MAPPING_TABLE')} dm\n    ON (\n        rs.dataset = dm.source_dataset AND\n        rs.specified_database = dm.source_database AND\n        rs.specified_schema = dm.source_schema AND\n        rs.identified_table = dm.source_table\n    )\nWHERE\n    dm.sink_database = '@{pipeline().parameters.P_SINK_CONTAINER}'\n    AND dm.sink_schema = '@{pipeline().parameters.P_SINK_DIRECTORY}'",
                  "type": "Expression"
                },
                "queryTimeout": "02:00:00",
                "partitionOption": "None"
              },
              "dataset": {
                "referenceName": "dcsazure_adls_to_adls_metadata_mask_ds",
                "type": "DatasetReference",
                "parameters": {
                  "DS_METADATA_SCHEMA": {
                    "value": "@variables('METADATA_SCHEMA')",
                    "type": "Expression"
                  },
                  "DS_METADATA_TABLE": {
                    "value": "@variables('METADATA_RULESET_TABLE')",
                    "type": "Expression"
                  }
                }
              },
              "firstRowOnly": false
            }
          },
          {
            "name": "For Each Table To Mask",
            "type": "ForEach",
            "dependsOn": [
              {
                "activity": "Select Tables That Require Masking",
                "dependencyConditions": [
                  "Succeeded"
                ]
              }
            ],
            "userProperties": [],
            "typeProperties": {
              "items": {
                "value": "@activity('Select Tables That Require Masking').output.value",
                "type": "Expression"
              },
              "isSequential": false,
              "activities": [
                {
                  "name": "Get Source Metadata Mask",
                  "type": "Lookup",
                  "dependsOn": [],
                  "policy": {
                    "timeout": "0.12:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                  },
                  "userProperties": [],
                  "typeProperties": {
                    "source": {
                      "type": "AzureSqlSource",
                      "sqlReaderQuery": {
                        "value": "SELECT TOP 1 metadata\nFROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')}\nWHERE dataset = '@{variables('DATASET')}'\nAND specified_database = '@{item().source_database}'\nAND specified_schema = '@{item().source_schema}'\nAND identified_table = '@{item().source_table}'\n",
                        "type": "Expression"
                      },
                      "queryTimeout": "02:00:00",
                      "partitionOption": "None"
                    },
                    "dataset": {
                      "referenceName": "dcsazure_adls_to_adls_metadata_mask_ds",
                      "type": "DatasetReference",
                      "parameters": {
                        "DS_METADATA_SCHEMA": {
                          "value": "@variables('METADATA_SCHEMA')",
                          "type": "Expression"
                        },
                        "DS_METADATA_TABLE": {
                          "value": "@variables('METADATA_RULESET_TABLE')",
                          "type": "Expression"
                        }
                      }
                    }
                  }
                },
                {
                  "name": "Check For Conditional Masking",
                  "description": "Identify whether the table needs to be masked with conditional masking",
                  "type": "IfCondition",
                  "dependsOn": [
                    {
                      "activity": "Get Source Metadata Mask",
                      "dependencyConditions": [
                        "Succeeded"
                      ]
                    }
                  ],
                  "userProperties": [],
                  "typeProperties": {
                    "expression": {
                      "value": "@greater(length(item().filter_alias),0)",
                      "type": "Expression"
                    },
                    "ifFalseActivities": [
                      {
                        "name": "Get Masking Parameters No Filter",
                        "description": "Get parameters needed for masking this table that has no conditional masking",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_adls_to_adls_unfiltered_mask_params_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "runId": "''",
                              "DF_METADATA_SCHEMA": "'dbo'",
                              "DF_METADATA_RULESET_TABLE": "'discovered_ruleset'",
                              "DF_METADATA_ADF_TYPE_MAPPING_TABLE": "'adf_type_mapping'",
                              "DF_SOURCE_CONTAINER": "''",
                              "DF_SOURCE_SCHEMA": "''",
                              "DF_SOURCE_TABLE": "''",
                              "DF_COLUMN_WIDTH_ESTIMATE": "1000"
                            },
                            "datasetParameters": {
                              "Ruleset": {},
                              "TypeMapping": {},
                              "MaskingParameterOutput": {}
                            }
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "None",
                          "cacheSinks": {
                            "firstRowOnly": true
                          }
                        }
                      },
                      {
                        "name": "Perform Masking Per Table No Filter",
                        "description": "Perform masking for the entire table",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [
                          {
                            "activity": "Get Masking Parameters No Filter",
                            "dependencyConditions": [
                              "Succeeded"
                            ]
                          }
                        ],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_adls_to_adls_delimited_unfiltered_mask_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "runId": "''",
                              "DF_SOURCE_CONTAINER": "''",
                              "DF_SINK_CONTAINER": "''",
                              "DF_SOURCE_DIRECTORY": "''",
                              "DF_SINK_DIRECTORY": "''",
                              "DF_SOURCE_PREFIX": "''",
                              "DF_SOURCE_TABLE": "''",
                              "DF_SINK_TABLE": "''",
                              "DF_COLUMN_DELIMITER": "''",
                              "DF_QUOTE_CHARACTER": "''",
                              "DF_ESCAPE_CHARACTER": "''",
                              "DF_NULL_VALUE": "''",
                              "DF_FIELD_ALGORITHM_ASSIGNMENT": "'{}'",
                              "DF_COLUMNS_TO_MASK": [
                                ""
                              ],
                              "DF_BODY_TYPE_MAPPING": "'(timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long)[])'",
                              "DF_TRIM_LENGTHS": [
                                -1
                              ],
                              "DF_FAIL_ON_NONCONFORMANT_DATA": "true()",
                              "DF_TARGET_BATCH_SIZE": "2000",
                              "DF_FIELD_DATE_FORMAT": "'{}'"
                            },
                            "datasetParameters": {
                              "Source": {},
                              "Sink": {}
                            }
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "None",
                          "cacheSinks": {
                            "firstRowOnly": true
                          }
                        }
                      }
                    ],
                    "ifTrueActivities": [
                      {
                        "name": "Get Masking Parameters With Filter",
                        "description": "Get parameters needed for masking this table with the conditional masking filter",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_adls_to_adls_filtered_mask_params_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "DF_FILTER_KEY": {
                                "value": "'@{item().filter_alias}'",
                                "type": "Expression"
                              },
                              "DF_DATASET": {
                                "value": "'@{variables('DATASET')}'",
                                "type": "Expression"
                              }
                            },
                            "datasetParameters": {
                              "Ruleset": {},
                              "TypeMapping": {},
                              "MaskingParameterOutput": {}
                            }
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "None",
                          "cacheSinks": {
                            "firstRowOnly": true
                          }
                        }
                      },
                      {
                        "name": "Perform Masking Per Table With Filter",
                        "description": "Perform masking for the part of this table that satisfies the specified filter",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [
                          {
                            "activity": "Get Masking Parameters With Filter",
                            "dependencyConditions": [
                              "Succeeded"
                            ]
                          }
                        ],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_adls_to_adls_delimited_filtered_mask_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "DF_FILTER_CONDITION": {
                                "value": "@item().filter_value",
                                "type": "Expression"
                              }
                            },
                            "datasetParameters": {
                              "Source": {},
                              "Sink": {}
                            }
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "Fine"
                        }
                      }
                    ]
                  }
                }
              ]
            }
          },
          {
            "name": "Select Tables Without Required Masking",
            "description": "Select tables with a data mapping, and no assigned algorithms.",
            "type": "Lookup",
            "dependsOn": [],
            "policy": {
              "timeout": "0.12:00:00",
              "retry": 0,
              "retryIntervalInSeconds": 30,
              "secureOutput": false,
              "secureInput": false
            },
            "userProperties": [],
            "typeProperties": {
              "source": {
                "type": "AzureSqlSource",
                "sqlReaderQuery": {
                  "value": "SELECT DISTINCT\nsource_database,\nsource_schema,\nREVERSE(SUBSTRING(REVERSE(source_schema), CHARINDEX('/', REVERSE(source_schema)), LEN(source_schema))) AS source_directory,\nREVERSE(SUBSTRING(REVERSE(source_schema),0, CHARINDEX('/', REVERSE(source_schema)))) AS source_prefix,\nsource_table,\nsink_database,\nsink_schema,\nREVERSE(SUBSTRING(REVERSE(sink_schema), CHARINDEX('/', REVERSE(sink_schema)), LEN(sink_schema))) AS sink_directory,\nREVERSE(SUBSTRING(REVERSE(source_schema),0, CHARINDEX('/', REVERSE(source_schema)))) AS sink_prefix,\nsink_table\nFROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')} rs\nJOIN @{variables('METADATA_SCHEMA')}.@{variables('METADATA_SOURCE_TO_SINK_MAPPING_TABLE')} am\nON (am.source_database = rs.specified_database AND am.source_schema = rs.specified_schema AND am.source_table = rs.identified_table)\nWHERE rs.dataset = '@{variables('DATASET')}'\nAND am.source_dataset = '@{variables('DATASET')}'\nAND am.sink_dataset = '@{variables('DATASET')}'\nAND am.source_database = '@{pipeline().parameters.P_SOURCE_CONTAINER}'\nAND am.sink_database = '@{pipeline().parameters.P_SINK_CONTAINER}'\nAND am.source_schema LIKE '@{pipeline().parameters.P_SOURCE_DIRECTORY}%'\nAND am.sink_schema LIKE '@{pipeline().parameters.P_SINK_DIRECTORY}%'\nEXCEPT\nSELECT DISTINCT\nsource_database,\nsource_schema,\nREVERSE(SUBSTRING(REVERSE(source_schema), CHARINDEX('/', REVERSE(source_schema)), LEN(source_schema))) AS source_directory,\nREVERSE(SUBSTRING(REVERSE(source_schema),0, CHARINDEX('/', REVERSE(source_schema)))) AS source_prefix,\nsource_table,\nsink_database,\nsink_schema,\nREVERSE(SUBSTRING(REVERSE(sink_schema), CHARINDEX('/', REVERSE(sink_schema)), LEN(sink_schema))) AS sink_directory,\nREVERSE(SUBSTRING(REVERSE(source_schema),0, CHARINDEX('/', REVERSE(source_schema)))) AS sink_prefix,\nsink_table\nFROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')} rs\nJOIN @{variables('METADATA_SCHEMA')}.@{variables('METADATA_SOURCE_TO_SINK_MAPPING_TABLE')} am\nON (am.source_database = rs.specified_database AND am.source_schema = rs.specified_schema AND am.source_table = rs.identified_table)\nWHERE rs.dataset = '@{variables('DATASET')}'\nAND rs.assigned_algorithm IS NOT NULL\nAND rs.assigned_algorithm != ''\nAND am.source_dataset = '@{variables('DATASET')}'\nAND am.sink_dataset = '@{variables('DATASET')}'\nAND am.source_database = '@{pipeline().parameters.P_SOURCE_CONTAINER}'\nAND am.sink_database = '@{pipeline().parameters.P_SINK_CONTAINER}'\nAND am.source_schema LIKE '@{pipeline().parameters.P_SOURCE_DIRECTORY}%'\nAND am.sink_schema LIKE '@{pipeline().parameters.P_SINK_DIRECTORY}%';",
                  "type": "Expression"
                },
                "queryTimeout": "02:00:00",
                "partitionOption": "None"
              },
              "dataset": {
                "referenceName": "dcsazure_adls_to_adls_metadata_mask_ds",
                "type": "DatasetReference",
                "parameters": {
                  "DS_METADATA_SCHEMA": {
                    "value": "@variables('METADATA_SCHEMA')",
                    "type": "Expression"
                  },
                  "DS_METADATA_TABLE": {
                    "value": "@variables('METADATA_RULESET_TABLE')",
                    "type": "Expression"
                  }
                }
              },
              "firstRowOnly": false
            }
          },
          {
            "name": "Filter If Copy Unmasked Enabled",
            "type": "Filter",
            "dependsOn": [
              {
                "activity": "Select Tables Without Required Masking",
                "dependencyConditions": [
                  "Succeeded"
                ]
              }
            ],
            "userProperties": [],
            "typeProperties": {
              "items": {
                "value": "@activity('Select Tables Without Required Masking').output.value",
                "type": "Expression"
              },
              "condition": {
                "value": "@pipeline().parameters.P_COPY_UNMASKED_TABLES",
                "type": "Expression"
              }
            }
          },
          {
            "name": "For Each Table With No Masking",
            "type": "ForEach",
            "dependsOn": [
              {
                "activity": "Filter If Copy Unmasked Enabled",
                "dependencyConditions": [
                  "Succeeded"
                ]
              }
            ],
            "userProperties": [],
            "typeProperties": {
              "items": {
                "value": "@activity('Filter If Copy Unmasked Enabled').output.value",
                "type": "Expression"
              },
              "activities": [
                {
                  "name": "If Copy Via Dataflow",
                  "description": "Determine if we should copy using a dataflow activity or a copy activity",
                  "type": "IfCondition",
                  "dependsOn": [
                    {
                      "activity": "Get Source Metadata No Masking",
                      "dependencyConditions": [
                        "Succeeded"
                      ]
                    }
                  ],
                  "userProperties": [],
                  "typeProperties": {
                    "expression": {
                      "value": "@pipeline().parameters.P_COPY_USE_DATAFLOW",
                      "type": "Expression"
                    },
                    "ifFalseActivities": [
                      {
                        "name": "Copy Unmasked Data",
                        "type": "Copy",
                        "dependsOn": [],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "source": {
                            "type": "DelimitedTextSource",
                            "storeSettings": {
                              "type": "AzureBlobFSReadSettings",
                              "recursive": true,
                              "wildcardFolderPath": {
                                "value": "@item().source_directory",
                                "type": "Expression"
                              },
                              "wildcardFileName": {
                                "value": "@concat(item().source_prefix, '*', item().source_table)",
                                "type": "Expression"
                              },
                              "enablePartitionDiscovery": false
                            },
                            "formatSettings": {
                              "type": "DelimitedTextReadSettings"
                            }
                          },
                          "sink": {
                            "type": "DelimitedTextSink",
                            "storeSettings": {
                              "type": "AzureBlobFSWriteSettings",
                              "copyBehavior": "PreserveHierarchy"
                            },
                            "formatSettings": {
                              "type": "DelimitedTextWriteSettings",
                              "quoteAllText": true,
                              "fileExtension": ""
                            }
                          },
                          "enableStaging": false,
                          "translator": {
                            "type": "TabularTranslator",
                            "typeConversion": true,
                            "typeConversionSettings": {
                              "allowDataTruncation": true,
                              "treatBooleanAsNumber": false
                            }
                          }
                        },
                        "inputs": [
                          {
                            "referenceName": "dcsazure_adls_container_and_directory_mask_ds",
                            "type": "DatasetReference",
                            "parameters": {
                              "DS_CONTAINER": {
                                "value": "@item().source_database",
                                "type": "Expression"
                              },
                              "DS_DIRECTORY": {
                                "value": "@item().source_directory",
                                "type": "Expression"
                              }
                            }
                          }
                        ],
                        "outputs": [
                          {
                            "referenceName": "dcsazure_adls_container_and_directory_mask_ds",
                            "type": "DatasetReference",
                            "parameters": {
                              "DS_CONTAINER": {
                                "value": "@item().sink_database",
                                "type": "Expression"
                              },
                              "DS_DIRECTORY": {
                                "value": "@item().sink_directory",
                                "type": "Expression"
                              }
                            }
                          }
                        ]
                      }
                    ],
                    "ifTrueActivities": [
                      {
                        "name": "Call Copy Dataflow",
                        "type": "ExecuteDataFlow",
                        "dependsOn": [],
                        "policy": {
                          "timeout": "0.12:00:00",
                          "retry": 0,
                          "retryIntervalInSeconds": 30,
                          "secureOutput": false,
                          "secureInput": false
                        },
                        "userProperties": [],
                        "typeProperties": {
                          "dataflow": {
                            "referenceName": "dcsazure_adls_to_adls_delimited_copy_df",
                            "type": "DataFlowReference",
                            "parameters": {
                              "runId": {
                                "value": "'@{pipeline().RunId}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_CONTAINER": {
                                "value": "'@{item().source_database}'",
                                "type": "Expression"
                              },
                              "DF_SINK_CONTAINER": {
                                "value": "'@{item().sink_database}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_DIRECTORY": {
                                "value": "'@{item().source_directory}'",
                                "type": "Expression"
                              },
                              "DF_SINK_DIRECTORY": {
                                "value": "'@{item().sink_directory}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_PREFIX": {
                                "value": "'@{item().source_prefix}'",
                                "type": "Expression"
                              },
                              "DF_COLUMN_DELIMITER": {
                                "value": "'@{json(activity('Get Source Metadata No Masking').output.firstRow.metadata).column_delimiter}'",
                                "type": "Expression"
                              },
                              "DF_QUOTE_CHARACTER": {
                                "value": "'@{json(activity('Get Source Metadata No Masking').output.firstRow.metadata).quote_character}'",
                                "type": "Expression"
                              },
                              "DF_ESCAPE_CHARACTER": {
                                "value": "'@{json(activity('Get Source Metadata No Masking').output.firstRow.metadata).escape_character}'",
                                "type": "Expression"
                              },
                              "DF_NULL_VALUE": {
                                "value": "'@{json(activity('Get Source Metadata No Masking').output.firstRow.metadata).null_value}'",
                                "type": "Expression"
                              },
                              "DF_SOURCE_SUFFIX": {
                                "value": "'@{item().source_table}'",
                                "type": "Expression"
                              }
                            },
                            "datasetParameters": {
                              "Source": {},
                              "Sink": {}
                            }
                          },
                          "staging": {},
                          "compute": {
                            "coreCount": 8,
                            "computeType": "General"
                          },
                          "traceLevel": "Fine"
                        }
                      }
                    ]
                  }
                },
                {
                  "name": "Get Source Metadata No Masking",
                  "type": "Lookup",
                  "dependsOn": [],
                  "policy": {
                    "timeout": "0.12:00:00",
                    "retry": 0,
                    "retryIntervalInSeconds": 30,
                    "secureOutput": false,
                    "secureInput": false
                  },
                  "userProperties": [],
                  "typeProperties": {
                    "source": {
                      "type": "AzureSqlSource",
                      "sqlReaderQuery": {
                        "value": "SELECT TOP 1 metadata\nFROM @{variables('METADATA_SCHEMA')}.@{variables('METADATA_RULESET_TABLE')}\nWHERE dataset = '@{variables('DATASET')}'\nAND specified_database = '@{item().source_database}'\nAND specified_schema = '@{item().source_schema}'\nAND identified_table = '@{item().source_table}'",
                        "type": "Expression"
                      },
                      "queryTimeout": "02:00:00",
                      "partitionOption": "None"
                    },
                    "dataset": {
                      "referenceName": "dcsazure_adls_to_adls_metadata_mask_ds",
                      "type": "DatasetReference",
                      "parameters": {
                        "DS_METADATA_SCHEMA": {
                          "value": "@variables('METADATA_SCHEMA')",
                          "type": "Expression"
                        },
                        "DS_METADATA_TABLE": {
                          "value": "@variables('METADATA_RULESET_TABLE')",
                          "type": "Expression"
                        }
                      }
                    }
                  }
                }
              ]
            }
          },
          {
            "name": "Test Filter Condition",
            "description": "Use data preview on Lookup \"Select Tables That Require Masking\" activity to confirm what filter conditions are to be applied. Leverage data preview on this data flow to confirm your filter is working as expected.",
            "type": "ExecuteDataFlow",
            "state": "Inactive",
            "onInactiveMarkAs": "Succeeded",
            "dependsOn": [],
            "policy": {
              "timeout": "0.12:00:00",
              "retry": 0,
              "retryIntervalInSeconds": 30,
              "secureOutput": false,
              "secureInput": false
            },
            "userProperties": [],
            "typeProperties": {
              "dataflow": {
                "referenceName": "dcsazure_adls_to_adls_delimited_filter_test_utility_df",
                "type": "DataFlowReference",
                "parameters": {
                  "runId": "''",
                  "DF_SOURCE_CONTAINER": "''",
                  "DF_SINK_CONTAINER": "''",
                  "DF_SOURCE_DIRECTORY": "''",
                  "DF_SINK_DIRECTORY": "''",
                  "DF_SOURCE_PREFIX": "''",
                  "DF_COLUMN_DELIMITER": "','",
                  "DF_QUOTE_CHARACTER": "'\"'",
                  "DF_ESCAPE_CHARACTER": "'\\\\'",
                  "DF_NULL_VALUE": "''",
                  "DF_SOURCE_SUFFIX": "''",
                  "DF_FILTER_CONDITION": "true()"
                },
                "datasetParameters": {
                  "Source": {},
                  "Sink": {}
                }
              },
              "staging": {},
              "compute": {
                "coreCount": 8,
                "computeType": "General"
              },
              "traceLevel": "Fine"
            }
          }
        ],
        "policy": {
          "elapsedTimeMetric": {}
        },
        "parameters": {
          "P_COPY_UNMASKED_TABLES": {
            "type": "bool",
            "defaultValue": false
          },
          "P_COPY_USE_DATAFLOW": {
            "type": "bool",
            "defaultValue": false
          },
          "P_FAIL_ON_NONCONFORMANT_DATA": {
            "type": "bool",
            "defaultValue": true
          },
          "P_SOURCE_CONTAINER": {
            "type": "string"
          },
          "P_SINK_CONTAINER": {
            "type": "string"
          },
          "P_SOURCE_DIRECTORY": {
            "type": "string"
          },
          "P_SINK_DIRECTORY": {
            "type": "string"
          }
        },
        "variables": {
          "METADATA_SCHEMA": {
            "type": "String",
            "defaultValue": "dbo"
          },
          "METADATA_RULESET_TABLE": {
            "type": "String",
            "defaultValue": "discovered_ruleset"
          },
          "METADATA_SOURCE_TO_SINK_MAPPING_TABLE": {
            "type": "String",
            "defaultValue": "adf_data_mapping"
          },
          "METADATA_ADF_TYPE_MAPPING_TABLE": {
            "type": "String",
            "defaultValue": "adf_type_mapping"
          },
          "TARGET_BATCH_SIZE": {
            "type": "Integer",
            "defaultValue": 2000
          },
          "DATASET": {
            "type": "String",
            "defaultValue": "ADLS"
          },
          "CONDITIONAL_MASKING_RESERVED_CHARACTER": {
            "type": "String",
            "defaultValue": "%"
          }
        },
        "folder": {
          "name": "dcsazure_adls_to_adls"
        },
        "annotations": [],
        "lastPublishTime": "2024-07-25T15:56:23Z"
      },
      "dependsOn": [
        "[concat(variables('factoryId'), '/datasets/dcsazure_adls_to_adls_metadata_mask_ds')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_adls_to_adls_delimited_filter_test_utility_df')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_adls_to_adls_unfiltered_mask_params_df')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_adls_to_adls_delimited_unfiltered_mask_df')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_adls_to_adls_filtered_mask_params_df')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_adls_to_adls_delimited_filtered_mask_df')]",
        "[concat(variables('factoryId'), '/datasets/dcsazure_adls_container_and_directory_mask_ds')]",
        "[concat(variables('factoryId'), '/dataflows/dcsazure_adls_to_adls_delimited_copy_df')]"
      ]
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_adls_to_adls_metadata_mask_ds')]",
      "type": "Microsoft.DataFactory/factories/datasets",
      "apiVersion": "2018-06-01",
      "properties": {
        "linkedServiceName": {
          "referenceName": "[parameters('Metadata Datastore')]",
          "type": "LinkedServiceReference"
        },
        "parameters": {
          "DS_METADATA_SCHEMA": {
            "type": "string"
          },
          "DS_METADATA_TABLE": {
            "type": "string"
          }
        },
        "folder": {
          "name": "dcsazure_adls_to_adls"
        },
        "annotations": [],
        "type": "AzureSqlTable",
        "schema": [
          {
            "name": "dataset",
            "type": "varchar"
          },
          {
            "name": "specified_database",
            "type": "varchar"
          },
          {
            "name": "specified_schema",
            "type": "varchar"
          },
          {
            "name": "identified_table",
            "type": "varchar"
          },
          {
            "name": "identified_column",
            "type": "varchar"
          },
          {
            "name": "identified_column_type",
            "type": "varchar"
          },
          {
            "name": "identified_column_max_length",
            "type": "int",
            "precision": 10
          },
          {
            "name": "ordinal_position",
            "type": "int",
            "precision": 10
          },
          {
            "name": "row_count",
            "type": "bigint",
            "precision": 19
          },
          {
            "name": "metadata",
            "type": "nvarchar"
          },
          {
            "name": "profiled_domain",
            "type": "varchar"
          },
          {
            "name": "profiled_algorithm",
            "type": "varchar"
          },
          {
            "name": "confidence_score",
            "type": "decimal",
            "precision": 6,
            "scale": 5
          },
          {
            "name": "rows_profiled",
            "type": "bigint",
            "precision": 19
          },
          {
            "name": "assigned_algorithm",
            "type": "varchar"
          },
          {
            "name": "last_profiled_updated_timestamp",
            "type": "datetime",
            "precision": 23,
            "scale": 3
          }
        ],
        "typeProperties": {
          "schema": {
            "value": "@dataset().DS_METADATA_SCHEMA",
            "type": "Expression"
          },
          "table": {
            "value": "@dataset().DS_METADATA_TABLE",
            "type": "Expression"
          }
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_adls_to_adls_delimited_filter_test_utility_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "folder": {
          "name": "dcsazure_adls_to_adls"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Source')]",
                "type": "LinkedServiceReference"
              },
              "name": "Source",
              "description": "Select source data in source container DF_SOURCE_CONTAINER, and wildcard that is constructed based on DF_SOURCE_DIRECTORY, DF_SOURCE_PREFIX, and DF_SOURCE_TABLE using an inline dataset, with DF_COLUMN_DELIMITER, DF_QUOTE_CHARACTER, DF_ESCAPE_CHARACTER, and DF_NULL_VALUE as specified in the parameters, and storing the file name in DELPHIX_COMPLIANCE_SERVICE_FILE_NAME"
            }
          ],
          "sinks": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Sink')]",
                "type": "LinkedServiceReference"
              },
              "name": "Sink",
              "description": "Sink results of masking to data store by sinking all columns but DELPHIX_COMPLIANCE_SERVICE_BATCH_ID, DELPHIX_COMPLIANCE_SERVICE_SORT_ID, and DELPHIX_COMPLIANCE_SERVICE_FILE_NAME to the data sink, naming the file as DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME, and using the same metadata settings as were used in the source with DF_COLUMN_DELIMITER, DF_QUOTE_CHARACTER, DF_ESCAPE_CHARACTER, and DF_NULL_VALUE"
            }
          ],
          "transformations": [
            {
              "name": "CreateSinkFileName",
              "description": "Create column DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME by replacing DF_SOURCE_DIRECTORY with DF_SINK_DIRECTORY in the value in DELPHIX_COMPLIANCE_SERVICE_FILE_NAME"
            },
            {
              "name": "FilterToAppropriateRows",
              "description": "Filter base table based on supplied filter"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SINK_CONTAINER as string (''),",
            "     DF_SOURCE_DIRECTORY as string (''),",
            "     DF_SINK_DIRECTORY as string (''),",
            "     DF_SOURCE_PREFIX as string (''),",
            "     DF_COLUMN_DELIMITER as string (','),",
            "     DF_QUOTE_CHARACTER as string ('\"'),",
            "     DF_ESCAPE_CHARACTER as string ('\\\\'),",
            "     DF_NULL_VALUE as string (''),",
            "     DF_SOURCE_SUFFIX as string (''),",
            "     DF_FILTER_CONDITION as boolean (true())",
            "}",
            "source(useSchema: false,",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     ignoreNoFilesFound: false,",
            "     rowUrlColumn: 'DELPHIX_COMPLIANCE_SERVICES_FILE_NAME',",
            "     format: 'delimited',",
            "     fileSystem: ($DF_SOURCE_CONTAINER),",
            "     columnDelimiter: ($DF_COLUMN_DELIMITER),",
            "     escapeChar: ($DF_ESCAPE_CHARACTER),",
            "     quoteChar: ($DF_QUOTE_CHARACTER),",
            "     nullValue: ($DF_NULL_VALUE),",
            "     columnNamesAsHeader: true,",
            "     wildcardPaths:[(concat($DF_SOURCE_DIRECTORY,iif(length($DF_SOURCE_PREFIX) == 0,iif(equals('NO_EXT',$DF_SOURCE_SUFFIX),'*',concat('*',$DF_SOURCE_SUFFIX)),iif(equals('NO_EXT',$DF_SOURCE_SUFFIX),concat($DF_SOURCE_PREFIX,'*'),concat($DF_SOURCE_PREFIX,'*',$DF_SOURCE_SUFFIX)))))]) ~> Source",
            "FilterToAppropriateRows derive(DELPHIX_COMPLIANCE_SERVICES_SINK_FILE_NAME = replace(DELPHIX_COMPLIANCE_SERVICES_FILE_NAME, $DF_SOURCE_DIRECTORY, $DF_SINK_DIRECTORY)) ~> CreateSinkFileName",
            "Source filter($DF_FILTER_CONDITION) ~> FilterToAppropriateRows",
            "CreateSinkFileName sink(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'delimited',",
            "     fileSystem: ($DF_SINK_DATABASE),",
            "     folderPath: ($DF_SINK_DIRECTORY),",
            "     columnDelimiter: ($DF_COLUMN_DELIMITER),",
            "     escapeChar: ($DF_ESCAPE_CHARACTER),",
            "     quoteChar: ($DF_QUOTE_CHARACTER),",
            "     nullValue: ($DF_NULL_VALUE),",
            "     columnNamesAsHeader: true,",
            "     rowUrlColumn:'DELPHIX_COMPLIANCE_SERVICES_SINK_FILE_NAME',",
            "     umask: 0022,",
            "     preCommands: [],",
            "     postCommands: [],",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     mapColumn(",
            "          each(match(name!=\"DELPHIX_COMPLIANCE_SERVICES_FILE_NAME\"&&name!=\"DELPHIX_COMPLIANCE_SERVICES_SINK_FILE_NAME\"))",
            "     )) ~> Sink"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_adls_to_adls_unfiltered_mask_params_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "folder": {
          "name": "dcsazure_adls_to_adls"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('Metadata Datastore')]",
                "type": "LinkedServiceReference"
              },
              "name": "Ruleset",
              "description": "Get the ruleset table from the metadata store at DF_METADATA_SCHEMA.DF_METADATA_RULESET_TABLE"
            },
            {
              "linkedService": {
                "referenceName": "[parameters('Metadata Datastore')]",
                "type": "LinkedServiceReference"
              },
              "name": "TypeMapping",
              "description": "Get the type mapping table from the metadata store at DF_METADATA_STORE.DF_METADATA_ADF_TYPE_MAPPING_TABLE"
            }
          ],
          "sinks": [
            {
              "name": "MaskingParameterOutput",
              "description": "Sink results of computing masking parameters to activity output cache"
            }
          ],
          "transformations": [
            {
              "name": "FilterToSingleTable",
              "description": "Filter ruleset table down to the table in question by specifying dataset, specified_database, specified_schema, identified_table, and assigned_algorithm - making sure they match the dataset associated with each version of the dataflow, DF_SOURCE_DATABASE, DF_SOURCE_SCHEMA, DF_SOURCE_TABLE, and not empty or null (respectively). This filters the ruleset down to only the rules that need to be applied for masking this particular table"
            },
            {
              "name": "RulesetWithTypes",
              "description": "Join the ruleset table with the type mapping table based on the type of the column and the translation of that type to an ADF type"
            },
            {
              "name": "RulesetWithAlgorithmTypeMapping",
              "description": "Generate several columns:\n\n    output_row that always contains 1 (used later for Aggregate and Join operations)\n    adf_type_conversion that contains a string like <column_name> as <adf_type>\n    column_width_estimate that contains an integer that uses DF_COLUMN_WIDTH_ESTIMATE as the width for any column where identified_column_max_length is not positive, and identified_column_max_length plus some padding otherwise"
            },
            {
              "name": "GenerateMaskParameters",
              "description": "Grouped by output_row produce the following aggregates\n\n    FieldAlgorithmAssignments - a JSON string that maps a column name to its assigned algorithm\n    ColumnsToMask - a list of the column names that have an algorithm assigned\n    DataFactoryTypeMapping - a string that can be used by ADF to parse the output of a call to the Delphix masking endpoint, leveraging the adf_type_conversion column derived previously\n    NumberOfBatches - an integer value determined by computing the number of batches leveraging the max row_count as specified in the ruleset table, and the sum of column_width_estimate column derived previously\n    TrimLengths - a list of the actual widths of the columns so that will be used by the masking data flow to trim output before sinking"
            },
            {
              "name": "ModifyNumberOfBatches",
              "description": "Modifies the number of batches to be at least 1"
            },
            {
              "name": "FilterAdlsType",
              "description": "Filter type mapping table down to only the dataset in question"
            },
            {
              "name": "ParseMetadata",
              "description": "Parse the content from the metadata column that contains JSON, specifically handling parsing of known keys (i.e. date_format)"
            },
            {
              "name": "DateFormatHeader",
              "description": "Create DateFormatAssignment, grouped by output_row (which is always 1), generating a JSON string that maps a column to its specified date format"
            },
            {
              "name": "DateFormatString",
              "description": "Derive columns as necessary for handling the parsed data (i.e. consume parsed_metadata.date_format and put it in a column date_format_string), and add an output_row column that always contains 1 (used later for Aggregate and Join operations)"
            },
            {
              "name": "AllMaskingParameters",
              "description": "Perform an inner join on output_row with the computed DateFormatHeaders - combining all masking parameters into the same output stream"
            },
            {
              "name": "RemoveUnnecessaryColumns",
              "description": "Remove intermediate columns"
            },
            {
              "name": "SplitOnDateFormat",
              "description": "Split the data into two streams, data that contains a specified date\nformat, and data that does not"
            },
            {
              "name": "NoFormatHeader",
              "description": "Create NoFormatHeader, that generates a JSON string containing an empty map when all values are null, grouped by output_row (which is always 1)"
            },
            {
              "name": "JoinDateHeaders",
              "description": "Full outer join both DateFormatHeader and NoFormatHeader where output_row = output_row"
            },
            {
              "name": "DateFormatHeaderHandlingNulls",
              "description": "Update column DateFormatAssignments to coalesce DateFormatAssignments, and NoFormatHeader (i.e. if DateFormatAssignments is null, take NoFormatHeader, which won't be null), similarly with output_row"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_METADATA_SCHEMA as string ('dbo'),",
            "     DF_METADATA_RULESET_TABLE as string ('discovered_ruleset'),",
            "     DF_METADATA_ADF_TYPE_MAPPING_TABLE as string ('adf_type_mapping'),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SOURCE_SCHEMA as string (''),",
            "     DF_SOURCE_TABLE as string (''),",
            "     DF_COLUMN_WIDTH_ESTIMATE as integer (1000)",
            "}",
            "source(output(",
            "          dataset as string,",
            "          specified_database as string,",
            "          specified_schema as string,",
            "          identified_table as string,",
            "          identified_column as string,",
            "          identified_column_type as string,",
            "          identified_column_max_length as integer,",
            "          ordinal_position as integer,",
            "          row_count as long,",
            "          metadata as string,",
            "          profiled_domain as string,",
            "          profiled_algorithm as string,",
            "          confidence_score as decimal(6,5),",
            "          rows_profiled as long,",
            "          assigned_algorithm as string,",
            "          last_profiled_updated_timestamp as timestamp",
            "     ),",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'table',",
            "     store: 'sqlserver',",
            "     schemaName: ($DF_METADATA_SCHEMA),",
            "     tableName: ($DF_METADATA_RULESET_TABLE),",
            "     isolationLevel: 'READ_UNCOMMITTED') ~> Ruleset",
            "source(output(",
            "          dataset as string,",
            "          dataset_type as string,",
            "          adf_type as string",
            "     ),",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'table',",
            "     store: 'sqlserver',",
            "     schemaName: ($DF_METADATA_SCHEMA),",
            "     tableName: ($DF_METADATA_ADF_TYPE_MAPPING_TABLE),",
            "     isolationLevel: 'READ_UNCOMMITTED') ~> TypeMapping",
            "Ruleset filter(equalsIgnoreCase(dataset, 'ADLS') ",
            "&& equalsIgnoreCase(specified_database, $DF_SOURCE_CONTAINER) ",
            "&& equalsIgnoreCase(specified_schema, $DF_SOURCE_SCHEMA) ",
            "&& equalsIgnoreCase(identified_table, $DF_SOURCE_TABLE)",
            "&& !equalsIgnoreCase(assigned_algorithm, '')",
            "&& !isNull(assigned_algorithm)) ~> FilterToSingleTable",
            "FilterToSingleTable, FilterAdlsType join(identified_column_type <=> dataset_type,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'left')~> RulesetWithTypes",
            "RulesetWithTypes derive(adf_type_conversion = concat(identified_column, ' as ', adf_type),",
            "          output_row = 1,",
            "          column_width_estimate = iif(identified_column_max_length > 0, identified_column_max_length+4, $DF_COLUMN_WIDTH_ESTIMATE)) ~> RulesetWithAlgorithmTypeMapping",
            "RulesetWithAlgorithmTypeMapping aggregate(groupBy(output_row),",
            "     FieldAlgorithmAssignments = regexReplace(reduce(mapAssociation(keyValues(collect(identified_column), collect(assigned_algorithm)), '\"' + #key + '\":\"' + #value + '\"'), '{', #acc + #item + ',', #result + '}'), ',}', '}'),",
            "          ColumnsToMask = regexReplace(reduce(collect(identified_column), '[',  #acc + '\"' + #item + '\",', #result + ']'), ',]', ']'),",
            "          DataFactoryTypeMapping = concat(\"'\", '(timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long, ',",
            "    regexReplace(reduce(collect(identified_column + ' as ' + adf_type), '', #acc + #item + ', ', #result + ')'), ', \\\\)', ')'),",
            "    '[])', \"'\"),",
            "          NumberOfBatches = toInteger(ceil(((max(row_count) * (sum(column_width_estimate) + log10(max(row_count))+1)) / (2000000 * .9)))),",
            "          TrimLengths = regexReplace(reduce(collect(identified_column_max_length), '[',  #acc + toString(#item) + ',', toString(#result) + ']'), ',]', ']')) ~> GenerateMaskParameters",
            "GenerateMaskParameters derive(NumberOfBatches = iif(NumberOfBatches > 0, NumberOfBatches, 1)) ~> ModifyNumberOfBatches",
            "TypeMapping filter(equalsIgnoreCase(dataset, 'ADLS')) ~> FilterAdlsType",
            "FilterToSingleTable parse(parsed_metadata = metadata ? (date_format as string),",
            "     format: 'json',",
            "     documentForm: 'singleDocument') ~> ParseMetadata",
            "SplitOnDateFormat@ContainsDateFormat aggregate(groupBy(output_row),",
            "     DateFormatAssignments = regexReplace(reduce(mapAssociation(keyValues(collect(identified_column), collect(date_format_string)), '\"' + #key + '\":\"' + #value + '\"'), '{', #acc + #item + ',', #result + '}'), ',}', '}')) ~> DateFormatHeader",
            "ParseMetadata derive(output_row = 1,",
            "          date_format_string = parsed_metadata.date_format) ~> DateFormatString",
            "ModifyNumberOfBatches, RemoveUnnecessaryColumns join(GenerateMaskParameters@output_row == RemoveUnnecessaryColumns@output_row,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> AllMaskingParameters",
            "DateFormatHeaderHandlingNulls select(mapColumn(",
            "          output_row,",
            "          DateFormatAssignments",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> RemoveUnnecessaryColumns",
            "DateFormatString split(not(isNull(date_format_string)),",
            "     disjoint: false) ~> SplitOnDateFormat@(ContainsDateFormat, DoesNotContainDateFormat)",
            "SplitOnDateFormat@DoesNotContainDateFormat aggregate(groupBy(output_row),",
            "     NoFormatHeader = reduce(collect(date_format_string), '{', #acc + #item + ',', #result + '}')) ~> NoFormatHeader",
            "DateFormatHeader, NoFormatHeader join(DateFormatHeader@output_row == NoFormatHeader@output_row,",
            "     joinType:'outer',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> JoinDateHeaders",
            "JoinDateHeaders derive(DateFormatAssignments = coalesce(DateFormatAssignments,'{}'),",
            "          output_row = coalesce(DateFormatHeader@output_row, NoFormatHeader@output_row)) ~> DateFormatHeaderHandlingNulls",
            "AllMaskingParameters sink(validateSchema: false,",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     store: 'cache',",
            "     format: 'inline',",
            "     output: true,",
            "     saveOrder: 1) ~> MaskingParameterOutput"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_adls_to_adls_delimited_unfiltered_mask_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "folder": {
          "name": "dcsazure_adls_to_adls"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Source')]",
                "type": "LinkedServiceReference"
              },
              "name": "Source",
              "description": "Select source data in source container DF_SOURCE_CONTAINER, and wildcard that is constructed based on DF_SOURCE_DIRECTORY, DF_SOURCE_PREFIX, and DF_SOURCE_TABLE using an inline dataset, with DF_COLUMN_DELIMITER, DF_QUOTE_CHARACTER, DF_ESCAPE_CHARACTER, and DF_NULL_VALUE as specified in the parameters, and storing the file name in DELPHIX_COMPLIANCE_SERVICE_FILE_NAME"
            }
          ],
          "sinks": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Sink')]",
                "type": "LinkedServiceReference"
              },
              "name": "Sink",
              "description": "Sink results of masking to data store by sinking all columns but DELPHIX_COMPLIANCE_SERVICE_BATCH_ID, DELPHIX_COMPLIANCE_SERVICE_SORT_ID, and DELPHIX_COMPLIANCE_SERVICE_FILE_NAME to the data sink, naming the file as DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME, and using the same metadata settings as were used in the source with DF_COLUMN_DELIMITER, DF_QUOTE_CHARACTER, DF_ESCAPE_CHARACTER, and DF_NULL_VALUE"
            }
          ],
          "transformations": [
            {
              "name": "DCSForAzureAPI",
              "linkedService": {
                "referenceName": "[parameters('ProdDCSForAzureService')]",
                "type": "LinkedServiceReference"
              }
            },
            {
              "name": "AddSortKey",
              "description": "Create column DELPHIX_COMPLIANCE_SERVICE_SORT_ID that consists of SHA of the data across all columns in the table - every row will have this value and it cannot be null"
            },
            {
              "name": "SortBySortKey",
              "description": "Sort the table by the value in DELPHIX_COMPLIANCE_SERVICE_SORT_ID, as we need the table to be in a particular order before we apply a surrogate key"
            },
            {
              "name": "CreateSurrogateKey",
              "description": "Add a DELPHIX_COMPLIANCE_SERVICE_BATCH_ID column that increments by 1 and starts at 1 after applying the sorting"
            },
            {
              "name": "WrapValuesInArray",
              "description": "For each column we wish to mask, convert the value into an array, this is needed to preserve null values as null when using collect, as null values become []"
            },
            {
              "name": "SelectColumnsUnmasked",
              "description": "Select only columns that don't require masking"
            },
            {
              "name": "AggregateColumnsByBatch",
              "description": "For each column we wish to mask, aggregate to a list using collect, grouped by DELPHIX_COMPLIANCE_SERVICE_BATCH_ID batches by using DF_TARGET_BATCH_SIZE and the number of columns being masked, name the group as DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP"
            },
            {
              "name": "FlattenValuesOutOfArray",
              "description": "For each column we wish to mask, flatten the value out of the array, in the case where the value was previously [], it becomes null"
            },
            {
              "name": "AssertNoFailures",
              "description": "Confirm that we received a 200 response status from the API request"
            },
            {
              "name": "FlattenAggregateData",
              "description": "Unroll the API response body into named columns"
            },
            {
              "name": "JoinMaskedAndUnmaskedData",
              "description": "Inner join on SelectColumnsUnmasked and FlattenAggregateData based on matching DELPHIX_COMPLIANCE_SERVICE_BATCH_ID"
            },
            {
              "name": "TrimMaskedStrings",
              "description": "For each column with a string type, trim the string to length based on the value in DF_TRIM_LENGTHS - this is needed as masking a string may produce a longer string that exceeds the column width in the sink"
            },
            {
              "name": "CreateSinkFileName",
              "description": "Create column DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME by replacing DF_SOURCE_DIRECTORY with DF_SINK_DIRECTORY in the value in DELPHIX_COMPLIANCE_SERVICE_FILE_NAME"
            },
            {
              "name": "CreateAlterRow",
              "description": "Add an alter row condition so that all rows will be inserted into the existing table with the correct column order"
            },
            {
              "name": "CombineRows",
              "description": "Union the output of RemoveAmbiguousColumn with the empty CreateAlterRow table, and performing the union by name"
            },
            {
              "name": "RemoveAmbiguousColumn",
              "description": "Since both FlattenAggregateData and SelectColumnsUnmasked have a column DELPHIX_COMPLIANCE_SERVICE_BATCH_ID, we need to remove that column, so select all columns whose name isn't DELPHIX_COMPLIANCE_SERVICE_BATCH_ID"
            },
            {
              "name": "RemoveAllData",
              "description": "To preserve the order of the columns, we will remove all rows from the table by filtering on false()"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SINK_CONTAINER as string (''),",
            "     DF_SOURCE_DIRECTORY as string (''),",
            "     DF_SINK_DIRECTORY as string (''),",
            "     DF_SOURCE_PREFIX as string (''),",
            "     DF_SOURCE_TABLE as string (''),",
            "     DF_SINK_TABLE as string (''),",
            "     DF_COLUMN_DELIMITER as string (''),",
            "     DF_QUOTE_CHARACTER as string (''),",
            "     DF_ESCAPE_CHARACTER as string (''),",
            "     DF_NULL_VALUE as string (''),",
            "     DF_FIELD_ALGORITHM_ASSIGNMENT as string ('{}'),",
            "     DF_COLUMNS_TO_MASK as string[] ([\"\"]),",
            "     DF_BODY_TYPE_MAPPING as string ('(timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long)[])'),",
            "     DF_TRIM_LENGTHS as integer[] ([-1]),",
            "     DF_FAIL_ON_NONCONFORMANT_DATA as boolean (true()),",
            "     DF_TARGET_BATCH_SIZE as integer (2000),",
            "     DF_FIELD_DATE_FORMAT as string ('{}')",
            "}",
            "source(useSchema: false,",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     ignoreNoFilesFound: false,",
            "     rowUrlColumn: 'DELPHIX_COMPLIANCE_SERVICE_FILE_NAME',",
            "     format: 'delimited',",
            "     fileSystem: ($DF_SOURCE_CONTAINER),",
            "     columnDelimiter: ($DF_COLUMN_DELIMITER),",
            "     escapeChar: ($DF_ESCAPE_CHARACTER),",
            "     quoteChar: ($DF_QUOTE_CHARACTER),",
            "     nullValue: ($DF_NULL_VALUE),",
            "     columnNamesAsHeader: true,",
            "     wildcardPaths:[(concat($DF_SOURCE_DIRECTORY,iif(length($DF_SOURCE_PREFIX) == 0,iif(equals('NO_EXT',$DF_SOURCE_TABLE),'*',concat('*',$DF_SOURCE_TABLE)),iif(equals('NO_EXT',$DF_SOURCE_TABLE),concat($DF_SOURCE_PREFIX,'*'),concat($DF_SOURCE_PREFIX,'*',$DF_SOURCE_TABLE)))))],",
            "     partitionBy('roundRobin', 32)) ~> Source",
            "FlattenValuesOutOfArray call(mapColumn(",
            "          each(match(name!='DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP'))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     output(",
            "          headers as [string,string],",
            "          body as (timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long)[]),",
            "          status as string",
            "     ),",
            "     allowSchemaDrift: true,",
            "     format: 'rest',",
            "     store: 'restservice',",
            "     timeout: 300,",
            "     requestInterval: 0,",
            "     headers = ['Run-Id' -> $runId, 'Field-Algorithm-Assignment' -> $DF_FIELD_ALGORITHM_ASSIGNMENT, 'Fail-On-Non-Conformant-Data' -> iif($DF_FAIL_ON_NONCONFORMANT_DATA,'true','false'), 'Field-Date-Format' -> $DF_FIELD_DATE_FORMAT],",
            "     httpMethod: 'POST',",
            "     entity: 'v1/masking/batchMaskByColumn',",
            "     headerColumnName: 'headers',",
            "     bodyColumnName: 'body',",
            "     statusColumnName: 'status',",
            "     addResponseCode: true,",
            "     requestFormat: ['type' -> 'json'],",
            "     responseFormat: ['type' -> 'json', 'documentForm' -> 'documentPerLine'],",
            "     columnTypeMap: ['body'->'$DF_BODY_TYPE_MAPPING']) ~> DCSForAzureAPI",
            "Source derive(DELPHIX_COMPLIANCE_SERVICE_SORT_ID = sha2(256, columns())) ~> AddSortKey",
            "AddSortKey sort(asc(DELPHIX_COMPLIANCE_SERVICE_SORT_ID, false)) ~> SortBySortKey",
            "SortBySortKey keyGenerate(output(DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long),",
            "     startAt: 1L,",
            "     stepValue: 1L) ~> CreateSurrogateKey",
            "CreateSurrogateKey derive(each(match(contains($DF_COLUMNS_TO_MASK,#item==name)), $$ = array($$))) ~> WrapValuesInArray",
            "CreateSurrogateKey select(mapColumn(",
            "          each(match(!contains($DF_COLUMNS_TO_MASK,#item==name)))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SelectColumnsUnmasked",
            "WrapValuesInArray aggregate(groupBy(DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP = toInteger(ceil(DELPHIX_COMPLIANCE_SERVICE_BATCH_ID/($DF_TARGET_BATCH_SIZE/size($DF_COLUMNS_TO_MASK))))),",
            "     each(match(contains($DF_COLUMNS_TO_MASK,#item==name)||(name==\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\")), $$ = collect($$))) ~> AggregateColumnsByBatch",
            "AggregateColumnsByBatch derive(each(match(contains($DF_COLUMNS_TO_MASK,#item==name)||(name==\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\")), $$ = flatten($$))) ~> FlattenValuesOutOfArray",
            "DCSForAzureAPI assert(expectTrue(toInteger(regexExtract(status, '(\\\\d+)', 1)) == 200, false, 'Failed_request', null, iif(isNull(body.message), status, concatWS(', ', 'timestamp: ' + toString(body.timestamp), 'status: ' + body.status, 'message: ' + body.message, 'trace_id: ' + body.trace_id))),",
            "     abort: true) ~> AssertNoFailures",
            "AssertNoFailures foldDown(unroll(body.items),",
            "     mapColumn(",
            "          every(body.items,match(true()))",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenAggregateData",
            "SelectColumnsUnmasked, FlattenAggregateData join(SelectColumnsUnmasked@DELPHIX_COMPLIANCE_SERVICE_BATCH_ID == FlattenAggregateData@DELPHIX_COMPLIANCE_SERVICE_BATCH_ID,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'off')~> JoinMaskedAndUnmaskedData",
            "FlattenAggregateData derive(each(match(type=='string'), $$ = substring($$, 1, $DF_TRIM_LENGTHS[toInteger($# - 1)]))) ~> TrimMaskedStrings",
            "CombineRows derive(DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME = replace(DELPHIX_COMPLIANCE_SERVICE_FILE_NAME, $DF_SOURCE_DIRECTORY, $DF_SINK_DIRECTORY)) ~> CreateSinkFileName",
            "RemoveAllData alterRow(insertIf(true())) ~> CreateAlterRow",
            "CreateAlterRow, RemoveAmbiguousColumn union(byName: true)~> CombineRows",
            "JoinMaskedAndUnmaskedData select(mapColumn(",
            "          each(match(name!=\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\"))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> RemoveAmbiguousColumn",
            "CreateSurrogateKey filter(false()) ~> RemoveAllData",
            "CreateSinkFileName sink(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'delimited',",
            "     fileSystem: ($DF_SINK_CONTAINER),",
            "     folderPath: ($DF_SINK_DIRECTORY),",
            "     columnDelimiter: ($DF_COLUMN_DELIMITER),",
            "     escapeChar: ($DF_ESCAPE_CHARACTER),",
            "     quoteChar: ($DF_QUOTE_CHARACTER),",
            "     nullValue: ($DF_NULL_VALUE),",
            "     columnNamesAsHeader: true,",
            "     rowUrlColumn:'DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME',",
            "     umask: 0022,",
            "     preCommands: [],",
            "     postCommands: [],",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     mapColumn(",
            "          each(match(name!=\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\"&&name!=\"DELPHIX_COMPLIANCE_SERVICE_SORT_ID\"&&name!=\"DELPHIX_COMPLIANCE_SERVICE_FILE_NAME\"))",
            "     )) ~> Sink"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_adls_to_adls_filtered_mask_params_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "description": "Generate masking parameters when conditional algorithms are defined",
        "folder": {
          "name": "dcsazure_adls_to_adls"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('Metadata Datastore')]",
                "type": "LinkedServiceReference"
              },
              "name": "Ruleset",
              "description": "Get the ruleset table from the metadata store at DF_METADATA_SCHEMA.DF_METADATA_RULESET_TABLE"
            },
            {
              "linkedService": {
                "referenceName": "[parameters('Metadata Datastore')]",
                "type": "LinkedServiceReference"
              },
              "name": "TypeMapping",
              "description": "Get the type mapping table from the metadata store at DF_METADATA_STORE.DF_METADATA_ADF_TYPE_MAPPING_TABLE"
            }
          ],
          "sinks": [
            {
              "name": "MaskingParameterOutput",
              "description": "Sink results of computing masking parameters to activity output cache"
            }
          ],
          "transformations": [
            {
              "name": "FilterToSingleTable",
              "description": "Filter ruleset table down to the table in question by specifying dataset, specified_database, specified_schema, identified_table, and assigned_algorithm - making sure they match the dataset associated with each version of the dataflow, DF_SOURCE_DATABASE, DF_SOURCE_SCHEMA, DF_SOURCE_TABLE, and not empty or null (respectively). This filters the ruleset down to only the rules that need to be applied for masking this particular table"
            },
            {
              "name": "RulesetWithTypes",
              "description": "Join the ruleset table with the type mapping table based on the type of the column and the translation of that type to an ADF type"
            },
            {
              "name": "RulesetWithAlgorithmTypeMapping",
              "description": "Generate several columns:\n\n    output_row that always contains 1 (used later for Aggregate and Join operations)\n    adf_type_conversion that contains a string like <column_name> as <adf_type>\n    column_width_estimate that contains an integer that uses DF_COLUMN_WIDTH_ESTIMATE as the width for any column where identified_column_max_length is not positive, and identified_column_max_length plus some padding otherwise"
            },
            {
              "name": "GenerateMaskParameters",
              "description": "Grouped by output_row produce the following aggregates\n\n    FieldAlgorithmAssignments - a JSON string that maps a column name to its assigned algorithm\n    ColumnsToMask - a list of the column names that have an algorithm assigned\n    DataFactoryTypeMapping - a string that can be used by ADF to parse the output of a call to the Delphix masking endpoint, leveraging the adf_type_conversion column derived previously\n    NumberOfBatches - an integer value determined by computing the number of batches leveraging the max row_count as specified in the ruleset table, and the sum of column_width_estimate column derived previously\n    TrimLengths - a list of the actual widths of the columns so that will be used by the masking data flow to trim output before sinking"
            },
            {
              "name": "ModifyNumberOfBatches",
              "description": "Modifies the number of batches to be at least 1"
            },
            {
              "name": "FilterToDataSourceType",
              "description": "Filter type mapping table down to only the dataset in question"
            },
            {
              "name": "ParseMetadata",
              "description": "Parse the content from the metadata column that contains JSON, specifically handling parsing of known keys (i.e. date_format)"
            },
            {
              "name": "DateFormatHeader",
              "description": "Create DateFormatAssignment, grouped by output_row (which is always 1), generating a JSON string that maps a column to its specified date format"
            },
            {
              "name": "DateFormatString",
              "description": "Derive columns as necessary for handling the parsed data (i.e. consume parsed_metadata.date_format and put it in a column date_format_string), and add an output_row column that always contains 1 (used later for Aggregate and Join operations)"
            },
            {
              "name": "AllMaskingParameters",
              "description": "Perform an inner join on output_row with the computed DateFormatHeaders - combining all masking parameters into the same output stream"
            },
            {
              "name": "RemoveUnnecessaryColumns",
              "description": "Remove intermediate columns"
            },
            {
              "name": "SplitOnDateFormat",
              "description": "Split the data into two streams, data that contains a specified date\nformat, and data that does not"
            },
            {
              "name": "NoFormatHeader",
              "description": "Create NoFormatHeader, that generates a JSON string containing an empty map when all values are null, grouped by output_row (which is always 1)"
            },
            {
              "name": "JoinDateHeaders",
              "description": "Full outer join both DateFormatHeader and NoFormatHeader where output_row = output_row"
            },
            {
              "name": "DateFormatHeaderHandlingNulls",
              "description": "Update column DateFormatAssignments to coalesce DateFormatAssignments, and NoFormatHeader (i.e. if DateFormatAssignments is null, take NoFormatHeader, which won't be null), similarly with output_row"
            },
            {
              "name": "HandleConditionalAlgorithms",
              "description": "Conditionally distributing the data in assigned_algorithm groups, based on the type of data in assigned_algorithm"
            },
            {
              "name": "SimplifySimpleRulesetTable",
              "description": "Simplify the columns of the ruleset table for algorithms that are always applied"
            },
            {
              "name": "ParseAlgorithm",
              "description": "Parse conditional algorithm assignment"
            },
            {
              "name": "FlattenAlgorithmAssignments",
              "description": "Unroll the conditions from the conditional algorithm assignment"
            },
            {
              "name": "ParseKeyColumn",
              "description": "Parse key column conditions"
            },
            {
              "name": "FlattenKeyConditions",
              "description": "Unroll the aliases and conditions from the key column"
            },
            {
              "name": "JoinConditionalAlgorithms",
              "description": "Join the key column, its conditions, and the algorithms assigned to those conditions"
            },
            {
              "name": "FilterToConditionKey",
              "description": "Filter out rows that don't apply to this condition key, and rows that don't include an assigned algorithm"
            },
            {
              "name": "SimplifyConditionalRulesetTable",
              "description": "Rename columns in the conditional ruleset table to match the simplified ruleset table"
            },
            {
              "name": "UnionAllRules",
              "description": "Combining rows from conditional and non-conditional ruleset tables"
            },
            {
              "name": "FlattenConditionalFormatting",
              "description": "Unroll the conditions from the conditional date_format assignment"
            },
            {
              "name": "FilterUnmatchingAlias",
              "description": "Filter down to only this filer alias, as necessary"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_METADATA_SCHEMA as string ('dbo'),",
            "     DF_METADATA_RULESET_TABLE as string ('discovered_ruleset'),",
            "     DF_METADATA_ADF_TYPE_MAPPING_TABLE as string ('adf_type_mapping'),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SOURCE_SCHEMA as string (''),",
            "     DF_SOURCE_TABLE as string (''),",
            "     DF_COLUMN_WIDTH_ESTIMATE as integer (1000),",
            "     DF_FILTER_KEY as string (''),",
            "     DF_DATASET as string ('ADLS')",
            "}",
            "source(output(",
            "          dataset as string,",
            "          specified_database as string,",
            "          specified_schema as string,",
            "          identified_table as string,",
            "          identified_column as string,",
            "          identified_column_type as string,",
            "          identified_column_max_length as integer,",
            "          ordinal_position as integer,",
            "          row_count as long,",
            "          metadata as string,",
            "          profiled_domain as string,",
            "          profiled_algorithm as string,",
            "          confidence_score as decimal(6,5),",
            "          rows_profiled as long,",
            "          assigned_algorithm as string,",
            "          last_profiled_updated_timestamp as timestamp",
            "     ),",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'table',",
            "     store: 'sqlserver',",
            "     schemaName: ($DF_METADATA_SCHEMA),",
            "     tableName: ($DF_METADATA_RULESET_TABLE),",
            "     isolationLevel: 'READ_UNCOMMITTED') ~> Ruleset",
            "source(output(",
            "          dataset as string,",
            "          dataset_type as string,",
            "          adf_type as string",
            "     ),",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'table',",
            "     store: 'sqlserver',",
            "     schemaName: ($DF_METADATA_SCHEMA),",
            "     tableName: ($DF_METADATA_ADF_TYPE_MAPPING_TABLE),",
            "     isolationLevel: 'READ_UNCOMMITTED') ~> TypeMapping",
            "Ruleset filter(equalsIgnoreCase(dataset, $DF_DATASET) ",
            "&& equalsIgnoreCase(specified_database, $DF_SOURCE_CONTAINER) ",
            "&& equalsIgnoreCase(specified_schema, $DF_SOURCE_SCHEMA) ",
            "&& equalsIgnoreCase(identified_table, $DF_SOURCE_TABLE)",
            "&& !equalsIgnoreCase(assigned_algorithm, '')",
            "&& !isNull(assigned_algorithm)) ~> FilterToSingleTable",
            "UnionAllRules, FilterToDataSourceType join(identified_column_type <=> dataset_type,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'left')~> RulesetWithTypes",
            "RulesetWithTypes derive(adf_type_conversion = concat(identified_column, ' as ', adf_type),",
            "          output_row = 1,",
            "          column_width_estimate = iif(identified_column_max_length > 0, identified_column_max_length+4, $DF_COLUMN_WIDTH_ESTIMATE)) ~> RulesetWithAlgorithmTypeMapping",
            "RulesetWithAlgorithmTypeMapping aggregate(groupBy(output_row),",
            "     FieldAlgorithmAssignments = regexReplace(reduce(mapAssociation(keyValues(collect(identified_column), collect(assigned_algorithm)), '\"' + #key + '\":\"' + #value + '\"'), '{', #acc + #item + ',', #result + '}'), ',}', '}'),",
            "          ColumnsToMask = regexReplace(reduce(collect(identified_column), '[',  #acc + '\"' + #item + '\",', #result + ']'), ',]', ']'),",
            "          DataFactoryTypeMapping = concat(\"'\", '(timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long, ',",
            "    regexReplace(reduce(collect(identified_column + ' as ' + adf_type), '', #acc + #item + ', ', #result + ')'), ', \\\\)', ')'),",
            "    '[])', \"'\"),",
            "          NumberOfBatches = toInteger(ceil(((max(row_count) * (sum(column_width_estimate) + log10(max(row_count))+1)) / (2000000 * .9)))),",
            "          TrimLengths = regexReplace(reduce(collect(identified_column_max_length), '[',  #acc + toString(#item) + ',', toString(#result) + ']'), ',]', ']')) ~> GenerateMaskParameters",
            "GenerateMaskParameters derive(NumberOfBatches = iif(NumberOfBatches > 0, NumberOfBatches, 1)) ~> ModifyNumberOfBatches",
            "TypeMapping filter(equalsIgnoreCase(dataset, $DF_DATASET)) ~> FilterToDataSourceType",
            "FilterToSingleTable parse(parsed_metadata = metadata ? (date_format as string),",
            "          conditional_formatting = metadata ? (key_column as string,",
            "          conditions as (alias as string, date_format as string)[]),",
            "     format: 'json',",
            "     documentForm: 'singleDocument') ~> ParseMetadata",
            "SplitOnDateFormat@ContainsDateFormat aggregate(groupBy(output_row),",
            "     DateFormatAssignments = regexReplace(reduce(mapAssociation(keyValues(collect(identified_column), collect(date_format_string)), '\"' + #key + '\":\"' + #value + '\"'), '{', #acc + #item + ',', #result + '}'), ',}', '}')) ~> DateFormatHeader",
            "FilterUnmatchingAlias derive(output_row = 1,",
            "          date_format_string = coalesce(conditional_date_format, parsed_metadata.date_format)) ~> DateFormatString",
            "ModifyNumberOfBatches, RemoveUnnecessaryColumns join(GenerateMaskParameters@output_row == RemoveUnnecessaryColumns@output_row,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> AllMaskingParameters",
            "DateFormatHeaderHandlingNulls select(mapColumn(",
            "          output_row,",
            "          DateFormatAssignments",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> RemoveUnnecessaryColumns",
            "DateFormatString split(not(isNull(date_format_string)),",
            "     disjoint: false) ~> SplitOnDateFormat@(ContainsDateFormat, DoesNotContainDateFormat)",
            "SplitOnDateFormat@DoesNotContainDateFormat aggregate(groupBy(output_row),",
            "     NoFormatHeader = reduce(collect(date_format_string), '{', #acc + #item + ',', #result + '}')) ~> NoFormatHeader",
            "DateFormatHeader, NoFormatHeader join(DateFormatHeader@output_row == NoFormatHeader@output_row,",
            "     joinType:'outer',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> JoinDateHeaders",
            "JoinDateHeaders derive(DateFormatAssignments = coalesce(DateFormatAssignments,'{}'),",
            "          output_row = coalesce(DateFormatHeader@output_row, NoFormatHeader@output_row)) ~> DateFormatHeaderHandlingNulls",
            "FilterToSingleTable split(like(assigned_algorithm, '[%]'),",
            "     like(assigned_algorithm, '{%}'),",
            "     disjoint: false) ~> HandleConditionalAlgorithms@(KeyColumn, ConditionalAlgorithm, StandardAlgorithm)",
            "HandleConditionalAlgorithms@StandardAlgorithm select(mapColumn(",
            "          dataset,",
            "          specified_database,",
            "          specified_schema,",
            "          identified_table,",
            "          identified_column,",
            "          identified_column_type,",
            "          identified_column_max_length,",
            "          row_count,",
            "          assigned_algorithm",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SimplifySimpleRulesetTable",
            "HandleConditionalAlgorithms@ConditionalAlgorithm parse(conditional_algorithm = assigned_algorithm ? (key_column as string,",
            "          conditions as (alias as string, algorithm as string)[]),",
            "     format: 'json',",
            "     documentForm: 'documentPerLine') ~> ParseAlgorithm",
            "ParseAlgorithm foldDown(unroll(conditional_algorithm.conditions),",
            "     mapColumn(",
            "          dataset,",
            "          specified_database,",
            "          specified_schema,",
            "          identified_table,",
            "          identified_column,",
            "          identified_column_type,",
            "          identified_column_max_length,",
            "          row_count,",
            "          assigned_algorithm,",
            "          key_column = conditional_algorithm.key_column,",
            "          alias = conditional_algorithm.conditions.alias,",
            "          algorithm = conditional_algorithm.conditions.algorithm",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenAlgorithmAssignments",
            "HandleConditionalAlgorithms@KeyColumn parse(conditions_set = assigned_algorithm ? (alias as string, condition as string)[],",
            "     format: 'json',",
            "     documentForm: 'arrayOfDocuments') ~> ParseKeyColumn",
            "ParseKeyColumn foldDown(unroll(conditions_set),",
            "     mapColumn(",
            "          dataset,",
            "          specified_database,",
            "          specified_schema,",
            "          identified_table,",
            "          identified_column,",
            "          identified_column_type,",
            "          identified_column_max_length,",
            "          row_count,",
            "          assigned_algorithm,",
            "          alias = conditions_set.alias,",
            "          condition = conditions_set.condition",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenKeyConditions",
            "FlattenKeyConditions, FlattenAlgorithmAssignments join(FlattenKeyConditions@identified_column == key_column",
            "     && FlattenKeyConditions@alias == FlattenAlgorithmAssignments@alias,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'auto')~> JoinConditionalAlgorithms",
            "JoinConditionalAlgorithms filter(equalsIgnoreCase(FlattenAlgorithmAssignments@alias, $DF_FILTER_KEY) && not(isNull(algorithm)) && algorithm != '') ~> FilterToConditionKey",
            "FilterToConditionKey select(mapColumn(",
            "          dataset = FlattenAlgorithmAssignments@dataset,",
            "          specified_database = FlattenAlgorithmAssignments@specified_database,",
            "          specified_schema = FlattenAlgorithmAssignments@specified_schema,",
            "          identified_table = FlattenAlgorithmAssignments@identified_table,",
            "          identified_column = FlattenAlgorithmAssignments@identified_column,",
            "          identified_column_type = FlattenAlgorithmAssignments@identified_column_type,",
            "          identified_column_max_length = FlattenAlgorithmAssignments@identified_column_max_length,",
            "          row_count = FlattenAlgorithmAssignments@row_count,",
            "          assigned_algorithm = FlattenAlgorithmAssignments@assigned_algorithm,",
            "          key_column,",
            "          assigned_algorithm = algorithm",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SimplifyConditionalRulesetTable",
            "SimplifyConditionalRulesetTable, SimplifySimpleRulesetTable union(byName: true)~> UnionAllRules",
            "ParseMetadata foldDown(unroll(conditional_formatting.conditions),",
            "     mapColumn(",
            "          dataset,",
            "          specified_database,",
            "          specified_schema,",
            "          identified_table,",
            "          identified_column,",
            "          identified_column_type,",
            "          identified_column_max_length,",
            "          ordinal_position,",
            "          row_count,",
            "          metadata,",
            "          profiled_domain,",
            "          profiled_algorithm,",
            "          confidence_score,",
            "          rows_profiled,",
            "          assigned_algorithm,",
            "          last_profiled_updated_timestamp,",
            "          parsed_metadata,",
            "          conditional_formatting_key_column = conditional_formatting.key_column,",
            "          alias = conditional_formatting.conditions.alias,",
            "          conditional_date_format = conditional_formatting.conditions.date_format",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenConditionalFormatting",
            "FlattenConditionalFormatting filter(alias == $DF_FILTER_KEY || isNull(alias)) ~> FilterUnmatchingAlias",
            "AllMaskingParameters sink(validateSchema: false,",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     store: 'cache',",
            "     format: 'inline',",
            "     output: true,",
            "     saveOrder: 1) ~> MaskingParameterOutput"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_adls_to_adls_delimited_filtered_mask_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "description": "Perform masking on a subset of the data that matches the specified filter",
        "folder": {
          "name": "dcsazure_adls_to_adls"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Source')]",
                "type": "LinkedServiceReference"
              },
              "name": "Source",
              "description": "Select source data in source container DF_SOURCE_CONTAINER, and wildcard that is constructed based on DF_SOURCE_DIRECTORY, DF_SOURCE_PREFIX, and DF_SOURCE_TABLE using an inline dataset, with DF_COLUMN_DELIMITER, DF_QUOTE_CHARACTER, DF_ESCAPE_CHARACTER, and DF_NULL_VALUE as specified in the parameters, and storing the file name in DELPHIX_COMPLIANCE_SERVICE_FILE_NAME"
            }
          ],
          "sinks": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Sink')]",
                "type": "LinkedServiceReference"
              },
              "name": "Sink",
              "description": "Sink results of masking to data store by sinking all columns but DELPHIX_COMPLIANCE_SERVICE_BATCH_ID, DELPHIX_COMPLIANCE_SERVICE_SORT_ID, and DELPHIX_COMPLIANCE_SERVICE_FILE_NAME to the data sink, naming the file as DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME, and using the same metadata settings as were used in the source with DF_COLUMN_DELIMITER, DF_QUOTE_CHARACTER, DF_ESCAPE_CHARACTER, and DF_NULL_VALUE"
            }
          ],
          "transformations": [
            {
              "name": "DCSForAzureAPI",
              "linkedService": {
                "referenceName": "[parameters('ProdDCSForAzureService')]",
                "type": "LinkedServiceReference"
              }
            },
            {
              "name": "AddSortKey",
              "description": "Create column DELPHIX_COMPLIANCE_SERVICE_SORT_ID that consists of SHA of the data across all columns in the table - every row will have this value and it cannot be null"
            },
            {
              "name": "SortBySortKey",
              "description": "Sort the table by the value in DELPHIX_COMPLIANCE_SERVICE_SORT_ID, as we need the table to be in a particular order before we apply a surrogate key"
            },
            {
              "name": "CreateSurrogateKey",
              "description": "Add a DELPHIX_COMPLIANCE_SERVICE_BATCH_ID column that increments by 1 and starts at 1 after applying the sorting"
            },
            {
              "name": "WrapValuesInArray",
              "description": "For each column we wish to mask, convert the value into an array, this is needed to preserve null values as null when using collect, as null values become []"
            },
            {
              "name": "SelectColumnsUnmasked",
              "description": "Select only columns that don't require masking"
            },
            {
              "name": "AggregateColumnsByBatch",
              "description": "For each column we wish to mask, aggregate to a list using collect, grouped by DELPHIX_COMPLIANCE_SERVICE_BATCH_ID batches by using DF_TARGET_BATCH_SIZE and the number of columns being masked, name the group as DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP"
            },
            {
              "name": "FlattenValuesOutOfArray",
              "description": "For each column we wish to mask, flatten the value out of the array, in the case where the value was previously [], it becomes null"
            },
            {
              "name": "AssertNoFailures",
              "description": "Confirm that we received a 200 response status from the API request"
            },
            {
              "name": "FlattenAggregateData",
              "description": "Unroll the API response body into named columns"
            },
            {
              "name": "JoinMaskedAndUnmaskedData",
              "description": "Inner join on SelectColumnsUnmasked and FlattenAggregateData based on matching DELPHIX_COMPLIANCE_SERVICE_BATCH_ID"
            },
            {
              "name": "TrimMaskedStrings",
              "description": "For each column with a string type, trim the string to length based on the value in DF_TRIM_LENGTHS - this is needed as masking a string may produce a longer string that exceeds the column width in the sink"
            },
            {
              "name": "CreateSinkFileName",
              "description": "Create column DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME by replacing DF_SOURCE_DIRECTORY with DF_SINK_DIRECTORY in the value in DELPHIX_COMPLIANCE_SERVICE_FILE_NAME"
            },
            {
              "name": "CreateAlterRow",
              "description": "Add an alter row condition so that all rows will be inserted into the existing table with the correct column order"
            },
            {
              "name": "CombineRows",
              "description": "Union the output of RemoveAmbiguousColumn with the empty CreateAlterRow table, and performing the union by name"
            },
            {
              "name": "RemoveAmbiguousColumn",
              "description": "Since both FlattenAggregateData and SelectColumnsUnmasked have a column DELPHIX_COMPLIANCE_SERVICE_BATCH_ID, we need to remove that column, so select all columns whose name isn't DELPHIX_COMPLIANCE_SERVICE_BATCH_ID"
            },
            {
              "name": "RemoveAllData",
              "description": "To preserve the order of the columns, we will remove all rows from the table by filtering on false()"
            },
            {
              "name": "ApplyTableFilter",
              "description": "Filter base table based on supplied filter"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SINK_CONTAINER as string (''),",
            "     DF_SOURCE_DIRECTORY as string (''),",
            "     DF_SINK_DIRECTORY as string (''),",
            "     DF_SOURCE_PREFIX as string (''),",
            "     DF_SOURCE_TABLE as string (''),",
            "     DF_SINK_TABLE as string (''),",
            "     DF_COLUMN_DELIMITER as string (''),",
            "     DF_QUOTE_CHARACTER as string (''),",
            "     DF_ESCAPE_CHARACTER as string (''),",
            "     DF_NULL_VALUE as string (''),",
            "     DF_FIELD_ALGORITHM_ASSIGNMENT as string ('{}'),",
            "     DF_COLUMNS_TO_MASK as string[] ([\"\"]),",
            "     DF_BODY_TYPE_MAPPING as string ('(timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long)[])'),",
            "     DF_TRIM_LENGTHS as integer[] ([-1]),",
            "     DF_FAIL_ON_NONCONFORMANT_DATA as boolean (true()),",
            "     DF_TARGET_BATCH_SIZE as integer (2000),",
            "     DF_FIELD_DATE_FORMAT as string ('{}'),",
            "     DF_FILTER_CONDITION as boolean (true())",
            "}",
            "source(useSchema: false,",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     ignoreNoFilesFound: false,",
            "     rowUrlColumn: 'DELPHIX_COMPLIANCE_SERVICE_FILE_NAME',",
            "     format: 'delimited',",
            "     fileSystem: ($DF_SOURCE_CONTAINER),",
            "     columnDelimiter: ($DF_COLUMN_DELIMITER),",
            "     escapeChar: ($DF_ESCAPE_CHARACTER),",
            "     quoteChar: ($DF_QUOTE_CHARACTER),",
            "     nullValue: ($DF_NULL_VALUE),",
            "     columnNamesAsHeader: true,",
            "     wildcardPaths:[(concat($DF_SOURCE_DIRECTORY,iif(length($DF_SOURCE_PREFIX) == 0,iif(equals('NO_EXT',$DF_SOURCE_TABLE),'*',concat('*',$DF_SOURCE_TABLE)),iif(equals('NO_EXT',$DF_SOURCE_TABLE),concat($DF_SOURCE_PREFIX,'*'),concat($DF_SOURCE_PREFIX,'*',$DF_SOURCE_TABLE)))))],",
            "     partitionBy('roundRobin', 32)) ~> Source",
            "FlattenValuesOutOfArray call(mapColumn(",
            "          each(match(name!='DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP'))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     output(",
            "          headers as [string,string],",
            "          body as (timestamp as date, status as string, message as string, trace_id as string, items as (DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long)[]),",
            "          status as string",
            "     ),",
            "     allowSchemaDrift: true,",
            "     format: 'rest',",
            "     store: 'restservice',",
            "     timeout: 300,",
            "     requestInterval: 0,",
            "     headers = ['Run-Id' -> $runId, 'Field-Algorithm-Assignment' -> $DF_FIELD_ALGORITHM_ASSIGNMENT, 'Fail-On-Non-Conformant-Data' -> iif($DF_FAIL_ON_NONCONFORMANT_DATA,'true','false'), 'Field-Date-Format' -> $DF_FIELD_DATE_FORMAT],",
            "     httpMethod: 'POST',",
            "     entity: 'v1/masking/batchMaskByColumn',",
            "     headerColumnName: 'headers',",
            "     bodyColumnName: 'body',",
            "     statusColumnName: 'status',",
            "     addResponseCode: true,",
            "     requestFormat: ['type' -> 'json'],",
            "     responseFormat: ['type' -> 'json', 'documentForm' -> 'documentPerLine'],",
            "     columnTypeMap: ['body'->'$DF_BODY_TYPE_MAPPING']) ~> DCSForAzureAPI",
            "ApplyTableFilter derive(DELPHIX_COMPLIANCE_SERVICE_SORT_ID = sha2(256, columns())) ~> AddSortKey",
            "AddSortKey sort(asc(DELPHIX_COMPLIANCE_SERVICE_SORT_ID, false)) ~> SortBySortKey",
            "SortBySortKey keyGenerate(output(DELPHIX_COMPLIANCE_SERVICE_BATCH_ID as long),",
            "     startAt: 1L,",
            "     stepValue: 1L) ~> CreateSurrogateKey",
            "CreateSurrogateKey derive(each(match(contains($DF_COLUMNS_TO_MASK,#item==name)), $$ = array($$))) ~> WrapValuesInArray",
            "CreateSurrogateKey select(mapColumn(",
            "          each(match(!contains($DF_COLUMNS_TO_MASK,#item==name)))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> SelectColumnsUnmasked",
            "WrapValuesInArray aggregate(groupBy(DELPHIX_COMPLIANCE_SERVICE_BATCH_GROUP = toInteger(ceil(DELPHIX_COMPLIANCE_SERVICE_BATCH_ID/($DF_TARGET_BATCH_SIZE/size($DF_COLUMNS_TO_MASK))))),",
            "     each(match(contains($DF_COLUMNS_TO_MASK,#item==name)||(name==\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\")), $$ = collect($$))) ~> AggregateColumnsByBatch",
            "AggregateColumnsByBatch derive(each(match(contains($DF_COLUMNS_TO_MASK,#item==name)||(name==\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\")), $$ = flatten($$))) ~> FlattenValuesOutOfArray",
            "DCSForAzureAPI assert(expectTrue(toInteger(regexExtract(status, '(\\\\d+)', 1)) == 200, false, 'Failed_request', null, iif(isNull(body.message), status, concatWS(', ', 'timestamp: ' + toString(body.timestamp), 'status: ' + body.status, 'message: ' + body.message, 'trace_id: ' + body.trace_id))),",
            "     abort: true) ~> AssertNoFailures",
            "AssertNoFailures foldDown(unroll(body.items),",
            "     mapColumn(",
            "          every(body.items,match(true()))",
            "     ),",
            "     skipDuplicateMapInputs: false,",
            "     skipDuplicateMapOutputs: false) ~> FlattenAggregateData",
            "SelectColumnsUnmasked, FlattenAggregateData join(SelectColumnsUnmasked@DELPHIX_COMPLIANCE_SERVICE_BATCH_ID == FlattenAggregateData@DELPHIX_COMPLIANCE_SERVICE_BATCH_ID,",
            "     joinType:'inner',",
            "     matchType:'exact',",
            "     ignoreSpaces: false,",
            "     broadcast: 'off')~> JoinMaskedAndUnmaskedData",
            "FlattenAggregateData derive(each(match(type=='string'), $$ = substring($$, 1, $DF_TRIM_LENGTHS[toInteger($# - 1)]))) ~> TrimMaskedStrings",
            "CombineRows derive(DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME = replace(DELPHIX_COMPLIANCE_SERVICE_FILE_NAME, $DF_SOURCE_DIRECTORY, $DF_SINK_DIRECTORY)) ~> CreateSinkFileName",
            "RemoveAllData alterRow(insertIf(true())) ~> CreateAlterRow",
            "CreateAlterRow, RemoveAmbiguousColumn union(byName: true)~> CombineRows",
            "JoinMaskedAndUnmaskedData select(mapColumn(",
            "          each(match(name!=\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\"))",
            "     ),",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true) ~> RemoveAmbiguousColumn",
            "CreateSurrogateKey filter(false()) ~> RemoveAllData",
            "Source filter($DF_FILTER_CONDITION) ~> ApplyTableFilter",
            "CreateSinkFileName sink(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'delimited',",
            "     fileSystem: ($DF_SINK_CONTAINER),",
            "     folderPath: ($DF_SINK_DIRECTORY),",
            "     columnDelimiter: ($DF_COLUMN_DELIMITER),",
            "     escapeChar: ($DF_ESCAPE_CHARACTER),",
            "     quoteChar: ($DF_QUOTE_CHARACTER),",
            "     nullValue: ($DF_NULL_VALUE),",
            "     columnNamesAsHeader: true,",
            "     rowUrlColumn:'DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME',",
            "     umask: 0022,",
            "     preCommands: [],",
            "     postCommands: [],",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     mapColumn(",
            "          each(match(name!=\"DELPHIX_COMPLIANCE_SERVICE_BATCH_ID\"&&name!=\"DELPHIX_COMPLIANCE_SERVICE_SORT_ID\"&&name!=\"DELPHIX_COMPLIANCE_SERVICE_FILE_NAME\"))",
            "     )) ~> Sink"
          ]
        }
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_adls_container_and_directory_mask_ds')]",
      "type": "Microsoft.DataFactory/factories/datasets",
      "apiVersion": "2018-06-01",
      "properties": {
        "linkedServiceName": {
          "referenceName": "[parameters('AzureDataLakeStorage_Source')]",
          "type": "LinkedServiceReference"
        },
        "parameters": {
          "DS_CONTAINER": {
            "type": "string"
          },
          "DS_DIRECTORY": {
            "type": "string"
          }
        },
        "folder": {
          "name": "dcsazure_adls_to_adls"
        },
        "annotations": [],
        "type": "DelimitedText",
        "typeProperties": {
          "location": {
            "type": "AzureBlobFSLocation",
            "folderPath": {
              "value": "@dataset().DS_DIRECTORY",
              "type": "Expression"
            },
            "fileSystem": {
              "value": "@dataset().DS_CONTAINER",
              "type": "Expression"
            }
          },
          "columnDelimiter": ",",
          "escapeChar": "\\",
          "firstRowAsHeader": false,
          "quoteChar": "\""
        },
        "schema": [
          {
            "type": "String"
          }
        ]
      },
      "dependsOn": []
    },
    {
      "name": "[concat(parameters('factoryName'), '/dcsazure_adls_to_adls_delimited_copy_df')]",
      "type": "Microsoft.DataFactory/factories/dataflows",
      "apiVersion": "2018-06-01",
      "properties": {
        "folder": {
          "name": "dcsazure_adls_to_adls"
        },
        "type": "MappingDataFlow",
        "typeProperties": {
          "sources": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Source')]",
                "type": "LinkedServiceReference"
              },
              "name": "Source",
              "description": "Select source data in source container DF_SOURCE_CONTAINER, and wildcard that is constructed based on DF_SOURCE_DIRECTORY, DF_SOURCE_PREFIX, and DF_SOURCE_TABLE using an inline dataset, with DF_COLUMN_DELIMITER, DF_QUOTE_CHARACTER, DF_ESCAPE_CHARACTER, and DF_NULL_VALUE as specified in the parameters, and storing the file name in DELPHIX_COMPLIANCE_SERVICE_FILE_NAME"
            }
          ],
          "sinks": [
            {
              "linkedService": {
                "referenceName": "[parameters('AzureDataLakeStorage_Sink')]",
                "type": "LinkedServiceReference"
              },
              "name": "Sink",
              "description": "Sink results of masking to data store by sinking all columns but DELPHIX_COMPLIANCE_SERVICE_BATCH_ID, DELPHIX_COMPLIANCE_SERVICE_SORT_ID, and DELPHIX_COMPLIANCE_SERVICE_FILE_NAME to the data sink, naming the file as DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME, and using the same metadata settings as were used in the source with DF_COLUMN_DELIMITER, DF_QUOTE_CHARACTER, DF_ESCAPE_CHARACTER, and DF_NULL_VALUE"
            }
          ],
          "transformations": [
            {
              "name": "CreateSinkFileName",
              "description": "Create column DELPHIX_COMPLIANCE_SERVICE_SINK_FILE_NAME by replacing DF_SOURCE_DIRECTORY with DF_SINK_DIRECTORY in the value in DELPHIX_COMPLIANCE_SERVICE_FILE_NAME"
            }
          ],
          "scriptLines": [
            "parameters{",
            "     runId as string (''),",
            "     DF_SOURCE_CONTAINER as string (''),",
            "     DF_SINK_CONTAINER as string (''),",
            "     DF_SOURCE_DIRECTORY as string (''),",
            "     DF_SINK_DIRECTORY as string (''),",
            "     DF_SOURCE_PREFIX as string (''),",
            "     DF_COLUMN_DELIMITER as string (','),",
            "     DF_QUOTE_CHARACTER as string ('\"'),",
            "     DF_ESCAPE_CHARACTER as string ('\\\\'),",
            "     DF_NULL_VALUE as string (''),",
            "     DF_SOURCE_SUFFIX as string ('')",
            "}",
            "source(useSchema: false,",
            "     allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     ignoreNoFilesFound: false,",
            "     rowUrlColumn: 'DELPHIX_COMPLIANCE_SERVICES_FILE_NAME',",
            "     format: 'delimited',",
            "     fileSystem: ($DF_SOURCE_DATABASE),",
            "     columnDelimiter: ($DF_COLUMN_DELIMITER),",
            "     escapeChar: ($DF_ESCAPE_CHARACTER),",
            "     quoteChar: ($DF_QUOTE_CHARACTER),",
            "     nullValue: ($DF_NULL_VALUE),",
            "     columnNamesAsHeader: true,",
            "     wildcardPaths:[(concat($DF_SOURCE_DIRECTORY,iif(length($DF_SOURCE_PREFIX) == 0,iif(equals('NO_EXT',$DF_SOURCE_SUFFIX),'*',concat('*',$DF_SOURCE_SUFFIX)),iif(equals('NO_EXT',$DF_SOURCE_SUFFIX),concat($DF_SOURCE_PREFIX,'*'),concat($DF_SOURCE_PREFIX,'*',$DF_SOURCE_SUFFIX)))))]) ~> Source",
            "Source derive(DELPHIX_COMPLIANCE_SERVICES_SINK_FILE_NAME = replace(DELPHIX_COMPLIANCE_SERVICES_FILE_NAME, $DF_SOURCE_DIRECTORY, $DF_SINK_DIRECTORY)) ~> CreateSinkFileName",
            "CreateSinkFileName sink(allowSchemaDrift: true,",
            "     validateSchema: false,",
            "     format: 'delimited',",
            "     fileSystem: ($DF_SINK_DATABASE),",
            "     folderPath: ($DF_SINK_DIRECTORY),",
            "     columnDelimiter: ($DF_COLUMN_DELIMITER),",
            "     escapeChar: ($DF_ESCAPE_CHARACTER),",
            "     quoteChar: ($DF_QUOTE_CHARACTER),",
            "     nullValue: ($DF_NULL_VALUE),",
            "     columnNamesAsHeader: true,",
            "     rowUrlColumn:'DELPHIX_COMPLIANCE_SERVICES_SINK_FILE_NAME',",
            "     umask: 0022,",
            "     preCommands: [],",
            "     postCommands: [],",
            "     skipDuplicateMapInputs: true,",
            "     skipDuplicateMapOutputs: true,",
            "     mapColumn(",
            "          each(match(name!=\"DELPHIX_COMPLIANCE_SERVICES_FILE_NAME\"&&name!=\"DELPHIX_COMPLIANCE_SERVICES_SINK_FILE_NAME\"))",
            "     )) ~> Sink"
          ]
        }
      },
      "dependsOn": []
    }
  ]
}
