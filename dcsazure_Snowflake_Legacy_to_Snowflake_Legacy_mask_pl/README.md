# dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_pl
## Delphix Compliance Services (DCS) for Azure - Snowflake (Legacy) to Snowflake (Legacy) Masking Pipeline

This pipeline will perform masking of your Snowflake Instance.

### Prerequisites
1. Configure the hosted metadata database and associated Azure SQL service.
1. Configure the DCS for Azure REST service.
1. Configure the Snowflake (Legacy) linked service.
1. Configure the Blob Storage linked service.

### Importing
There are several linked services that will need to be selected in order to perform the masking of your Snowflake
instance.

These linked services types are needed for the following steps:

`Azure Blob Storage` (staging) - Linked service associated with a blob storage container that can be used to stage data
when performing a data copy from Snowflake. This will be used for the following steps:
* If Copy Via Dataflow (If Condition activity)

`Snowflake (Legacy)` (source) - Linked service associated with unmasked Snowflake data. This will be used for the following
steps:
* dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_df/SnowflakeSource (dataFlow)
* dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_df/SnowflakeSink (dataFlow)
* dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_source_ds (Snowflake (Legacy) dataset)
* dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_copy_df/SnowflakeSource (dataFlow)
* dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_copy_df/SnowflakeSink (dataFlow)

`Azure SQL` (metadata) - Linked service associated with your hosted metadata store. This will be used for the following
steps:
* dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_metadata_ds (Azure SQL Database dataset)
* dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_params_df/Ruleset (dataFlow)
* dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_params_df/TypeMapping (dataFlow)

`REST` (DCS for Azure) - Linked service associated with calling DCS for Azure. This will be used for the following
steps:
* dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_df (dataFlow)

### How It Works
* Select Tables Without Required Masking. This is done by querying the metadata data store.
  * Filter If Copy Unmasked Enabled. This is done by applying a filter based on the value of `P_COPY_UNMASKED_TABLES`
    * For Each Table With No Masking. Provided we have any rows left after applying the filter
      * If Copy Via Dataflow - based on the value of `P_COPY_USE_DATAFLOW`
        * If the data flow is to be used for copy, then call `dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_copy_df`
        * If the data flow is not to be used for copy, then use a copy activity
* Select Tables That Require Masking
  * For Each Table To Mask
    * Call the `dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_params_df` data flow to generate masking parameters
    *  Call the `dcsazure_Snowflake_Legacy_to_Snowflake_Legacy_mask_df` data flow, passing in parameters as generated by
       the generate masking parameters dataflow

### Parameters

* `P_SOURCE_DATABASE` - This is the database in Snowflake that contains data we will mask
* `P_SINK_DATABASE` - This is the database in Snowflake that contains the location where we should put masked data
* `P_SOURCE_SCHEMA` - This is the schema within the above source database that we will mask
* `P_SINK_SCHEMA` - This is the schema within the above sink database where we will place masked data
* `P_METADATA_SCHEMA` - This is the schema to be used for in the self-hosted AzureSQL database for storing metadata
(default `dbo`)
* `P_METADATA_RULESET_TABLE` - This is the table to be used for storing the discovered ruleset
    (default `discovered_ruleset`)
* `P_METADATA_SOURCE_TO_SINK_MAPPING_TABLE` - This is the table in the metadata schema that will contain the data
mapping, defining where unmasked data lives, and where masked data should go (default `adf_data_mapping`)
* `P_METADATA_ADF_TYPE_MAPPING_TABLE` - This is the table that maps from data types in various datasets to the
associated datatype required in ADF as neede for the pipeline (default `adf_type_mapping`)
* `P_COPY_UNMASKED_TABLES` - This is a flag to indicate whether or not we should copy over data that does not have any
assigned algorithms (default `false`)
* `P_COPY_USE_DATAFLOW` - This is a flag to indicate whether we should use a copy activity or a copy data flow and is
only relevant when using `P_COPY_UNMASKED_TABLES` (default `false`)
* `P_STAGING_STORAGE_PATH` - This is a path that specifies where we should stage data as it moves through the pipeline
and should reference a storage container in a storage account
